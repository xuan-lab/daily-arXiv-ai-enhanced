<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 154]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.HC](#cs.HC) [Total: 15]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 18]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 6]
- [math.CO](#math.CO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
*ByteDance Seed,:,Yufeng Yuan,Yu Yue,Mingxuan Wang,Xiaochen Zuo,Jiaze Chen,Lin Yan,Wenyuan Xu,Chi Zhang,Xin Liu,Chengyi Wang,TianTian Fan,Lingjun Liu,Qiying Yu,Xiangpeng Wei,Zhiqi Lin,Ruofei Zhu,Qingping Yang,Chengzhi Wei,Jerry He,Guanlin Liu,Zheng Wu,Xiangyu Yu,Zhicheng Liu,Jingjing Xu,Jiangjie Chen,Haojie Pan,Shengding Hu,Zhengyin Du,Wenqi Wang,Zewei Sun,Chenwei Lou,Bole Ma,Zihan Wang,Mofan Zhang,Wang Zhang,Gaohong Liu,Kaihua Jiang,Haibin Lin,Ru Zhang,Juncai Liu,Li Han,Jinxin Chi,Wenqiang Zhang,Jiayi Xu,Jun Yuan,Zhen Xiao,Yuqiao Xian,Jingqiao Wu,Kai Hua,Na Zhou,Jianhui Duan,Heyang Lu,Changbao Wang,Jinxiang Ou,Shihang Wang,Xiaoran Jin,Xuesong Yao,Chengyin Xu,Wenchang Ma,Zhecheng An,Renming Pang,Xia Xiao,Jing Su,Yuyu Zhang,Tao Sun,Kaibo Liu,Yifan Sun,Kai Shen,Sijun Zhang,Yiyuan Ma,Xingyan Bin,Ji Li,Yao Luo,Deyi Liu,Shiyi Zhan,Yunshui Li,Yuan Yang,Defa Zhu,Ke Shen,Chenggang Li,Xun Zhou,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-Thinking-v1.5是一种具有优秀推理能力的混合专家模型，在STEM和编码任务中表现优异，并在非推理任务中展现出广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 旨在提升模型在推理任务中的性能，并验证其在多样化领域的泛化能力。

Method: 采用混合专家（MoE）架构，激活参数20B，总参数200B，通过预思考机制提升推理能力。

Result: 在AIME 2024、Codeforces和GPQA等基准测试中表现优异，非推理任务中胜率超过DeepSeek R1 8%。

Conclusion: Seed-Thinking-v1.5在推理和泛化能力上均优于现有模型，同时发布了两个新基准支持未来研究。

Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before
responding, resulting in improved performance on a wide range of benchmarks.
Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on
GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond
reasoning tasks, the method demonstrates notable generalization across diverse
domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on
non-reasoning tasks, indicating its broader applicability. Compared to other
state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts
(MoE) model with a relatively small size, featuring 20B activated and 200B
total parameters. As part of our effort to assess generalized reasoning, we
develop two internal benchmarks, BeyondAIME and Codeforces, both of which will
be publicly released to support future research.

</details>


### [2] [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
*Djamila Mohdeb,Meriem Laifa,Zineb Guemraoui,Dalila Behih*

Main category: cs.CL

TL;DR: 通过计算分析阿拉伯语在线内容，研究揭示了六类阴谋论在阿拉伯数字空间中的传播，填补了以英语内容为主的阴谋论研究空白。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补阿拉伯语数字空间中阴谋论研究的空白，传统研究多集中于英语内容或线下数据。

Method: 结合命名实体识别和主题建模（Top2Vec算法），分析阿拉伯博客和Facebook数据，分类阴谋论叙事。

Result: 识别出六类阴谋论：性别/女权、地缘政治、政府掩盖、末日论、犹太共济会和地球工程。

Conclusion: 研究揭示了阿拉伯社交媒体中阴谋论的深层嵌入及其对公共话语的影响，为理解阿拉伯世界的阴谋论提供了新视角。

Abstract: This study investigates the spread of conspiracy theories in Arabic digital
spaces through computational analysis of online content. By combining Named
Entity Recognition and Topic Modeling techniques, specifically the Top2Vec
algorithm, we analyze data from Arabic blogs and Facebook to identify and
classify conspiratorial narratives. Our analysis uncovers six distinct
categories: gender/feminist, geopolitical, government cover-ups, apocalyptic,
Judeo-Masonic, and geoengineering. The research highlights how these narratives
are deeply embedded in Arabic social media discourse, shaped by regional
historical, cultural, and sociopolitical contexts. By applying advanced Natural
Language Processing methods to Arabic content, this study addresses a gap in
conspiracy theory research, which has traditionally focused on English-language
content or offline data. The findings provide new insights into the
manifestation and evolution of conspiracy theories in Arabic digital spaces,
enhancing our understanding of their role in shaping public discourse in the
Arab world.

</details>


### [3] [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
*Jaime Raldua Veuthey,Zainab Ali Majid,Suhas Hariharan,Jacob Haimes*

Main category: cs.CL

TL;DR: MEQA框架用于评估问答基准的质量，提出标准化评分和比较方法，并以网络安全领域为例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，对其评估的需求日益迫切，但现有基准的质量评估存在空白。

Method: 提出MEQA框架，结合人类和LLM评估者，对问答基准进行标准化和量化评分。

Result: 在网络安全领域的实验中，MEQA成功揭示了基准的优势和不足。

Conclusion: MEQA为问答基准的评估提供了有效工具，尤其适用于LLMs的双重角色（防御工具与安全威胁）领域。

Abstract: As Large Language Models (LLMs) advance, their potential for widespread
societal impact grows simultaneously. Hence, rigorous LLM evaluations are both
a technical necessity and social imperative. While numerous evaluation
benchmarks have been developed, there remains a critical gap in
meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a
framework for the meta-evaluation of question and answer (QA) benchmarks, to
provide standardized assessments, quantifiable scores, and enable meaningful
intra-benchmark comparisons. We demonstrate this approach on cybersecurity
benchmarks, using human and LLM evaluators, highlighting the benchmarks'
strengths and weaknesses. We motivate our choice of test domain by AI models'
dual nature as powerful defensive tools and security threats.

</details>


### [4] [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
*Laerdon Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于4位量化Gemma 2 9B模型的少样本学习方法，用于分类Reddit心理健康数据中的自我状态。通过句子分块预处理和二元分类，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决心理健康数据中自我状态的分类问题，提升分类性能。

Method: 使用4位量化Gemma 2 9B模型进行少样本学习，结合句子分块预处理和二元分类。

Result: 在14个系统中排名第三，测试召回率为0.579。

Conclusion: 句子分块预处理能有效提升模型性能，匹配人类标注粒度并简化任务。

Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states
in mental health data taken from Reddit. We use few-shot learning with a 4-bit
quantized Gemma 2 9B model and a data preprocessing step which first identifies
relevant sentences indicating self-state evidence, and then performs a binary
classification to determine whether the sentence is evidence of an adaptive or
maladaptive self-state. This system outperforms our other method which relies
on an LLM to highlight spans of variable length independently. We attribute the
performance of our model to the benefits of this sentence chunking step for two
reasons: partitioning posts into sentences 1) broadly matches the granularity
at which self-states were human-annotated and 2) simplifies the task for our
language model to a binary classification problem. Our system places third out
of fourteen systems submitted for Task A.1, achieving a test-time recall of
0.579.

</details>


### [5] [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
*Kang He,Kaushik Roy*

Main category: cs.CL

TL;DR: LogicTree是一个推理时模块化框架，通过算法引导搜索解决LLMs在复杂逻辑推理中的挑战，提升证明准确性和推理效率。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂逻辑推理中面临系统性探索和逻辑一致性的挑战，以及大前提空间下组合搜索的困难。

Method: 提出LogicTree框架，结合缓存机制和线性化前提搜索，引入LLM-free启发式策略优化证明搜索。

Result: 在五个数据集上，LogicTree平均比CoT和ToT分别提升23.6%和12.5%的证明准确性，GPT-4o在框架内表现更优。

Conclusion: LogicTree通过结构化推理和高效前提选择，显著提升了LLMs在复杂逻辑任务中的表现。

Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning
capabilities across various domains. However, LLMs still face distinct
challenges in complex logical reasoning, as (1) proof-finding requires
systematic exploration and the maintenance of logical coherence and (2)
searching the right combination of premises at each reasoning step is
inherently challenging in tasks with large premise space. To address this, we
propose LogicTree, an inference-time modular framework employing
algorithm-guided search to automate structured proof exploration and ensure
logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate
caching mechanism into LogicTree to enable effective utilization of historical
knowledge, preventing reasoning stagnation and minimizing redundancy.
Furthermore, we address the combinatorial complexity of premise search by
decomposing it into a linear process. The refined premise selection restricts
subsequent inference to at most one derivation per step, enhancing reasoning
granularity and enforcing strict step-by-step reasoning. Additionally, we
introduce two LLM-free heuristics for premise prioritization, enabling
strategic proof search. Experimental results on five datasets demonstrate that
LogicTree optimally scales inference-time computation to achieve higher proof
accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%
and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o
outperforms o3-mini by 7.6% on average.

</details>


### [6] [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
*Nusrat Jahan Prottasha,Upama Roy Chowdhury,Shetu Mohanto,Tasfia Nuzhat,Abdullah As Sami,Md Shamol Ali,Md Shohanur Islam Sobuj,Hafijur Raman,Md Kowsher,Ozlem Ozmen Garibay*

Main category: cs.CL

TL;DR: 本文综述了参数高效微调（PEFT）技术，旨在解决大型模型（如LLMs和VLMs）全微调的高成本问题，通过仅更新少量参数实现任务适配。


<details>
  <summary>Details</summary>
Motivation: 传统微调需要大量计算资源和任务数据，存在过拟合、灾难性遗忘和参数低效等问题，PEFT提供了一种高效解决方案。

Method: 提出PEFT的分类法，包括加法、选择性、重参数化、混合和统一框架，并系统比较其机制与权衡。

Result: PEFT在语言、视觉和生成建模等领域表现出色，资源成本更低。

Conclusion: PEFT在实用性和可持续性方面具有潜力，未来可探索联邦学习、领域适应和理论支持等方向。

Abstract: Large models such as Large Language Models (LLMs) and Vision Language Models
(VLMs) have transformed artificial intelligence, powering applications in
natural language processing, computer vision, and multimodal learning. However,
fully fine-tuning these models remains expensive, requiring extensive
computational resources, memory, and task-specific data. Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting
large models to downstream tasks by updating only a small portion of
parameters. This survey presents a comprehensive overview of PEFT techniques,
focusing on their motivations, design principles, and effectiveness. We begin
by analyzing the resource and accessibility challenges posed by traditional
fine-tuning and highlight key issues, such as overfitting, catastrophic
forgetting, and parameter inefficiency. We then introduce a structured taxonomy
of PEFT methods -- grouped into additive, selective, reparameterized, hybrid,
and unified frameworks -- and systematically compare their mechanisms and
trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse
domains, including language, vision, and generative modeling, showing how these
techniques offer strong performance with lower resource costs. We also discuss
important open challenges in scalability, interpretability, and robustness, and
suggest future directions such as federated learning, domain adaptation, and
theoretical grounding. Our goal is to provide a unified understanding of PEFT
and its growing role in enabling practical, efficient, and sustainable use of
large models.

</details>


### [7] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
*Katie Matton,Robert Osazuwa Ness,John Guttag,Emre Kıcıman*

Main category: cs.CL

TL;DR: 论文提出了一种新方法来衡量大语言模型（LLM）解释的忠实性，通过定义忠实性并开发基于反事实和贝叶斯层次模型的方法，揭示了LLM解释中可能隐藏的社会偏见和误导性证据。


<details>
  <summary>Details</summary>
Motivation: LLM生成的解释可能不忠实，导致过度信任和误用，因此需要一种方法来量化其解释的忠实性。

Method: 定义忠实性为解释中隐含的概念与真实影响概念之间的差异，使用辅助LLM生成反事实输入，并通过贝叶斯层次模型量化概念的影响。

Result: 实验表明，该方法能有效量化不忠实性，并在社会偏见和医学问答任务中发现LLM解释中的隐藏偏见和误导性证据。

Conclusion: 该方法为评估LLM解释的忠实性提供了新工具，揭示了其潜在问题，有助于减少误用。

Abstract: Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.

</details>


### [8] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
*Zhiyuan Wang,Qingni Wang,Yue Zhang,Tianlong Chen,Xiaofeng Zhu,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: 论文提出了一种名为选择性共形不确定性（SConU）的新方法，通过开发两种共形p值来检测异常数据，以管理误覆盖率并提高预测效率。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法识别违反交换性假设的异常数据，导致误覆盖率无限制和预测集不可操作。

Method: 开发两种共形p值，用于检测样本是否偏离校准集的不确定性分布，并管理误覆盖率。

Result: SConU方法在单领域和跨学科背景下有效管理误覆盖率，并提高预测效率。

Conclusion: SConU方法为高风险的问答任务提供了近似条件覆盖率的解决方案。

Abstract: As large language models are increasingly utilized in real-world
applications, guarantees of task-specific metrics are essential for their
reliable deployment. Previous studies have introduced various criteria of
conformal uncertainty grounded in split conformal prediction, which offer
user-specified correctness coverage. However, existing frameworks often fail to
identify uncertainty data outliers that violate the exchangeability assumption,
leading to unbounded miscoverage rates and unactionable prediction sets. In
this paper, we propose a novel approach termed Selective Conformal Uncertainty
(SConU), which, for the first time, implements significance tests, by
developing two conformal p-values that are instrumental in determining whether
a given sample deviates from the uncertainty distribution of the calibration
set at a specific manageable risk level. Our approach not only facilitates
rigorous management of miscoverage rates across both single-domain and
interdisciplinary contexts, but also enhances the efficiency of predictions.
Furthermore, we comprehensively analyze the components of the conformal
procedures, aiming to approximate conditional coverage, particularly in
high-stakes question-answering tasks.

</details>


### [9] [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
*Ziyan Zhang,Yang Hou,Chen Gong,Zhenghua Li*

Main category: cs.CL

TL;DR: 论文分析了大型语言模型（LLMs）在句法解析任务中的不足，并提出了一种利用现有树库语法规则的自校正方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理任务中表现优异，但在句法解析等基础任务上仍存在不足，限制了其深层语言理解能力。

Method: 提出自校正方法，通过检测潜在错误并动态搜索相关语法规则，引导LLMs自我修正。

Result: 在多个数据集和LLMs上的实验表明，该方法显著提升了英语和中文数据集的性能。

Conclusion: 自校正方法有效帮助LLMs在不额外训练的情况下提升句法解析能力。

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing (NLP) tasks. However, recent studies suggest that
they still face challenges in performing fundamental NLP tasks essential for
deep language understanding, particularly syntactic parsing. In this paper, we
conduct an in-depth analysis of LLM parsing capabilities, delving into the
specific shortcomings of their parsing results. We find that LLMs may stem from
limitations to fully leverage grammar rules in existing treebanks, which
restricts their capability to generate valid syntactic structures. To help LLMs
acquire knowledge without additional training, we propose a self-correction
method that leverages grammar rules from existing treebanks to guide LLMs in
correcting previous errors. Specifically, we automatically detect potential
errors and dynamically search for relevant rules, offering hints and examples
to guide LLMs in making corrections themselves. Experimental results on three
datasets with various LLMs, demonstrate that our method significantly improves
performance in both in-domain and cross-domain settings on the English and
Chinese datasets.

</details>


### [10] [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: 研究发现，基于LLM的查询扩展方法在零样本检索任务中的性能提升可能源于基准测试中的知识泄漏，而非方法本身的有效性。


<details>
  <summary>Details</summary>
Motivation: 挑战LLM生成假设文档能提升检索性能的假设，探究基准测试中知识泄漏对性能的影响。

Method: 以事实验证为测试平台，分析生成文档是否包含与真实证据相关的信息及其对性能的影响。

Result: 性能提升仅发生在生成文档包含与真实证据相关的句子时，表明基准测试中可能存在知识泄漏。

Conclusion: LLM查询扩展方法的性能可能被高估，尤其是在需要检索小众或新知识的实际场景中。

Abstract: Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyzed whether the generated documents contained information
entailed by ground truth evidence and assessed their impact on performance. Our
findings indicate that performance improvements occurred consistently only for
claims whose generated documents included sentences entailed by ground truth
evidence. This suggests that knowledge leakage may be present in these
benchmarks, inflating the perceived performance of LLM-based query expansion
methods, particularly in real-world scenarios that require retrieving niche or
novel knowledge.

</details>


### [11] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
*Xinlin Zhuang,Jiahui Peng,Ren Ma,Yinfan Wang,Tianyi Bai,Xingjian Wei,Jiantao Qiu,Chi Zhang,Ying Qian,Conghui He*

Main category: cs.CL

TL;DR: 论文提出PRRC和Meta-rater方法，通过多维度评估数据质量，显著提升模型训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM预训练数据集不透明，且数据选择方法单一，限制了模型性能优化。

Method: 提出PRRC（专业性、可读性、推理性、清洁性）评估框架和Meta-rater多维度数据选择方法，结合代理模型预测验证损失。

Result: Meta-rater使1.3B参数模型收敛速度翻倍，下游任务性能提升3.23，并在3.3B模型上验证了可扩展性。

Conclusion: 多维度数据质量整合显著优于传统单维度方法，为预训练效率和模型能力提升提供了可扩展范式。

Abstract: The composition of pre-training datasets for large language models (LLMs)
remains largely undisclosed, hindering transparency and efforts to optimize
data quality, a critical driver of model performance. Current data selection
methods, such as natural language quality assessments, diversity-based filters,
and classifier-based approaches, are limited by single-dimensional evaluation
or redundancy-focused strategies. To address these gaps, we propose PRRC to
evaluate data quality across Professionalism, Readability, Reasoning, and
Cleanliness. We further introduce Meta-rater, a multi-dimensional data
selection method that integrates these dimensions with existing quality metrics
through learned optimal weightings. Meta-rater employs proxy models to train a
regression model that predicts validation loss, enabling the identification of
optimal combinations of quality scores. Experiments demonstrate that Meta-rater
doubles convergence speed for 1.3B parameter models and improves downstream
task performance by 3.23, with scalable benefits observed in 3.3B models
trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B
dataset, labeled across 25 quality metrics (including PRRC), to advance
research in data-centric LLM development. Our work establishes that holistic,
multi-dimensional quality integration significantly outperforms conventional
single-dimension approaches, offering a scalable paradigm for enhancing
pre-training efficiency and model capability.

</details>


### [12] [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
*Jian Zhang,Tianqing Zhang,Qi Li,Hongwei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的损失函数EIoU-EMC，用于解决嵌套NER任务中的低资源和类别不平衡问题，在生物医学和工业领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 生物医学和工业领域的嵌套NER任务面临低资源和类别不平衡的挑战，需要一种更有效的方法。

Method: 设计了EIoU-EMC损失函数，结合了IoU和多类损失，利用实体边界和分类信息提升模型性能。

Result: 在三个生物医学数据集和一个工业数据集上表现优于基线方法，尤其在实体边界识别和分类上有显著提升。

Conclusion: EIoU-EMC方法有效解决了特定领域嵌套NER任务中的挑战，具有广泛应用潜力。

Abstract: In recent years, research has mainly focused on the general NER task. There
still have some challenges with nested NER task in the specific domains.
Specifically, the scenarios of low resource and class imbalance impede the wide
application for biomedical and industrial domains. In this study, we design a
novel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss
and Multiclass loss. Our proposed method specially leverages the information of
entity boundary and entity classification, thereby enhancing the model's
capacity to learn from a limited number of data samples. To validate the
performance of this innovative method in enhancing NER task, we conducted
experiments on three distinct biomedical NER datasets and one dataset
constructed by ourselves from industrial complex equipment maintenance
documents. Comparing to strong baselines, our method demonstrates the
competitive performance across all datasets. During the experimental analysis,
our proposed method exhibits significant advancements in entity boundary
recognition and entity classification. Our code are available here.

</details>


### [13] [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
*Takuma Udagawa,Yang Zhao,Hiroshi Kanayama,Bishwaranjan Bhattacharjee*

Main category: cs.CL

TL;DR: 提出了一种高效的标注流程，用于分析预训练语料库中的社会偏见，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 预训练数据中的社会偏见可能被大语言模型放大，需要有效的方法来分析和缓解。

Method: 采用保护属性检测和尊重分类的标注流程，分析语言对不同属性的极性。

Result: 实验证明了该偏见分析和缓解措施的有效性。

Conclusion: 提出的方法能有效识别和缓解预训练语料中的社会偏见。

Abstract: Large language models (LLMs) acquire general linguistic knowledge from
massive-scale pretraining. However, pretraining data mainly comprised of
web-crawled texts contain undesirable social biases which can be perpetuated or
even amplified by LLMs. In this study, we propose an efficient yet effective
annotation pipeline to investigate social biases in the pretraining corpora.
Our pipeline consists of protected attribute detection to identify diverse
demographics, followed by regard classification to analyze the language
polarity towards each attribute. Through our experiments, we demonstrate the
effect of our bias analysis and mitigation measures, focusing on Common Crawl
as the most representative pretraining corpus.

</details>


### [14] [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
*Junchi Yao,Shu Yang,Jianhua Xu,Lijie Hu,Mengdi Li,Di Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为“Duplicatus Charm”的新方法，通过机制可解释性研究大语言模型（LLMs）中重复文本生成的根源，并利用稀疏自编码器（SAEs）识别和操控“重复特征”，有效缓解了“重复诅咒”。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多个领域取得显著进展，但其文本生成中常出现重复现象（“重复诅咒”），现有解码策略未能深入探索其机制。

Method: 通过机制可解释性分析重复根源，利用SAEs提取“重复特征”，并通过激活操控验证和缓解重复问题。

Result: 成功识别并操控导致重复的模型激活特征，构建了重复数据集并量化其影响，通过去激活这些特征有效缓解了重复问题。

Conclusion: 研究揭示了LLMs中重复生成的机制，并提出了一种有效缓解重复问题的方法，为未来模型优化提供了新思路。

Abstract: Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse.

</details>


### [15] [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
*Michael Färber,Parisa Aghdam,Kyuri Im,Mario Tawfelis,Hardik Ghoshal*

Main category: cs.CL

TL;DR: 本文介绍了首个利用GPT-4和Llama-3的多格式输入文本简化系统，支持定制化输出，以提升包容性。


<details>
  <summary>Details</summary>
Motivation: 复杂文本对多样化受众的理解构成挑战，现有简化方法未能充分利用大语言模型（LLMs）实现定制化。

Method: 开发了支持多格式输入的系统，采用GPT-4和Llama-3，并通过多指标评估输出。

Result: 系统能生成定制化的简明语言内容，促进包容性。

Conclusion: 研究推动了自动文本简化领域，并强调了定制化沟通的重要性。

Abstract: Text simplification is essential for making complex content accessible to
diverse audiences who face comprehension challenges. Yet, the limited
availability of simplified materials creates significant barriers to personal
and professional growth and hinders social inclusion. Although researchers have
explored various methods for automatic text simplification, none fully leverage
large language models (LLMs) to offer tailored customization for different
target groups and varying levels of simplicity. Moreover, despite its proven
benefits for both consumers and organizations, the well-established practice of
plain language remains underutilized. In this paper, we
https://simplifymytext.org, the first system designed to produce plain language
content from multiple input formats, including typed text and file uploads,
with flexible customization options for diverse audiences. We employ GPT-4 and
Llama-3 and evaluate outputs across multiple metrics. Overall, our work
contributes to research on automatic text simplification and highlights the
importance of tailored communication in promoting inclusivity.

</details>


### [16] [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
*Bowen Jiang,Zhuoqun Hao,Young-Min Cho,Bryan Li,Yuan Yuan,Sihao Chen,Lyle Ungar,Camillo J. Taylor,Dan Roth*

Main category: cs.CL

TL;DR: 论文介绍了PERSONAMEM基准，用于评估LLMs如何利用用户交互历史进行个性化响应，发现现有模型在动态跟踪用户偏好方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何利用用户交互历史内部化用户特性和偏好，并动态跟踪其变化以生成个性化响应。

Method: 提出PERSONAMEM基准，包含180个模拟用户-LLM交互历史，评估LLMs在动态用户画像下的响应能力。

Result: 当前LLMs（如GPT-4.1等）在直接提示方法下表现不佳，整体准确率仅约50%。

Conclusion: PERSONAMEM为开发真正用户感知的聊天机器人提供了研究基础。

Abstract: Large Language Models (LLMs) have emerged as personalized assistants for
users across a wide range of tasks -- from offering writing support to
delivering tailored recommendations or consultations. Over time, the
interaction history between a user and an LLM can provide extensive information
about an individual's traits and preferences. However, open questions remain on
how well LLMs today can effectively leverage such history to (1) internalize
the user's inherent traits and preferences, (2) track how the user profiling
and preferences evolve over time, and (3) generate personalized responses
accordingly in new scenarios.
  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features
curated user profiles with over 180 simulated user-LLM interaction histories,
each containing up to 60 sessions of multi-turn conversations across 15
real-world tasks that require personalization. Given an in-situ user query,
i.e. query issued by the user from the first-person perspective, we evaluate
LLM chatbots' ability to identify the most suitable response according to the
current state of the user's profile. We observe that current LLMs still
struggle to recognize the dynamic evolution in users' profiles over time
through direct prompting approaches. As a consequence, LLMs often fail to
deliver responses that align with users' current situations and preferences,
with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0
achieving only around 50% overall accuracy, suggesting room for improvement. We
hope that PERSONAMEM, along with the user profile and conversation simulation
pipeline, can facilitate future research in the development of truly user-aware
chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.

</details>


### [17] [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
*Demetris Paschalides,George Pallis,Marios D. Dikaiakos*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在政治意识形态谱系中的可操纵性，超越了传统的左右二分法，提出了一种多任务数据集来评估模型的意识形态表达。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在政治敏感领域的意识形态操纵风险，特别是超越传统左右二分法的多维度影响。

Method: 通过多任务数据集（如意识形态问答、宣言填空等）微调三种LLMs（Phi-2、Mistral、Llama-3），评估其意识形态表达。

Result: 微调显著提升模型的意识形态对齐能力，而显式提示仅能带来微小改进。

Conclusion: LLMs易受意识形态操纵，需更强防护措施。

Abstract: Large Language Models (LLMs) have transformed natural language processing,
but concerns have emerged about their susceptibility to ideological
manipulation, particularly in politically sensitive areas. Prior work has
focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning
on political QA datasets. In this work, we move beyond this binary approach to
explore the extent to which LLMs can be influenced across a spectrum of
political ideologies, from Progressive-Left to Conservative-Right. We introduce
a novel multi-task dataset designed to reflect diverse ideological positions
through tasks such as ideological QA, statement ranking, manifesto cloze
completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,
Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and
express these nuanced ideologies. Our findings indicate that fine-tuning
significantly enhances nuanced ideological alignment, while explicit prompts
provide only minor refinements. This highlights the models' susceptibility to
subtle ideological manipulation, suggesting a need for more robust safeguards
to mitigate these risks.

</details>


### [18] [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
*Xingyu Li,Chen Gong,Guohong Fu*

Main category: cs.CL

TL;DR: 该论文介绍了TikTalkCoref，首个中文社交媒体多模态共指消解数据集，填补了真实对话场景中多模态共指研究的空白，并提出了基准方法。


<details>
  <summary>Details</summary>
Motivation: 多模态共指消解（MCR）对理解跨模态内容至关重要，但缺乏真实对话场景的数据资源。

Method: 构建TikTalkCoref数据集，包含短视频和用户评论的文本对话，并标注共指簇；提出针对名人领域的基准方法。

Result: 在TikTalkCoref数据集上进行了广泛实验，提供了可靠的基准结果。

Conclusion: TikTalkCoref数据集的发布将促进社交媒体对话中多模态共指消解的研究。

Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring
to the same entity across different modalities, such as text and visuals, and
is essential for understanding multimodal content. In the era of rapidly
growing mutimodal content and social media, MCR is particularly crucial for
interpreting user interactions and bridging text-visual references to improve
communication and personalization. However, MCR research for real-world
dialogues remains unexplored due to the lack of sufficient data resources.To
address this gap, we introduce TikTalkCoref, the first Chinese multimodal
coreference dataset for social media in real-world scenarios, derived from the
popular Douyin short-video platform. This dataset pairs short videos with
corresponding textual dialogues from user comments and includes manually
annotated coreference clusters for both person mentions in the text and the
coreferential person head regions in the corresponding video frames. We also
present an effective benchmark approach for MCR, focusing on the celebrity
domain, and conduct extensive experiments on our dataset, providing reliable
benchmark results for this newly constructed dataset. We will release the
TikTalkCoref dataset to facilitate future research on MCR for real-world social
media dialogues.

</details>


### [19] [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 本文研究了从Transformer教师模型到九种子二次复杂度学生模型的知识蒸馏效果，探讨了不同架构和初始化策略的影响。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度是推理时的瓶颈，因此探索子二次复杂度的替代模型（如SSMs、线性注意力和循环架构）具有重要意义。

Method: 系统评估了从Transformer教师模型到九种子二次复杂度学生模型的知识蒸馏效果，并研究了矩阵混合和QKV复制等初始化策略的影响。

Result: 在多NLP基准测试中，实证结果表明了效率和性能之间的权衡，并揭示了成功知识转移的关键因素。

Conclusion: 研究为子二次复杂度架构的知识蒸馏提供了实用见解，强调了架构选择和初始化策略的重要性。

Abstract: Knowledge distillation is a widely used technique for compressing large
language models (LLMs) by training a smaller student model to mimic a larger
teacher model. Typically, both the teacher and student are Transformer-based
architectures, leveraging softmax attention for sequence modeling. However, the
quadratic complexity of self-attention at inference time remains a significant
bottleneck, motivating the exploration of subquadratic alternatives such as
structured state-space models (SSMs), linear attention, and recurrent
architectures. In this work, we systematically evaluate the transferability of
knowledge distillation from a Transformer teacher to nine subquadratic student
architectures. Our study aims to determine which subquadratic model best aligns
with the teacher's learned representations and how different architectural
constraints influence the distillation process. We also investigate the impact
of intelligent initialization strategies, including matrix mixing and
query-key-value (QKV) copying, on the adaptation process. Our empirical results
on multiple NLP benchmarks provide insights into the trade-offs between
efficiency and performance, highlighting key factors for successful knowledge
transfer to subquadratic architectures.

</details>


### [20] [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
*Gabriel Machado Santos,Rita Maria da Silva Julia,Marcelo Zanchetta do Nascimento*

Main category: cs.CL

TL;DR: 论文提出了一种结合上下文无关语法和MAP-Elites算法的进化方法，用于系统探索提示空间，优化大型语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 提示工程对优化大型语言模型至关重要，但提示结构与任务性能之间的关系尚未充分研究。

Method: 采用上下文无关语法（CFG）与MAP-Elites算法结合的方法，系统探索提示空间，注重提示的质量和多样性。

Result: 在多个LLM和七项BigBench Lite任务上的评估表明，该方法揭示了结构变化对性能的影响，提升了提示设计的有效性和适应性。

Conclusion: 该方法为任务特定和适应性提示设计提供了实用见解，强调了质量和多样性的重要性。

Abstract: Prompt engineering is essential for optimizing large language models (LLMs),
yet the link between prompt structures and task performance remains
underexplored. This work introduces an evolutionary approach that combines
context-free grammar (CFG) with the MAP-Elites algorithm to systematically
explore the prompt space. Our method prioritizes quality and diversity,
generating high-performing and structurally varied prompts while analyzing
their alignment with diverse tasks by varying traits such as the number of
examples (shots) and reasoning depth. By systematically mapping the phenotypic
space, we reveal how structural variations influence LLM performance, offering
actionable insights for task-specific and adaptable prompt design. Evaluated on
seven BigBench Lite tasks across multiple LLMs, our results underscore the
critical interplay of quality and diversity, advancing the effectiveness and
versatility of LLMs.

</details>


### [21] [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
*Tong Chen,Faeze Brahman,Jiacheng Liu,Niloofar Mireshghallah,Weijia Shi,Pang Wei Koh,Luke Zettlemoyer,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: ParaPO是一种后训练方法，通过微调语言模型以减少无意的记忆复现，同时保持其整体实用性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在非对抗性场景下记忆并复现预训练数据的问题，涉及版权、抄袭、隐私和创造力等担忧。

Method: 引入Paraphrase Preference Optimization (ParaPO)，训练模型偏好记忆内容的改写版本，而非原始内容。通过系统提示控制复现行为。

Result: 在Llama3.1-8B和Tulu3-8B模型上，ParaPO显著减少复现指标，同时保留对名言的适当回忆能力。

Conclusion: ParaPO是一种有效的方法，能在减少无意识复现的同时保持语言模型的实用性。

Abstract: Language models (LMs) can memorize and reproduce segments from their
pretraining data verbatim even in non-adversarial settings, raising concerns
about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase
Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to
reduce unintentional regurgitation while preserving their overall utility.
ParaPO trains LMs to prefer paraphrased versions of memorized segments over the
original verbatim content from the pretraining data. To maintain the ability to
recall famous quotations when appropriate, we develop a variant of ParaPO that
uses system prompts to control regurgitation behavior. In our evaluation on
Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested
datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative
writing), whereas unlearning methods used in prior work to mitigate
regurgitation are less effective outside their targeted unlearned domain (from
17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO
with system prompting successfully preserves famous quotation recall while
reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when
prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the
model not to regurgitate produces only a marginal reduction (8.7 to 8.4).

</details>


### [22] [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
*Armin Toroghi,Willis Guo,Scott Sanner*

Main category: cs.CL

TL;DR: 论文提出新数据集CoLoTa，评估LLMs在长尾实体常识推理中的表现，揭示其局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在编码事实和常识知识方面表现出色，但在高风险场景中，幻觉和推理错误仍是主要障碍。

Method: 构建CoLoTa数据集（3,300个查询），涵盖多样常识推理任务，并测试LLM和KGQA方法的表现。

Result: 实验显示，现有LLM和KGQA方法在涉及常识推理的长尾实体任务中表现不佳。

Conclusion: CoLoTa可作为评估LLMs和KGQA方法常识推理能力的新基准。

Abstract: The rise of Large Language Models (LLMs) has redefined the AI landscape,
particularly due to their ability to encode factual and commonsense knowledge,
and their outstanding performance in tasks requiring reasoning. Despite these
advances, hallucinations and reasoning errors remain a significant barrier to
their deployment in high-stakes settings. In this work, we observe that even
the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning
errors and hallucinations on tasks requiring commonsense reasoning over
obscure, long-tail entities. To investigate this limitation, we present a new
dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that
consists of 3,300 queries from question answering and claim verification tasks
and covers a diverse range of commonsense reasoning skills. We remark that
CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset
since the support of knowledge required to answer its queries is present in the
Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that
merely focus on factoid questions, our CoLoTa queries also require commonsense
reasoning. Our experiments with strong LLM-based KGQA methodologies indicate
their severe inability to answer queries involving commonsense reasoning.
Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM
commonsense reasoning capabilities and their robustness to hallucinations on
long-tail entities and (ii) the commonsense reasoning capabilities of KGQA
methods.

</details>


### [23] [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
*Yijun Liu*

Main category: cs.CL

TL;DR: SSENSE框架通过对比学习将sEEG信号映射到CLIP模型的句子嵌入空间，实现从脑活动直接检索句子。


<details>
  <summary>Details</summary>
Motivation: 探索多模态基础模型在神经科学和AI中的应用，将侵入性脑记录与自然语言对齐。

Method: 使用InfoNCE损失训练神经编码器，将sEEG信号映射到冻结的CLIP文本编码器的嵌入空间。

Result: 在有限数据下，SSENSE展示了通用语言表征可作为神经解码的有效先验。

Conclusion: SSENSE证明了多模态模型在神经解码中的潜力，为脑活动与语言的关联提供了新方法。

Abstract: Interpreting neural activity through meaningful latent representations
remains a complex and evolving challenge at the intersection of neuroscience
and artificial intelligence. We investigate the potential of multimodal
foundation models to align invasive brain recordings with natural language. We
present SSENSE, a contrastive learning framework that projects single-subject
stereo-electroencephalography (sEEG) signals into the sentence embedding space
of a frozen CLIP model, enabling sentence-level retrieval directly from brain
activity. SSENSE trains a neural encoder on spectral representations of sEEG
using InfoNCE loss, without fine-tuning the text encoder. We evaluate our
method on time-aligned sEEG and spoken transcripts from a naturalistic
movie-watching dataset. Despite limited data, SSENSE achieves promising
results, demonstrating that general-purpose language representations can serve
as effective priors for neural decoding.

</details>


### [24] [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
*Xiang Li,Duyi Pan,Hongru Xiao,Jiale Han,Jing Tang,Jiabao Ma,Wei Wang,Bo Cheng*

Main category: cs.CL

TL;DR: 提出了一种基于混合代理的语音合成框架DialogueAgents，通过三个代理协作生成对话，并发布了高质量的双语多轮对话数据集MultiTalk。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成数据集构建成本高且多样性不足，限制了情感表达和上下文场景的丰富性。

Method: 集成脚本编写、语音合成和对话批评三个代理，基于多样化角色池迭代优化对话脚本和语音合成。

Result: 实验证明框架有效，MultiTalk数据集质量高，支持情感表达和副语言特征。

Conclusion: DialogueAgents和MultiTalk为语音合成研究提供了新工具和数据支持。

Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural
and intuitive communication. However, existing datasets involve high
construction costs due to manual annotation and suffer from limited character
diversity, contextual scenarios, and emotional expressiveness. To address these
issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis
framework, which integrates three specialized agents -- a script writer, a
speech synthesizer, and a dialogue critic -- to collaboratively generate
dialogues. Grounded in a diverse character pool, the framework iteratively
refines dialogue scripts and synthesizes speech based on speech review,
boosting emotional expressiveness and paralinguistic features of the
synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a
bilingual, multi-party, multi-turn speech dialogue dataset covering diverse
topics. Extensive experiments demonstrate the effectiveness of our framework
and the high quality of the MultiTalk dataset. We release the dataset and code
https://github.com/uirlx/DialogueAgents to facilitate future research on
advanced speech synthesis models and customized data generation.

</details>


### [25] [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
*Yichen Li,Zhiting Fan,Ruizhe Chen,Xiaotang Gai,Luqi Gong,Yan Zhang,Zuozhu Liu*

Main category: cs.CL

TL;DR: FairSteer是一种无需定制提示或模型重新训练的新型推理时去偏框架，通过检测偏激活、计算去偏转向向量（DSV）和动态激活转向来实现去偏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）容易从训练语料中捕获偏见，现有方法存在不稳定或计算开销大的问题。

Method: FairSteer基于线性表示假设，通过轻量级线性分类器检测偏激活，计算DSV作为干预方向，并在推理阶段动态调整激活。

Result: 在六种LLM上的综合评估表明，FairSteer在问答、反事实输入评估和开放式文本生成任务中表现优越。

Conclusion: FairSteer提供了一种高效且稳定的去偏方法，无需额外训练或复杂提示设计。

Abstract: Large language models (LLMs) are prone to capturing biases from training
corpus, leading to potential negative social impacts. Existing prompt-based
debiasing methods exhibit instability due to their sensitivity to prompt
changes, while fine-tuning-based techniques incur substantial computational
overhead and catastrophic forgetting. In this paper, we propose FairSteer, a
novel inference-time debiasing framework without requiring customized prompt
design or model retraining. Motivated by the linear representation hypothesis,
our preliminary investigation demonstrates that fairness-related features can
be encoded into separable directions in the hidden activation space. FairSteer
operates in three steps: biased activation detection, debiasing steering vector
(DSV) computation, and dynamic activation steering. Specifically, it first
trains a lightweight linear classifier to detect bias signatures in
activations, and then computes DSVs as intervention directions derived from
small contrastive prompt pairs. Subsequently, it performs debiasing by
adjusting activations with DSVs in the inference stage. Comprehensive
evaluation with six LLMs demonstrates the superiority of FairSteer across
question-answering, counterfactual input evaluation and open-ended text
generation tasks. Code will be released.

</details>


### [26] [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）的知识回忆机制，提出了一种功能结构抽象，并通过实验验证了激活向量在知识回忆中的独立功能作用。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs中知识回忆的机制，理解其功能结构。

Method: 设计基于修补的知识评分算法，识别知识感知的激活向量，并通过反知识测试验证其功能独立性。

Result: 通过激活修补改进了上下文知识编辑方法，提升了新知识提示的短期记忆保留能力。

Conclusion: 从功能视角改进了知识回忆机制，为LLMs的知识编辑提供了新思路。

Abstract: Pre-trained transformer large language models (LLMs) demonstrate strong
knowledge recall capabilities. This paper investigates the knowledge recall
mechanism in LLMs by abstracting it into a functional structure. We propose
that during knowledge recall, the model's hidden activation space implicitly
entails a function execution process where specific activation vectors align
with functional components (Input argument, Function body, and Return values).
Specifically, activation vectors of relation-related tokens define a mapping
function from subjects to objects, with subject-related token activations
serving as input arguments and object-related token activations as return
values. For experimental verification, we first design a patching-based
knowledge-scoring algorithm to identify knowledge-aware activation vectors as
independent functional components. Then, we conduct counter-knowledge testing
to examine the independent functional effects of each component on knowledge
recall outcomes. From this functional perspective, we improve the contextual
knowledge editing approach augmented by activation patching. By rewriting
incoherent activations in context, we enable improved short-term memory
retention for new knowledge prompting.

</details>


### [27] [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
*Zhijing Jin*

Main category: cs.CL

TL;DR: 该论文探讨了大语言模型（LLMs）中的因果推理能力，包括其机制、应用及改进方向。


<details>
  <summary>Details</summary>
Motivation: 因果推理是人类智能的核心，也是人工智能系统实现高级理解和决策的关键能力。

Method: 通过一系列研究、新数据集、基准任务和方法框架，分析LLMs的因果推理技能及其在NLP和社会科学中的应用。

Result: 揭示了LLMs在因果推理中的关键挑战和机遇，为未来研究提供了基础。

Conclusion: 该研究为提升LLMs的因果推理能力提供了全面的框架和方向。

Abstract: Causal reasoning is a cornerstone of human intelligence and a critical
capability for artificial systems aiming to achieve advanced understanding and
decision-making. This thesis delves into various dimensions of causal reasoning
and understanding in large language models (LLMs). It encompasses a series of
studies that explore the causal inference skills of LLMs, the mechanisms behind
their performance, and the implications of causal and anticausal learning for
natural language processing (NLP) tasks. Additionally, it investigates the
application of causal reasoning in text-based computational social science,
specifically focusing on political decision-making and the evaluation of
scientific impact through citations. Through novel datasets, benchmark tasks,
and methodological frameworks, this work identifies key challenges and
opportunities to improve the causal capabilities of LLMs, providing a
comprehensive foundation for future research in this evolving field.

</details>


### [28] [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
*Yiting Ran,Xintao Wang,Tian Qiu,Jiaqing Liang,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TL;DR: 论文介绍了BookWorld系统，用于构建和模拟基于书籍的多智能体社会，支持故事生成、互动游戏等应用，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 模拟已建立的虚构世界和角色具有重要实践价值，但此前研究较少。

Method: 设计BookWorld系统，涵盖角色多样性、世界观、地理约束等现实复杂性。

Result: BookWorld在保持对原著忠实度的同时生成高质量故事，胜率为75.36%。

Conclusion: BookWorld为探索和扩展虚构作品提供了新途径，效果显著。

Abstract: Recent advances in large language models (LLMs) have enabled social
simulation through multi-agent systems. Prior efforts focus on agent societies
created from scratch, assigning agents with newly defined personas. However,
simulating established fictional worlds and characters remain largely
underexplored, despite its significant practical value. In this paper, we
introduce BookWorld, a comprehensive system for constructing and simulating
book-based multi-agent societies. BookWorld's design covers comprehensive
real-world intricacies, including diverse and dynamic characters, fictional
worldviews, geographical constraints and changes, e.t.c. BookWorld enables
diverse applications including story generation, interactive games and social
simulation, offering novel ways to extend and explore beloved fictional works.
Through extensive experiments, we demonstrate that BookWorld generates
creative, high-quality stories while maintaining fidelity to the source books,
surpassing previous methods with a win rate of 75.36%. The code of this paper
can be found at the project page: https://bookworld2025.github.io/.

</details>


### [29] [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Baolong Bi,Yuyao Ge,Jun Wan,Yurong Wu,Xueqi Cheng*

Main category: cs.CL

TL;DR: 论文提出了一种名为环境增强生成（EAG）的框架，通过实时环境反馈、动态分支探索和经验学习，提升大语言模型（LLMs）的推理能力，解决了幻觉和逻辑错误问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理方面取得突破，但仍存在幻觉、逻辑错误和无法自我纠正的问题，现有方法（如思维链提示）在需要精确验证时表现不足。

Method: EAG框架通过实时环境反馈验证推理步骤、动态分支探索替代路径，以及从成功推理轨迹中学习，实现回溯和重新规划。

Result: a1-32B模型在同类模型中表现最佳，匹配更大模型（如o1）的数学竞赛表现，并超出可比模型24.4个百分点。

Conclusion: EAG通过环境交互和分支探索，为需要多步计算和逻辑验证的问题建立了可靠的机器推理新范式。

Abstract: Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,
yet continue to struggle with hallucinations, logical errors, and inability to
self-correct during complex multi-step tasks. Current approaches like
chain-of-thought prompting offer limited reasoning capabilities that fail when
precise step validation is required. We propose Environment Augmented
Generation (EAG), a framework that enhances LLM reasoning through: (1)
real-time environmental feedback validating each reasoning step, (2) dynamic
branch exploration for investigating alternative solution paths when faced with
errors, and (3) experience-based learning from successful reasoning
trajectories. Unlike existing methods, EAG enables deliberate backtracking and
strategic replanning through tight integration of execution feedback with
branching exploration. Our a1-32B model achieves state-of-the-art performance
among similar-sized models across all benchmarks, matching larger models like
o1 on competition mathematics while outperforming comparable models by up to
24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:
initial token investment in environment interaction yields substantial
long-term performance dividends, with advantages amplifying proportionally to
task complexity. EAG's theoretical framework demonstrates how environment
interactivity and systematic branch exploration together establish a new
paradigm for reliable machine reasoning, particularly for problems requiring
precise multi-step calculation and logical verification.

</details>


### [30] [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
*Yuri Balashov,Alex Balashov,Shiho Fukuda Koski*

Main category: cs.CL

TL;DR: 论文探讨了语言技术进步为个体译者和语言服务提供商带来的新机遇，重点介绍了如何将自动评估指标（如BLEU、chrF、TER和COMET）适应自由译者的需求。


<details>
  <summary>Details</summary>
Motivation: 研究旨在帮助自由译者利用新兴语言技术（如神经机器翻译和大语言模型）提升翻译质量和效率，适应行业变革。

Method: 提出了一种将自动评估指标（如BLEU、chrF、TER和COMET）应用于自由译者的实用框架，并通过医学领域的三语语料库进行验证。

Result: 统计分析显示自动评分与人工评估之间存在相关性，证明了这些指标对自由译者的实用性。

Conclusion: 自由译者应积极采用新兴技术，以在快速变化的专业环境中适应和发展。

Abstract: This is the first in a series of papers exploring the rapidly expanding new
opportunities arising from recent progress in language technologies for
individual translators and language service providers with modest resources.
The advent of advanced neural machine translation systems, large language
models, and their integration into workflows via computer-assisted translation
tools and translation management systems have reshaped the translation
landscape. These advancements enable not only translation but also quality
evaluation, error spotting, glossary generation, and adaptation to
domain-specific needs, creating new technical opportunities for freelancers. In
this series, we aim to empower translators with actionable methods to harness
these advancements. Our approach emphasizes Translation Analytics, a suite of
evaluation techniques traditionally reserved for large-scale industry
applications but now becoming increasingly available for smaller-scale users.
This first paper introduces a practical framework for adapting automatic
evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'
needs. We illustrate the potential of these metrics using a trilingual corpus
derived from a real-world project in the medical domain and provide statistical
analysis correlating human evaluations with automatic scores. Our findings
emphasize the importance of proactive engagement with emerging technologies to
not only adapt but thrive in the evolving professional environment.

</details>


### [31] [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
*Hongming Tan,Shaoxiong Zhan,Fengwei Jia,Hai-Tao Zheng,Wai Kin Chan*

Main category: cs.CL

TL;DR: HSPIM是一种基于大型语言模型的分层框架，通过分解论文为部分和问答对来评估创新性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于内容的方法无法全面捕捉论文创新性且缺乏泛化能力，因此需要一种更有效的方法。

Method: HSPIM通过分段、零样本LLM提示、QA增强和加权评分来评估创新性，并优化问题提示组合。

Result: HSPIM在科学会议论文数据集上表现优于基线方法，具有更高的有效性、泛化性和可解释性。

Conclusion: HSPIM为科学论文创新性评估提供了一种高效且可解释的解决方案。

Abstract: Measuring scientific paper innovation is both important and challenging.
Existing content-based methods often overlook the full-paper context, fail to
capture the full scope of innovation, and lack generalization. We propose
HSPIM, a hierarchical and training-free framework based on large language
models (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess
innovation. We segment the text by section titles and use zero-shot LLM
prompting to implement section classification, question-answering (QA)
augmentation, and weighted novelty scoring. The generated QA pair focuses on
section-level innovation and serves as additional context to improve the LLM
scoring. For each chunk, the LLM outputs a novelty score and a confidence
score. We use confidence scores as weights to aggregate novelty scores into a
paper-level innovation score. To further improve performance, we propose a
two-layer question structure consisting of common and section-specific
questions, and apply a genetic algorithm to optimize the question-prompt
combinations. Comprehensive experiments on scientific conference paper datasets
show that HSPIM outperforms baseline methods in effectiveness, generalization,
and interpretability.

</details>


### [32] [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
*Rondik Hadi Abdulrahman,Hossein Hassani*

Main category: cs.CL

TL;DR: 该研究为库尔德语（Sorani方言）开发了一个数据集和语言模型，用于自动文本摘要（ATS），填补了该语言的资源空白。


<details>
  <summary>Details</summary>
Motivation: 库尔德语在自动文本摘要领域资源匮乏，限制了相关研究的发展。

Method: 研究基于231篇库尔德语科学论文，使用句子加权和TF-IDF算法进行实验，并通过人工和自动（ROUGE指标）评估结果。

Result: 最佳准确率达到19.58%，实验结果因文档而异。

Conclusion: 该研究为库尔德语NLP研究者提供了宝贵资源，推动了ATS及相关领域的发展。

Abstract: Extracting concise information from scientific documents aids learners,
researchers, and practitioners. Automatic Text Summarization (ATS), a key
Natural Language Processing (NLP) application, automates this process. While
ATS methods exist for many languages, Kurdish remains underdeveloped due to
limited resources. This study develops a dataset and language model based on
231 scientific papers in Sorani Kurdish, collected from four academic
departments in two universities in the Kurdistan Region of Iraq (KRI),
averaging 26 pages per document. Using Sentence Weighting and Term
Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were
conducted, differing in whether the conclusions were included. The average word
count was 5,492.3 in the first experiment and 5,266.96 in the second. Results
were evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L
metrics, with the best accuracy reaching 19.58%. Six experts conducted manual
evaluations using three criteria, with results varying by document. This
research provides valuable resources for Kurdish NLP researchers to advance ATS
and related fields.

</details>


### [33] [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
*Soo-joon Choi,Ji-jun Park*

Main category: cs.CL

TL;DR: 论文提出了一种基于生成式大语言模型（LLM）的金融事件实体抽取方法，通过文本到结构化输出的生成任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统序列标注模型在处理金融文本中的长距离依赖和多实体重叠时表现不佳，而生成式LLM具有更强的语言理解和生成能力，适合解决这一问题。

Method: 采用参数高效微调（PEFT）对预训练LLM进行微调，直接生成包含实体及其字符跨度的结构化输出（如JSON）。

Result: 在CCKS 2019数据集上，该方法实现了新的最高F1分数，显著优于传统序列标注模型。

Conclusion: 生成式LLM在复杂领域特定信息抽取任务中具有潜力，尤其适用于需要结构化输出的场景。

Abstract: Financial event entity extraction is a crucial task for analyzing market
dynamics and building financial knowledge graphs, yet it presents significant
challenges due to the specialized language and complex structures in financial
texts. Traditional approaches often rely on sequence labeling models, which can
struggle with long-range dependencies and the inherent complexity of extracting
multiple, potentially overlapping entities. Motivated by the advanced language
understanding and generative capabilities of Large Language Models (LLMs), we
propose a novel method that reframes financial event entity extraction as a
text-to-structured-output generation task. Our approach involves fine-tuning a
pre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly
generate a structured representation, such as a JSON object, containing the
extracted entities and their precise character spans from the input text. We
evaluate our method on the challenging CCKS 2019 Financial Event Entity
Extraction dataset, comparing its performance against strong sequence labeling
baselines, including SEBERTNets and sebertNets. Experimental results
demonstrate that our generative LLM method achieves a new state-of-the-art F1
score on this benchmark, significantly outperforming previous methods. Through
detailed quantitative analysis across event types, entity types, and instance
complexity, as well as human evaluation, we show that our approach is more
effective at handling the nuances of financial text and extracting high-quality
entities. This work validates the potential of applying generative LLMs
directly to complex, domain-specific information extraction tasks requiring
structured output.

</details>


### [34] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
*Yihan Lin,Zhirong Bella Yu,Simon Lee*

Main category: cs.CL

TL;DR: LLMs生成合成电子健康记录（EHRs）在小规模特征上表现可靠，但随着数据维度增加，难以保持真实分布和相关性，限制了跨医院泛化能力。


<details>
  <summary>Details</summary>
Motivation: 合成EHRs能保护隐私并支持医疗应用，但需解决跨医院泛化的长期问题。

Method: 评估商用LLMs生成合成数据的能力，分析生成过程的多个方面。

Result: LLMs在小规模特征上可靠，但高维度数据中分布和相关性保持不足。

Conclusion: LLMs在合成EHRs中有潜力，但需改进以应对高维度数据的泛化挑战。

Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to
create privacy preserving and harmonized structured data, supporting numerous
applications in healthcare. Key benefits of synthetic data include precise
control over the data schema, improved fairness and representation of patient
populations, and the ability to share datasets without concerns about
compromising real individuals privacy. Consequently, the AI community has
increasingly turned to Large Language Models (LLMs) to generate synthetic data
across various domains. However, a significant challenge in healthcare is
ensuring that synthetic health records reliably generalize across different
hospitals, a long standing issue in the field. In this work, we evaluate the
current state of commercial LLMs for generating synthetic data and investigate
multiple aspects of the generation process to identify areas where these models
excel and where they fall short. Our main finding from this work is that while
LLMs can reliably generate synthetic health records for smaller subsets of
features, they struggle to preserve realistic distributions and correlations as
the dimensionality of the data increases, ultimately limiting their ability to
generalize across diverse hospital settings.

</details>


### [35] [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
*Wei Zou,Sen Yang,Yu Bao,Shujian Huang,Jiajun Chen,Shanbo Cheng*

Main category: cs.CL

TL;DR: TRANS-ZERO是一个利用单语数据和LLM内在多语言知识的自博弈框架，通过G-MCTS和偏好优化实现与监督方法媲美的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言机器翻译中低资源语言数据稀缺和灾难性遗忘的问题。

Method: 结合遗传蒙特卡洛树搜索（G-MCTS）和偏好优化，仅使用单语数据和LLM的多语言知识。

Result: 在非英语翻译方向上表现优异，性能与基于大规模平行数据训练的方法相当。

Conclusion: G-MCTS通过迭代翻译探索语义一致的候选，显著提升翻译质量，为框架成功奠定基础。

Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation
(MT), but multilingual MT still relies heavily on parallel data for supervised
fine-tuning (SFT), facing challenges like data scarcity for low-resource
languages and catastrophic forgetting. To address these issues, we propose
TRANS-ZERO, a self-play framework that leverages only monolingual data and the
intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic
Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong
translation performance that rivals supervised methods. Experiments demonstrate
that this approach not only matches the performance of models trained on
large-scale parallel data but also excels in non-English translation
directions. Further analysis reveals that G-MCTS itself significantly enhances
translation quality by exploring semantically consistent candidates through
iterative translations, providing a robust foundation for the framework's
succuss.

</details>


### [36] [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
*Mehrnoush Shamsfard,Zahra Saaberi,Mostafa Karimi manesh,Seyed Mohammad Hossein Hashemi,Zahra Vatankhah,Motahareh Ramezani,Niki Pourazin,Tara Zare,Maryam Azimi,Sarina Chitsaz,Sama Khoraminejad,Morteza Mahdavi Mortazavi,Mohammad Mahdi Chizari,Sahar Maleki,Seyed Soroush Majd,Mostafa Masumi,Sayed Ali Musavi Khoeini,Amir Mohseni,Sogol Alipour*

Main category: cs.CL

TL;DR: 论文介绍了FarsEval-PKBETS基准，用于评估波斯语大型语言模型的性能，结果显示当前模型的平均准确率低于50%。


<details>
  <summary>Details</summary>
Motivation: 研究波斯语等资源较少语言的LLM性能，填补现有研究的空白。

Method: 开发包含4000个问题的FarsEval-PKBETS基准，涵盖多领域和多任务格式。

Result: Llama3-70B、PersianMind和Dorna模型的平均准确率低于50%。

Conclusion: 当前LLM在波斯语任务上仍有较大提升空间。

Abstract: Research on evaluating and analyzing large language models (LLMs) has been
extensive for resource-rich languages such as English, yet their performance in
languages such as Persian has received considerably less attention. This paper
introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for
evaluating large language models in Persian. This benchmark consists of 4000
questions and answers in various formats, including multiple choice, short
answer and descriptive responses. It covers a wide range of domains and
tasks,including medicine, law, religion, Persian language, encyclopedic
knowledge, human preferences, social knowledge, ethics and bias, text
generation, and respecting others' rights. This bechmark incorporates
linguistics, cultural, and local considerations relevant to the Persian
language and Iran. To ensure the questions are challenging for current LLMs,
three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this
benchmark. Their average accuracy was below 50%, meaning they provided fully
correct answers to fewer than half of the questions. These results indicate
that current language models are still far from being able to solve this
benchmark

</details>


### [37] [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
*Songtao Jiang,Yuan Wang,Sibo Song,Yan Zhang,Zijie Meng,Bohan Lei,Jian Wu,Jimeng Sun,Zuozhu Liu*

Main category: cs.CL

TL;DR: OmniV-Med提出了一种统一的医学视觉-语言模型框架，通过多模态数据集、自适应编码器和令牌修剪机制，显著提升了性能并降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型通常对不同模态使用独立编码器，限制了多模态数据的无缝整合。

Method: 构建了OmniV-Med-Instruct数据集，设计了旋转位置自适应编码器，并引入医学感知令牌修剪机制。

Result: OmniV-Med-7B在7个基准测试中达到最优性能，轻量级版本（1.5B）仅需8块RTX3090 GPU即可训练。

Conclusion: OmniV-Med为多模态医学理解提供了高效、统一的解决方案，具有实际部署潜力。

Abstract: The practical deployment of medical vision-language models (Med-VLMs)
necessitates seamless integration of textual data with diverse visual
modalities, including 2D/3D images and videos, yet existing models typically
employ separate encoders for different modalities. To address this limitation,
we present OmniV-Med, a unified framework for multimodal medical understanding.
Our technical contributions are threefold: First, we construct
OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K
instructional samples spanning 14 medical image modalities and 11 clinical
tasks. Second, we devise a rotary position-adaptive encoder that processes
multi-resolution 2D/3D images and videos within a unified architecture,
diverging from conventional modality-specific encoders. Third, we introduce a
medical-aware token pruning mechanism that exploits spatial-temporal redundancy
in volumetric data (e.g., consecutive CT slices) and medical videos,
effectively reducing 60\% of visual tokens without performance degradation.
Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art
performance on 7 benchmarks spanning 2D/3D medical imaging and video
understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains
comparable performance while requiring only 8 RTX3090 GPUs for training and
supporting efficient long-video inference. Data, code and model will be
released.

</details>


### [38] [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 比较BERTopic、LDA和KMeans在比利时荷兰语日常叙事中的表现，发现BERTopic在语义相关性上优于LDA和KMeans，强调混合评估框架的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索BERTopic在形态丰富的语言（如比利时荷兰语）中的潜力，并对比其与LDA和KMeans的性能差异。

Method: 使用BERTopic、LDA和KMeans对开放式的比利时荷兰语日常叙事进行建模，并通过自动指标和人工评估对比性能。

Result: BERTopic生成的语义主题更具文化共鸣，而LDA在自动指标上表现较好但语义相关性不足，KMeans表现不如预期。

Conclusion: 研究强调NLP模型在非主流语言环境中需要更强的泛化能力，并推荐采用混合评估框架。

Abstract: This study explores BERTopic's potential for modeling open-ended Belgian
Dutch daily narratives, contrasting its performance with Latent Dirichlet
Allocation (LDA) and KMeans. Although LDA scores well on certain automated
metrics, human evaluations reveal semantically irrelevant co-occurrences,
highlighting the limitations of purely statistic-based methods. In contrast,
BERTopic's reliance on contextual embeddings yields culturally resonant themes,
underscoring the importance of hybrid evaluation frameworks that account for
morphologically rich languages. KMeans performed less coherently than prior
research suggested, pointing to the unique challenges posed by personal
narratives. Our findings emphasize the need for robust generalization in NLP
models, especially in underrepresented linguistic contexts.

</details>


### [39] [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
*Reya Vir,Shreya Shankar,Harrison Chase,Will Fu-Hinthorn,Aditya Parameswaran*

Main category: cs.CL

TL;DR: PROMPTEVALS是一个包含2087个LLM管道提示和12623个断言标准的数据集，用于提高LLM在生成输出时的可靠性。微调的Mistral和Llama 3模型在生成相关断言方面优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中常未能满足开发者期望，需要改进其可靠性。

Method: 通过PROMPTEVALS数据集，评估闭源和开源模型生成断言的能力。

Result: 微调的Mistral和Llama 3模型比GPT-4o平均性能提升20.93%，且延迟更低。

Conclusion: PROMPTEVALS数据集可推动LLM可靠性、对齐和提示工程的研究。

Abstract: Large language models (LLMs) are increasingly deployed in specialized
production data processing pipelines across diverse domains -- such as finance,
marketing, and e-commerce. However, when running them in production across many
inputs, they often fail to follow instructions or meet developer expectations.
To improve reliability in these applications, creating assertions or guardrails
for LLM outputs to run alongside the pipelines is essential. Yet, determining
the right set of assertions that capture developer requirements for a task is
challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM
pipeline prompts with 12623 corresponding assertion criteria, sourced from
developers using our open-source LLM pipeline tools. This dataset is 5x larger
than previous collections. Using a hold-out test split of PROMPTEVALS as a
benchmark, we evaluated closed- and open-source models in generating relevant
assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform
GPT-4o by 20.93% on average, offering both reduced latency and improved
performance. We believe our dataset can spur further research in LLM
reliability, alignment, and prompt engineering.

</details>


### [40] [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
*Saniya Karwa,Navpreet Singh*

Main category: cs.CL

TL;DR: 论文提出了一种分析BERT嵌入中编码语言属性的框架，通过LDSP-10数据集和多种方法识别关键维度，并引入EDI评分量化维度重要性。


<details>
  <summary>Details</summary>
Motivation: 理解BERT等高维不透明模型的嵌入机制，揭示其编码的语言属性。

Method: 使用LDSP-10数据集，结合Wilcoxon检验、互信息和递归特征消除等方法分析嵌入维度，提出EDI评分。

Result: 发现否定和极性等属性在特定维度中编码明显，而同义词等属性更复杂。

Conclusion: 研究为嵌入的可解释性提供了见解，有助于开发更透明的语言模型，减少偏见并促进AI负责任部署。

Abstract: Understanding the inner workings of neural embeddings, particularly in models
such as BERT, remains a challenge because of their high-dimensional and opaque
nature. This paper proposes a framework for uncovering the specific dimensions
of vector embeddings that encode distinct linguistic properties (LPs). We
introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which
isolates ten key linguistic features such as synonymy, negation, tense, and
quantity. Using this dataset, we analyze BERT embeddings with various methods,
including the Wilcoxon signed-rank test, mutual information, and recursive
feature elimination, to identify the most influential dimensions for each LP.
We introduce a new metric, the Embedding Dimension Impact (EDI) score, which
quantifies the relevance of each embedding dimension to a LP. Our findings show
that certain properties, such as negation and polarity, are robustly encoded in
specific dimensions, while others, like synonymy, exhibit more complex
patterns. This study provides insights into the interpretability of embeddings,
which can guide the development of more transparent and optimized language
models, with implications for model bias mitigation and the responsible
deployment of AI systems.

</details>


### [41] [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
*Luyang Fang,Xiaowei Yu,Jiazhang Cai,Yongkai Chen,Shushan Wu,Zhengliang Liu,Zhenyuan Yang,Haoran Lu,Xilin Gong,Yufang Liu,Terry Ma,Wei Ruan,Ali Abbasi,Jing Zhang,Tao Wang,Ehsan Latif,Wei Liu,Wei Zhang,Soheil Kolouri,Xiaoming Zhai,Dajiang Zhu,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.CL

TL;DR: 本文综述了知识蒸馏（KD）和数据集蒸馏（DD）两种互补范式，旨在压缩大型语言模型（LLMs）同时保留其推理能力和语言多样性。通过整合KD和DD，提出了更高效的压缩策略，并探讨了其在医疗和教育等领域的应用。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，计算和数据需求激增，亟需高效压缩策略以保持模型性能。

Method: 分析了KD中的任务对齐、基于理性的训练和多教师框架，以及DD中的梯度匹配、潜在空间正则化和生成合成技术。

Result: 整合KD和DD可解决模型扩展性、架构异质性和能力保留等挑战，并在实际应用中实现高效部署。

Conclusion: 尽管取得进展，仍需解决推理多样性、适应性和评估协议等问题，未来需进一步整合KD和DD以实现可持续的LLMs。

Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight
the need for efficient strategies to meet ever-expanding computational and data
demands. This survey provides a comprehensive analysis of two complementary
paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both
aimed at compressing LLMs while preserving their advanced reasoning
capabilities and linguistic diversity. We first examine key methodologies in
KD, such as task-specific alignment, rationale-based training, and
multi-teacher frameworks, alongside DD techniques that synthesize compact,
high-impact datasets through optimization-based gradient matching, latent space
regularization, and generative synthesis. Building on these foundations, we
explore how integrating KD and DD can produce more effective and scalable
compression strategies. Together, these approaches address persistent
challenges in model scalability, architectural heterogeneity, and the
preservation of emergent LLM abilities. We further highlight applications
across domains such as healthcare and education, where distillation enables
efficient deployment without sacrificing performance. Despite substantial
progress, open challenges remain in preserving emergent reasoning and
linguistic diversity, enabling efficient adaptation to continually evolving
teacher models and datasets, and establishing comprehensive evaluation
protocols. By synthesizing methodological innovations, theoretical foundations,
and practical insights, our survey charts a path toward sustainable,
resource-efficient LLMs through the tighter integration of KD and DD
principles.

</details>


### [42] [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
*Jiaxin GUO,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Zongyao Li,Hengchao Shang,Daimeng Wei,Hao Yang*

Main category: cs.CL

TL;DR: 本文探讨了文档级机器翻译的自动评估现状、挑战及未来趋势，强调了评估的重要性及现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 文档级翻译质量评估是当前急需解决的问题，自动评估指标对提升翻译系统至关重要。

Method: 分析了基于参考文本和无参考文本的评估方法，包括传统指标、模型指标和LLM指标。

Result: 指出了现有方法的挑战，如参考多样性不足、依赖句子级对齐信息以及LLM评估的偏见和不准确性。

Conclusion: 展望了未来趋势，如开发更友好的文档级评估方法和更稳健的LLM评估方法，并提出了研究方向。

Abstract: With the rapid development of deep learning technologies, the field of
machine translation has witnessed significant progress, especially with the
advent of large language models (LLMs) that have greatly propelled the
advancement of document-level translation. However, accurately evaluating the
quality of document-level translation remains an urgent issue. This paper first
introduces the development status of document-level translation and the
importance of evaluation, highlighting the crucial role of automatic evaluation
metrics in reflecting translation quality and guiding the improvement of
translation systems. It then provides a detailed analysis of the current state
of automatic evaluation schemes and metrics, including evaluation methods with
and without reference texts, as well as traditional metrics, Model-based
metrics and LLM-based metrics. Subsequently, the paper explores the challenges
faced by current evaluation methods, such as the lack of reference diversity,
dependence on sentence-level alignment information, and the bias, inaccuracy,
and lack of interpretability of the LLM-as-a-judge method. Finally, the paper
looks ahead to the future trends in evaluation methods, including the
development of more user-friendly document-level evaluation methods and more
robust LLM-as-a-judge methods, and proposes possible research directions, such
as reducing the dependency on sentence-level information, introducing
multi-level and multi-granular evaluation approaches, and training models
specifically for machine translation evaluation. This study aims to provide a
comprehensive analysis of automatic evaluation for document-level translation
and offer insights into future developments.

</details>


### [43] [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
*Mario M. Kubek,Shiraj Pokharel,Thomas Böhme,Emma L. McDaniel,Herwig Unger,Armin R. Mikler*

Main category: cs.CL

TL;DR: 提出了一种快速优化预训练静态词嵌入的方法，通过结合邻近词嵌入更新表示，解决词汇外问题，适用于特定领域语料。


<details>
  <summary>Details</summary>
Motivation: 解决预训练词嵌入在特定领域语料中的不足，尤其是词汇外问题和通用嵌入的局限性。

Method: 利用邻近词嵌入动态更新词表示，不依赖大型语言模型或浅层神经网络。

Result: 在特定领域（如风暴事件）中生成更有意义的词嵌入，提升语料探索和词义消歧效果。

Conclusion: 该方法在领域特定语料中显著优化词嵌入，为灾害叙事等应用提供更深入的洞察。

Abstract: This article introduces a novel and fast method for refining pre-trained
static word or, more generally, token embeddings. By incorporating the
embeddings of neighboring tokens in text corpora, it continuously updates the
representation of each token, including those without pre-assigned embeddings.
This approach effectively addresses the out-of-vocabulary problem, too.
Operating independently of large language models and shallow neural networks,
it enables versatile applications such as corpus exploration, conceptual
search, and word sense disambiguation. The method is designed to enhance token
representations within topically homogeneous corpora, where the vocabulary is
restricted to a specific domain, resulting in more meaningful embeddings
compared to general-purpose pre-trained vectors. As an example, the methodology
is applied to explore storm events and their impacts on infrastructure and
communities using narratives from a subset of the NOAA Storm Events database.
The article also demonstrates how the approach improves the representation of
storm-related terms over time, providing valuable insights into the evolving
nature of disaster narratives.

</details>


### [44] [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
*Jiajun Shen,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合外部和内部知识的引文生成任务，并设计了RAEL范式和INTRALIGN方法，实验表明其方法在跨场景性能上优于基线。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成和引文生成可以缓解大语言模型的幻觉问题，但其内部知识利用仍不透明，生成答案的可信度存疑。

Method: 引入Context-Prior Augmented Citation Generation任务，结合外部和内部知识生成引文，提出RAEL范式和INTRALIGN方法（包含数据生成和对齐算法）。

Result: 实验结果显示该方法在跨场景性能上优于基线，且检索质量、问题类型和模型知识对引文生成的可信度有显著影响。

Conclusion: 论文通过结合内外知识的方法提升了引文生成的可信度和性能，揭示了影响可信度的关键因素。

Abstract: While hallucinations of large language models could been alleviated through
retrieval-augmented generation and citation generation, how the model utilizes
internal knowledge is still opaque, and the trustworthiness of its generated
answers remains questionable. In this work, we introduce Context-Prior
Augmented Citation Generation task, requiring models to generate citations
considering both external and internal knowledge while providing trustworthy
references, with 5 evaluation metrics focusing on 3 aspects: answer
helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the
paradigm for our task, and also design INTRALIGN, an integrated method
containing customary data generation and an alignment algorithm. Our
experimental results show that our method achieves a better cross-scenario
performance with regard to other baselines. Our extended experiments further
reveal that retrieval quality, question types, and model knowledge have
considerable influence on the trustworthiness in citation generation.

</details>


### [45] [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
*Teppei Suzuki,Ryokan Ri,Sho Takase*

Main category: cs.CL

TL;DR: 研究发现，即使大型语言模型（LLM）在相同数据上训练，其生成的文本仍可区分来源模型，这些特征称为“自然指纹”。这些指纹源于训练过程中的细微差异，如参数大小、优化设置和随机种子。


<details>
  <summary>Details</summary>
Motivation: 探究LLM输出中可识别特征的成因，以理解偏见的来源并改进对模型行为的控制。

Method: 通过系统控制训练条件（如参数大小、优化设置、随机种子），分析不同LLM生成的文本特征。

Result: 发现训练过程中的细微差异会导致“自然指纹”，即使训练数据相同，模型生成的文本仍可区分。

Conclusion: 理解“自然指纹”有助于揭示偏见的起源，并为改进LLM行为控制提供新思路。

Abstract: Large language models (LLMs) often exhibit biases -- systematic deviations
from expected norms -- in their outputs. These range from overt issues, such as
unfair responses, to subtler patterns that can reveal which model produced
them. We investigate the factors that give rise to identifiable characteristics
in LLMs. Since LLMs model training data distribution, it is reasonable that
differences in training data naturally lead to the characteristics. However,
our findings reveal that even when LLMs are trained on the exact same data, it
is still possible to distinguish the source model based on its generated text.
We refer to these unintended, distinctive characteristics as natural
fingerprints. By systematically controlling training conditions, we show that
the natural fingerprints can emerge from subtle differences in the training
process, such as parameter sizes, optimization settings, and even random seeds.
We believe that understanding natural fingerprints offers new insights into the
origins of unintended bias and ways for improving control over LLM behavior.

</details>


### [46] [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
*Aoran Gan,Hao Yu,Kai Zhang,Qi Liu,Wenyu Yan,Zhenya Huang,Shiwei Tong,Guoping Hu*

Main category: cs.CL

TL;DR: 本文综述了检索增强生成（RAG）系统的评估方法，总结了传统与新兴评估方法，并分析了数据集和框架。


<details>
  <summary>Details</summary>
Motivation: RAG系统结合了检索与生成组件，评估面临独特挑战，需系统梳理现有方法以推动发展。

Method: 系统回顾RAG评估方法，包括性能、准确性、安全性和效率，并进行元分析。

Result: 总结了RAG评估的数据集和框架，填补了传统与LLM驱动方法间的空白。

Conclusion: 本文为RAG评估提供了全面资源，有助于推动该领域的发展。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have
revolutionized natural language processing by integrating Large Language Models
(LLMs) with external information retrieval, enabling accurate, up-to-date, and
verifiable text generation across diverse applications. However, evaluating RAG
systems presents unique challenges due to their hybrid architecture that
combines retrieval and generation components, as well as their dependence on
dynamic knowledge sources in the LLM era. In response, this paper provides a
comprehensive survey of RAG evaluation methods and frameworks, systematically
reviewing traditional and emerging evaluation approaches, for system
performance, factual accuracy, safety, and computational efficiency in the LLM
era. We also compile and categorize the RAG-specific datasets and evaluation
frameworks, conducting a meta-analysis of evaluation practices in high-impact
RAG research. To the best of our knowledge, this work represents the most
comprehensive survey for RAG evaluation, bridging traditional and LLM-driven
methods, and serves as a critical resource for advancing RAG development.

</details>


### [47] [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
*Yingming Zheng,Xiaoliang Liu,Peng Wu,Li Pan*

Main category: cs.CL

TL;DR: CRAVE提出了一种基于冲突推理的可解释性声明验证方法，利用大语言模型（LLMs）生成对立理性，并通过三模块框架提高复杂声明的验证准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 数字媒体和AI生成内容导致错误信息快速传播，传统依赖专家标注证据的方法效率低且难以扩展，现有自动化系统对复杂声明的处理仍有不足。

Method: CRAVE采用三模块框架：1) 消除歧义并检索证据；2) 利用LLMs从四个维度推理对立理性并初步判断；3) 微调小语言模型（SLM）评估理性置信度并最终判断。

Result: 在两个公开数据集上，CRAVE表现优于现有方法，证据检索和模型解释能力显著提升。

Conclusion: CRAVE通过冲突推理和模块化设计，显著提升了复杂声明的验证效果，同时增强了透明度和可解释性。

Abstract: The rapid spread of misinformation, driven by digital media and AI-generated
content, has made automatic claim verification essential. Traditional methods,
which depend on expert-annotated evidence, are labor-intensive and not
scalable. Although recent automated systems have improved, they still struggle
with complex claims that require nuanced reasoning. To address this, we propose
CRAVE, a Conflicting Reasoning Approach for explainable claim VErification,
that verify the complex claims based on the conflicting rationales reasoned by
large language models (LLMs). Specifically, CRAVE introduces a three-module
framework. Ambiguity Elimination enchanced Evidence Retrieval module performs
ambiguity elimination and entity-based search to gather relevant evidence
related to claim verification from external sources like Wikipedia. Conflicting
Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to
reason rationales with conflicting stances about claim verification from
retrieved evidence across four dimensions, i.e., direct evidence, semantic
relationships, linguistic patterns, and logical reasoning and make a
preliminary judgment. Finally, Small Language Model (SLM) based Judge module is
fine-tuned to make use of preliminary judgment from LLMs to assess the
confidence of the conflicting rationales and make a final authenticity
judgment. This methodology allows CRAVE to capture subtle inconsistencies in
complex claims, improving both the accuracy and transparency of claim
verification. Extensive experiments on two public claim verification datasets
demonstrate that our CRAVE model achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for finding relevant
evidence and explaining the model predictions. The code is provided at
https://github.com/8zym/CRAVE.

</details>


### [48] [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
*Rui Ribeiro,Luísa Coheur,Joao P. Carvalho*

Main category: cs.CL

TL;DR: 本文提出了一种基于模糊指纹和大预训练模型的文本说话人识别方法，结合说话人特定标记和上下文感知建模，显著提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在仅依赖文本数据时难以有效识别说话人，因此探索新方法以提升文本说话人识别的准确性和实用性。

Method: 采用模糊指纹技术和大预训练模型，结合说话人特定标记和上下文感知建模，优化文本说话人识别。

Result: 在Friends数据集上准确率达到70.6%，在Big Bang Theory数据集上达到67.7%，模糊指纹还能以较少的隐藏单元接近全微调性能。

Conclusion: 研究揭示了文本说话人识别的关键挑战，并提出了改进方向，为未来研究提供了重要参考。

Abstract: Speaker identification using voice recordings leverages unique acoustic
features, but this approach fails when only textual data is available. Few
approaches have attempted to tackle the problem of identifying speakers solely
from text, and the existing ones have primarily relied on traditional methods.
In this work, we explore the use of fuzzy fingerprints from large pre-trained
models to improve text-based speaker identification. We integrate
speaker-specific tokens and context-aware modeling, demonstrating that
conversational context significantly boosts accuracy, reaching 70.6% on the
Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show
that fuzzy fingerprints can approximate full fine-tuning performance with fewer
hidden units, offering improved interpretability. Finally, we analyze ambiguous
utterances and propose a mechanism to detect speaker-agnostic lines. Our
findings highlight key challenges and provide insights for future improvements
in text-based speaker identification.

</details>


### [49] [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
*Xiaodong Yang*

Main category: cs.CL

TL;DR: 提出一个评估大语言模型（LLMs）对中文话题结构敏感性的框架，重点关注其对孤岛限制的敏感性。


<details>
  <summary>Details</summary>
Motivation: 受Tian等人（2024）启发，旨在为未来研究提供基础，并邀请对方法论的反馈。

Method: 设计实验测试LLMs对汉语语法的知识，尚未进行实验。

Result: 暂无实验结果，仅为提案。

Conclusion: 为未来研究提供框架，并寻求方法论改进建议。

Abstract: This paper proposes a framework for evaluating large language models (LLMs)
on Chinese topic constructions, focusing on their sensitivity to island
constraints. Drawing inspiration from Tian et al. (2024), we outline an
experimental design for testing LLMs' grammatical knowledge of Mandarin syntax.
While no experiments have been conducted yet, this proposal aims to provide a
foundation for future studies and invites feedback on the methodology.

</details>


### [50] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
*Bohong Wu,Shen Yan,Sijun Zhang,Jianqiao Lu,Yutao Zeng,Ya Wang,Xun Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种名为PHD-Transformer的新框架，通过在预训练中实现高效的长度扩展，同时保持推理效率。通过创新的KV缓存管理策略和两种优化变体，实验证明了其在多个基准测试中的改进。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练后的长度扩展已显示出有效性，但在预训练中的潜力尚未充分探索。

Method: 提出PHD-Transformer框架，采用KV缓存管理策略区分原始令牌和隐藏解码令牌，并引入两种优化变体（PHD-SWA和PHD-CSWA）以提升性能。

Result: 实验结果表明，该方法在多个基准测试中均取得一致改进。

Conclusion: PHD-Transformer在预训练中实现了高效的长度扩展，同时保持了推理效率，为相关研究提供了新思路。

Abstract: Recent advances in large language models have demonstrated the effectiveness
of length scaling during post-training, yet its potential in pre-training
remains underexplored. We present the Parallel Hidden Decoding Transformer
(\textit{PHD}-Transformer), a novel framework that enables efficient length
scaling during pre-training while maintaining inference efficiency.
\textit{PHD}-Transformer achieves this through an innovative KV cache
management strategy that distinguishes between original tokens and hidden
decoding tokens. By retaining only the KV cache of original tokens for
long-range dependencies while immediately discarding hidden decoding tokens
after use, our approach maintains the same KV cache size as the vanilla
transformer while enabling effective length scaling. To further enhance
performance, we introduce two optimized variants: \textit{PHD-SWA} employs
sliding window attention to preserve local dependencies, while
\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate
linear growth in pre-filling time. Extensive experiments demonstrate consistent
improvements across multiple benchmarks.

</details>


### [51] [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
*Yow-Fu Liou,Yu-Chien Tang,An-Zi Yen*

Main category: cs.CL

TL;DR: 研究探讨了利用大型语言模型（LLMs）自动化生成教育材料和课程建议的潜力，通过实验验证其生成内容的高质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 减轻教师制作教育材料的负担，利用LLMs提升学习资源的丰富性和可访问性。

Method: 基于视频转录生成扩展文章，结合历史、文化等内容，并通过语义相似度推荐相关课程，最后用LLMs优化推荐。

Result: 实验表明，模型生成的内容质量高，课程推荐准确，提升了学习的趣味性和可访问性。

Conclusion: LLMs能有效连接核心内容与补充学习资源，辅助教师设计材料并为学生提供额外资源。

Abstract: The process of creating educational materials is both time-consuming and
demanding for educators. This research explores the potential of Large Language
Models (LLMs) to streamline this task by automating the generation of extended
reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper
sections as an initial exploration, we investigate how supplementary articles
can be enriched with contextual knowledge and connected to additional learning
resources. Our method begins by generating extended articles from video
transcripts, leveraging LLMs to include historical insights, cultural examples,
and illustrative anecdotes. A recommendation system employing semantic
similarity ranking identifies related courses, followed by an LLM-based
refinement process to enhance relevance. The final articles are tailored to
seamlessly integrate these recommendations, ensuring they remain cohesive and
informative. Experimental evaluations demonstrate that our model produces
high-quality content and accurate course suggestions, assessed through metrics
such as Hit Rate, semantic similarity, and coherence. Our experimental analysis
highlight the nuanced differences between the generated and existing materials,
underscoring the model's capacity to offer more engaging and accessible
learning experiences. This study showcases how LLMs can bridge the gap between
core content and supplementary learning, providing students with additional
recommended resources while also assisting teachers in designing educational
materials.

</details>


### [52] [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
*Muhammad Uzair Ul Haq,Davide Rigoni,Alessandro Sperduti*

Main category: cs.CL

TL;DR: 论文探讨了在NLP中利用LLMs自动标注数据的效率问题，提出通过检索增强生成（RAG）改进上下文示例选择，提升命名实体识别（NER）任务性能。


<details>
  <summary>Details</summary>
Motivation: 手动标注数据成本高且效率低，而现有LLMs的上下文学习（ICL）方法依赖人工选择示例，效果不佳。

Method: 比较多种LLMs和嵌入模型在不同数据集上的表现，引入RAG自动检索上下文示例。

Result: 结果表明LLM和嵌入模型的选择对性能至关重要，RAG方法能显著提升效果。

Conclusion: 研究强调了LLM规模与性能的权衡，并建议未来关注更具挑战性的数据集。

Abstract: In NLP, fine-tuning LLMs is effective for various applications but requires
high-quality annotated data. However, manual annotation of data is
labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly
used to automate the process, often employing in-context learning (ICL) in
which some examples related to the task are given in the prompt for better
performance. However, manually selecting context examples can lead to
inefficiencies and suboptimal model performance. This paper presents
comprehensive experiments comparing several LLMs, considering different
embedding models, across various datasets for the Named Entity Recognition
(NER) task. The evaluation encompasses models with approximately $7$B and $70$B
parameters, including both proprietary and non-proprietary models. Furthermore,
leveraging the success of Retrieval-Augmented Generation (RAG), it also
considers a method that addresses the limitations of ICL by automatically
retrieving contextual examples, thereby enhancing performance. The results
highlight the importance of selecting the appropriate LLM and embedding model,
understanding the trade-offs between LLM sizes and desired performance, and the
necessity to direct research efforts towards more challenging datasets.

</details>


### [53] [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
*Chengyu Wang,Junbing Yan,Yuanhao Yue,Jun Huang*

Main category: cs.CL

TL;DR: DistilQwen2.5是一系列轻量级大语言模型，通过蒸馏技术从Qwen2.5模型衍生而来，提升了指令跟随能力，同时降低了计算成本和部署难度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的场景下，提升大语言模型的计算效率并降低部署成本是关键挑战。

Method: 利用多代理教师模型选择和改写指令-响应对，结合标准微调和高效模型融合技术，逐步整合教师模型的细粒度知识。

Result: 蒸馏后的模型在能力上显著优于原始模型，并在实际应用中展示了其价值。

Conclusion: DistilQwen2.5模型已开源，为实际应用提供了高效、轻量化的解决方案。

Abstract: Enhancing computational efficiency and reducing deployment costs for large
language models (LLMs) have become critical challenges in various
resource-constrained scenarios. In this work, we present DistilQwen2.5, a
family of distilled, lightweight LLMs derived from the public Qwen2.5 models.
These distilled models exhibit enhanced instruction-following capabilities
compared to the original models based on a series of distillation techniques
that incorporate knowledge from much larger LLMs. In our industrial practice,
we first leverage powerful proprietary LLMs with varying capacities as
multi-agent teachers to select, rewrite, and refine instruction-response pairs
that are more suitable for student LLMs to learn. After standard fine-tuning,
we further leverage a computationally efficient model fusion approach that
enables student models to progressively integrate fine-grained hidden knowledge
from their teachers. Experimental evaluations demonstrate that the distilled
models possess significantly stronger capabilities than their original
checkpoints. Additionally, we present use cases to illustrate the applications
of our framework in real-world scenarios. To facilitate practical use, we have
released all the DistilQwen2.5 models to the open-source community.

</details>


### [54] [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
*Quy-Anh Dang,Chris Ngo,Truong-Son Hy*

Main category: cs.CL

TL;DR: RainbowPlus是一种基于进化计算的新型红队框架，通过自适应质量多样性搜索生成对抗性提示，显著提高了攻击成功率和多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受对抗性提示攻击，现有红队方法存在可扩展性、资源消耗和攻击策略多样性不足的问题。

Method: RainbowPlus采用多元素存档存储多样化高质量提示，并通过综合适应度函数评估多个提示，改进了传统质量多样性方法。

Result: 实验表明，RainbowPlus在攻击成功率和多样性上优于现有方法，生成更多独特提示，并在速度和效率上显著提升。

Conclusion: RainbowPlus为LLM安全性评估提供了可扩展工具，开源实现促进了进一步研究。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are
susceptible to adversarial prompts that exploit vulnerabilities to produce
unsafe or biased outputs. Existing red-teaming methods often face scalability
challenges, resource-intensive requirements, or limited diversity in attack
strategies. We propose RainbowPlus, a novel red-teaming framework rooted in
evolutionary computation, enhancing adversarial prompt generation through an
adaptive quality-diversity (QD) search that extends classical evolutionary
algorithms like MAP-Elites with innovations tailored for language models. By
employing a multi-element archive to store diverse high-quality prompts and a
comprehensive fitness function to evaluate multiple prompts concurrently,
RainbowPlus overcomes the constraints of single-prompt archives and pairwise
comparisons in prior QD methods like Rainbow Teaming. Experiments comparing
RainbowPlus to QD methods across six benchmark datasets and four open-source
LLMs demonstrate superior attack success rate (ASR) and diversity
(Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts
(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine
state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten
open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,
surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).
Our open-source implementation fosters further advancements in LLM safety,
offering a scalable tool for vulnerability assessment. Code and resources are
publicly available at https://github.com/knoveleng/rainbowplus, supporting
reproducibility and future research in LLM red-teaming.

</details>


### [55] [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
*Joachim Minder,Guillaume Wisniewski,Natalie Kübler*

Main category: cs.CL

TL;DR: 研究探讨了ChatGPT在基于错误分类的机器翻译输出标注中的能力，发现其在详细提示下表现良好，但自我评估能力有限。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在专业翻译领域中的错误识别与分类能力，填补以往研究主要关注通用语言的空白。

Method: 通过两种不同提示和自定义错误分类，比较ChatGPT与人类专家对DeepL和ChatGPT自身翻译的评估。

Result: ChatGPT对DeepL翻译的标注召回率和精确度较高，但自我评估表现较差；提示的详细程度影响分类准确性。

Conclusion: 大语言模型在翻译评估中具有潜力但存在局限，未来可研究开源模型及自动化评估在翻译教学中的应用。

Abstract: This study investigates the capabilities of large language models (LLMs),
specifically ChatGPT, in annotating MT outputs based on an error typology. In
contrast to previous work focusing mainly on general language, we explore
ChatGPT's ability to identify and categorise errors in specialised
translations. By testing two different prompts and based on a customised error
typology, we compare ChatGPT annotations with human expert evaluations of
translations produced by DeepL and ChatGPT itself. The results show that, for
translations generated by DeepL, recall and precision are quite high. However,
the degree of accuracy in error categorisation depends on the prompt's specific
features and its level of detail, ChatGPT performing very well with a detailed
prompt. When evaluating its own translations, ChatGPT achieves significantly
poorer results, revealing limitations with self-assessment. These results
highlight both the potential and the limitations of LLMs for translation
evaluation, particularly in specialised domains. Our experiments pave the way
for future research on open-source LLMs, which could produce annotations of
comparable or even higher quality. In the future, we also aim to test the
practical effectiveness of this automated evaluation in the context of
translation training, particularly by optimising the process of human
evaluation by teachers and by exploring the impact of annotations by LLMs on
students' post-editing and translation learning.

</details>


### [56] [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
*K. Wong,B. Wu,S. Bulathwela,M. Cukurova*

Main category: cs.CL

TL;DR: 研究探讨了多模态数据在检测学生协作问题解决（CPS）能力中的潜力，发现其在特定CPS子技能诊断中优于传统模型，但效果因标签复杂性和数据集组成而异。


<details>
  <summary>Details</summary>
Motivation: 探索多模态数据和先进模型在AI教育领域中对复杂CPS行为检测的实际价值。

Method: 使用文本和音频嵌入的多模态分类模型，比较单模态与多模态基于Transformer的模型性能。

Result: 多模态在Transformer模型中提升了社交认知类CPS的诊断性能，但对所有子技能并非普遍有效。

Conclusion: 多模态和模型选择需根据具体CPS指标定制，强调人机互补和进一步探索模型架构的必要性。

Abstract: Detecting collaborative and problem-solving behaviours from digital traces to
interpret students' collaborative problem solving (CPS) competency is a
long-term goal in the Artificial Intelligence in Education (AIEd) field.
Although multimodal data and advanced models are argued to have the potential
to detect complex CPS behaviours, empirical evidence on their value remains
limited with some contrasting evidence. In this study, we investigated the
potential of multimodal data to improve model performance in diagnosing 78
secondary school students' CPS subskills and indicators in authentic
educational settings. In particular, text embeddings from verbal data and
acoustic embeddings from audio data were used in a multimodal classification
model for CPS diagnosis. Both unimodal and multimodal transformer-based models
outperformed traditional models in detecting CPS classes. Although the
inclusion of multimodality did not improve the performance of traditional
unimodal models, its integration into transformer-based models demonstrated
improved performance for diagnosing social-cognitive CPS classes compared to
unimodal transformer-based models. Based on the results, the paper argues that
multimodality and the selection of a particular modelling technique should not
be taken for granted to achieve the best performance in the automated detection
of every CPS subskill and indicator. Rather, their value is limited to certain
types of CPS indicators, affected by the complexity of the labels, and
dependent on the composition of indicators in the dataset. We conclude the
paper by discussing the required nuance when considering the value of LLMs and
multimodality in automated CPS diagnosis, highlighting the need for human-AI
complementarity, and proposing the exploration of relevant model architectures
and techniques to improve CPS diagnosis in authentic educational contexts.

</details>


### [57] [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
*Khalil Hennara,Sara Chrouf,Mohamed Motaism Hamed,Zeina Aldallal,Omar Hadid,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 本文提出了一种将新语言整合到大型语言模型（LLM）中的方法，成功将阿拉伯语注入小型开源模型，性能提升8%，同时保留原有知识。


<details>
  <summary>Details</summary>
Motivation: 增强现有模型以融入新知识是AI发展的关键，本文旨在实现高效、低成本的语言模型扩展。

Method: 通过训练一个15亿参数的小型模型Kuwain，将阿拉伯语注入以英语为主的小型开源模型。

Result: 阿拉伯语性能平均提升8%，同时保留原有知识，提供了一种资源高效的替代方案。

Conclusion: 该方法展示了无需大量重新训练即可高效扩展语言模型的潜力。

Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI
development. This paper introduces a novel method for integrating a new
language into a large language model (LLM). Our approach successfully
incorporates a previously unseen target language into an existing LLM without
compromising its prior knowledge. We trained a tiny model with 1.5 billion
parameters named Kuwain by injecting the Arabic language into a small
open-source model mainly trained in English. Our method demonstrates
significant improvements in Arabic language performance, with an average 8%
improvement across various benchmarks, while retaining the model's existing
knowledge with a minimum amount of the original model's data. This offers a
cost-effective alternative to training a comprehensive model in both English
and Arabic. The results highlight the potential for efficient, targeted
language model expansion without extensive retraining or resource-intensive
processes.

</details>


### [58] [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
*Ziwen Xu,Shuxun Wang,Kewei Xu,Haoming Xu,Mengru Wang,Xinle Deng,Yunzhi Yao,Guozhou Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: EasyEdit2是一个框架，支持通过插件式调整控制大型语言模型（LLM）的行为，无需修改模型参数。


<details>
  <summary>Details</summary>
Motivation: 提供一种简单易用的方法，让用户无需深入技术知识即可精确控制LLM的行为。

Method: 采用新的架构，包括导向向量生成器和应用器，通过示例自动生成和应用导向向量。

Result: 在不同LLM上验证了模型导向的有效性，展示了技术的实用性。

Conclusion: EasyEdit2为LLM行为控制提供了高效且易用的解决方案，并开源了代码和演示资源。

Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable
plug-and-play adjustability for controlling Large Language Model (LLM)
behaviors. EasyEdit2 supports a wide range of test-time interventions,
including safety, sentiment, personality, reasoning patterns, factuality, and
language features. Unlike its predecessor, EasyEdit2 features a new
architecture specifically designed for seamless model steering. It comprises
key modules such as the steering vector generator and the steering vector
applier, which enable automatic generation and application of steering vectors
to influence the model's behavior without modifying its parameters. One of the
main advantages of EasyEdit2 is its ease of use-users do not need extensive
technical knowledge. With just a single example, they can effectively guide and
adjust the model's responses, making precise control both accessible and
efficient. Empirically, we report model steering performance across different
LLMs, demonstrating the effectiveness of these techniques. We have released the
source code on GitHub at https://github.com/zjunlp/EasyEdit along with a
demonstration notebook. In addition, we provide a demo video at
https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.

</details>


### [59] [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
*Joan C. Timoneda*

Main category: cs.CL

TL;DR: 论文提出了一种合成插补方法，利用生成式LLM（如GPT-4o）生成合成文本，以解决训练数据中类别不均衡的问题。该方法在75个原始样本时性能与完整样本相当，且过拟合可控。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中某些类别样本不足的问题，提升模型性能。

Method: 使用生成式LLM（GPT-4o）基于少量原始样本生成合成文本，确保多样性并保留语义。

Result: 在75个原始样本时性能与完整样本相当，50个样本时过拟合可控且可修正。

Conclusion: 合成插补方法为生成式LLM在研究中提供了新应用，帮助平衡数据集以优化性能。

Abstract: Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,
require that all categories in an annotation task be sufficiently represented
in the training data for optimal performance. However, it is often difficult to
find sufficient examples for all categories in a task when building a
high-quality training set. In this article, I describe this problem and propose
a solution, the synthetic imputation approach. Leveraging a generative LLM
(GPT-4o), this approach generates synthetic texts based on careful prompting
and five original examples drawn randomly with replacement from the sample.
This approach ensures that new synthetic texts are sufficiently different from
the original texts to reduce overfitting, but retain the underlying substantive
meaning of the examples to maximize out-of-sample performance. With 75 original
examples or more, synthetic imputation's performance is on par with a full
sample of original texts, and overfitting remains low, predictable and
correctable with 50 original samples. The synthetic imputation approach
provides a novel role for generative LLMs in research and allows applied
researchers to balance their datasets for best performance.

</details>


### [60] [On true empty category](https://arxiv.org/abs/2504.15168)
*Qilin Tian*

Main category: cs.CL

TL;DR: 论文讨论了空范畴的分类，并评估了Li等人提出的“真正空范畴”假说，认为其证据可以通过其他方式解释。


<details>
  <summary>Details</summary>
Motivation: 探讨空范畴的分类及其在语言理论中的影响，验证Li等人提出的假说是否必要。

Method: 通过主题化的证据分析，评估“真正空范畴”假说的合理性。

Result: 研究发现，主题化现象无需引入“真正空范畴”即可解释。

Conclusion: “真正空范畴”假说可能并非必要，现有理论足以解释相关语言现象。

Abstract: According to Chomsky (1981, 1986), empty categories consist of PRO, pro,
trace, and variable. However, some empty object positions seem to be
incompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)
and Li & Wei (2014) raise the true empty category hypothesis, which holds that
true empty category is only an empty position with category and Case features.
As a last resort option, it is used mainly to meet the subcatgorization of a
verb. This assumption is ingenious, and if proved to be true, it will exert a
great impact on the study of UG. In this paper, we evaluate their evidence from
topicalization and demonstrate that it can be accounted for without invoking
true empty category.

</details>


### [61] [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
*Nandan Thakur,Ronak Pradeep,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.CL

TL;DR: 研究比较了GPT-4o与人类评委在RAG评估中的支持性判断表现，发现GPT-4o在手动编辑条件下与人类评委一致性达72%，可作为可靠替代。


<details>
  <summary>Details</summary>
Motivation: 评估RAG系统中信息支持性的准确性，减少幻觉，并比较自动LLM评委与人类评委的表现。

Method: 对45份提交的36个主题进行大规模比较研究，分为完全手动评估和手动编辑LLM预测两种条件。

Result: GPT-4o与人类评委在完全手动条件下一致性为56%，手动编辑条件下提升至72%。独立人类评委与GPT-4o相关性更高。

Conclusion: LLM评委可作为支持性评估的可靠替代，未来需进一步优化人类与LLM的错误分析。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to
generate answers with citations from source documents containing "ground
truth", thereby reducing system hallucinations. A crucial factor in RAG
evaluation is "support", whether the information in the cited documents
supports the answer. To this end, we conducted a large-scale comparative study
of 45 participant submissions on 36 topics to the TREC 2024 RAG Track,
comparing an automatic LLM judge (GPT-4o) against human judges for support
assessment. We considered two conditions: (1) fully manual assessments from
scratch and (2) manual assessments with post-editing of LLM predictions. Our
results indicate that for 56% of the manual from-scratch assessments, human and
GPT-4o predictions match perfectly (on a three-level scale), increasing to 72%
in the manual with post-editing condition. Furthermore, by carefully analyzing
the disagreements in an unbiased study, we found that an independent human
judge correlates better with GPT-4o than a human judge, suggesting that LLM
judges can be a reliable alternative for support assessment. To conclude, we
provide a qualitative analysis of human and GPT-4o errors to help guide future
iterations of support assessment.

</details>


### [62] [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
*Manya Wadhwa,Zayne Sprague,Chaitanya Malaviya,Philippe Laban,Junyi Jessy Li,Greg Durrett*

Main category: cs.CL

TL;DR: EvalAgent是一个新框架，用于自动发现任务特定的隐式评估标准，结合专家指导和LLM生成的标准，提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常依赖显式标准，而高质量响应需要满足更多隐式任务特定标准。

Method: EvalAgent通过挖掘专家在线指导，提出基于可靠外部来源的多样化评估标准。

Result: EvalAgent的标准具有隐式性和特异性，能指导响应优化，且结合LLM标准后更符合人类价值。

Conclusion: EvalAgent为结构化写作任务的评估提供了更全面和有效的标准发现方法。

Abstract: Evaluation of language model outputs on structured writing tasks is typically
conducted with a number of desirable criteria presented to human evaluators or
large language models (LLMs). For instance, on a prompt like "Help me draft an
academic talk on coffee intake vs research productivity", a model response may
be evaluated for criteria like accuracy and coherence. However, high-quality
responses should do more than just satisfy basic task requirements. An
effective response to this query should include quintessential features of an
academic talk, such as a compelling opening, clear research questions, and a
takeaway. To help identify these implicit criteria, we introduce EvalAgent, a
novel framework designed to automatically uncover nuanced and task-specific
criteria. EvalAgent first mines expert-authored online guidance. It then uses
this evidence to propose diverse, long-tail evaluation criteria that are
grounded in reliable external sources. Our experiments demonstrate that the
grounded criteria produced by EvalAgent are often implicit (not directly stated
in the user's prompt), yet specific (high degree of lexical precision).
Further, EvalAgent criteria are often not satisfied by initial responses but
they are actionable, such that responses can be refined to satisfy them.
Finally, we show that combining LLM-generated and EvalAgent criteria uncovers
more human-valued criteria than using LLMs alone.

</details>


### [63] [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
*Julián Cendrero,Julio Gonzalo,Ivar Zapata*

Main category: cs.CL

TL;DR: 论文提出了完全贝叶斯的BToT模型和加权版本WBToT，解决了ToT模型的稳定性问题，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ToT模型在时间戳数据集上表现不稳定，且未采用完全贝叶斯方法。

Method: 引入Beta分布的共轭先验作为正则化，提出BToT；进一步提出WBToT，通过重复时间戳平衡词和时间的影响。

Result: WBToT在SOTU和Twitter数据集上表现优于LDA和BERTopic，主题稳定性提升51%和34%。

Conclusion: WBToT解决了ToT的稳定性问题，平衡了时间和词模态，适用于更大规模问题。

Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped
datasets by explicitly modeling publication dates jointly with word
co-occurrence patterns. However, ToT was not approached in a fully Bayesian
fashion, a flaw that makes it susceptible to stability problems. To address
this issue, we propose a fully Bayesian Topics over Time (BToT) model via the
introduction of a conjugate prior to the Beta distribution. This prior acts as
a regularization that prevents the online version of the algorithm from
unstable updates when a topic is poorly represented in a mini-batch. The
characteristics of this prior to the Beta distribution are studied here for the
first time. Still, this model suffers from a difference in scale between the
single-time observations and the multiplicity of words per document. A
variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a
solution. In WBToT, publication dates are repeated a certain number of times
per document, which balances the relative influence of words and timestamps
along the inference process. We have tested our models on two datasets: a
collection of over 200 years of US state-of-the-union (SOTU) addresses and a
large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that
WBToT captures events better than Latent Dirichlet Allocation and other SOTA
topic models like BERTopic: the median absolute deviation of the topic presence
over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also
demonstrate the superior coherence of WBToT over BToT, which highlights the
importance of balancing the time and word modalities. Finally, we illustrate
the stability of the online optimization algorithm in WBToT, which allows the
application of WBToT to problems that are intractable for standard ToT.

</details>


### [64] [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
*Saffron Huang,Esin Durmus,Miles McCain,Kunal Handa,Alex Tamkin,Jerry Hong,Michael Stern,Arushi Somani,Xiuruo Zhang,Deep Ganguli*

Main category: cs.CL

TL;DR: 研究开发了一种隐私保护方法，从Claude 3和3.5模型的真实交互中提取了3,307种AI价值观，发现其支持普世价值观且具有上下文依赖性。


<details>
  <summary>Details</summary>
Motivation: AI助手的价值观可能影响用户决策和世界观，但缺乏实证研究。

Method: 采用自下而上的隐私保护方法，分析数十万次真实交互。

Result: 发现AI支持普世价值观（如透明性），且价值观随上下文变化（如“预防伤害”在抵制用户时出现）。

Conclusion: 研究为AI价值观的评估和设计提供了实证基础。

Abstract: AI assistants can impart value judgments that shape people's decisions and
worldviews, yet little is known empirically about what values these systems
rely on in practice. To address this, we develop a bottom-up,
privacy-preserving method to extract the values (normative considerations
stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit
in hundreds of thousands of real-world interactions. We empirically discover
and taxonomize 3,307 AI values and study how they vary by context. We find that
Claude expresses many practical and epistemic values, and typically supports
prosocial human values while resisting values like "moral nihilism". While some
values appear consistently across contexts (e.g. "transparency"), many are more
specialized and context-dependent, reflecting the diversity of human
interlocutors and their varied contexts. For example, "harm prevention" emerges
when Claude resists users, "historical accuracy" when responding to queries
about controversial events, "healthy boundaries" when asked for relationship
advice, and "human agency" in technology ethics discussions. By providing the
first large-scale empirical mapping of AI values in deployment, our work
creates a foundation for more grounded evaluation and design of values in AI
systems.

</details>


### [65] [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
*Yahan Yang,Soham Dan,Shuo Li,Dan Roth,Insup Lee*

Main category: cs.CL

TL;DR: 提出了一种多语言护栏方法，通过合成数据、监督微调和GRPO框架提升性能，在多语言环境中有效检测和过滤不安全内容。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下LLMs易受攻击，且安全对齐数据有限，需开发能跨语言检测不安全内容的护栏。

Method: 包括合成多语言数据、监督微调和GRPO框架，提升多语言护栏性能。

Result: 在多语言环境中表现优于基线，并能生成多语言解释，帮助理解语言特定风险。

Conclusion: 该方法有效提升多语言内容审核能力，为LLMs的实际部署提供安全保障。

Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as
jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability
is exacerbated in multilingual setting, where multilingual safety-aligned data
are often limited. Thus, developing a guardrail capable of detecting and
filtering unsafe content across diverse languages is critical for deploying
LLMs in real-world applications. In this work, we propose an approach to build
a multilingual guardrail with reasoning. Our method consists of: (1) synthetic
multilingual data generation incorporating culturally and linguistically
nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group
Relative Policy Optimization (GRPO) framework that further improves
performance. Experimental results demonstrate that our multilingual guardrail
consistently outperforms recent baselines across both in-domain and
out-of-domain languages. The multilingual reasoning capability of our guardrail
enables it to generate multilingual explanations, which are particularly useful
for understanding language-specific risks and ambiguities in multilingual
content moderation.

</details>


### [66] [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
*Yilun Zhou,Austin Xu,Peifeng Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 论文研究了LLM-judges在测试时计算扩展中的表现，发现其在重新排序任务中表现良好，但在光束搜索和基于批判的响应优化中不如过程奖励模型。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM-judges在测试时计算扩展中的有效性，填补其在自动评估中表现的研究空白。

Method: 引入JETTS基准，评估LLM-judges在数学推理、代码生成和指令遵循三个领域的表现，涉及重新排序、光束搜索和批判优化三种任务设置。

Result: LLM-judges在重新排序中与结果奖励模型竞争，但在光束搜索中表现较差，且自然语言批判对响应优化的指导效果有限。

Conclusion: LLM-judges在特定任务中表现良好，但在复杂任务中仍需改进，批判功能目前效果不佳。

Abstract: Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding](https://arxiv.org/abs/2504.13915)
*Dibyadip Chatterjee,Edoardo Remelli,Yale Song,Bugra Tekin,Abhay Mittal,Bharat Bhatnagar,Necati Cihan Camgöz,Shreyas Hampali,Eric Sauser,Shugao Ma,Angela Yao,Fadime Sener*

Main category: cs.CV

TL;DR: ProVideLLM是一个实时程序视频理解框架，通过多模态缓存减少令牌数量，实现高效计算和内存管理，并在多个任务上取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在长时观察中令牌数量过多的问题，同时保持细粒度细节的编码能力。

Method: 结合文本令牌和视觉令牌的多模态缓存设计，使用DETR-QFormer编码视觉细节。

Result: 令牌数量减少22倍，支持10 FPS的逐帧推理和25 FPS的对话，GPU内存占用仅2GB，在六个任务上达到最优。

Conclusion: ProVideLLM通过创新的多模态缓存设计，实现了高效、实时的程序视频理解，性能显著优于现有方法。

Abstract: We introduce ProVideLLM, an end-to-end framework for real-time procedural
video understanding. ProVideLLM integrates a multimodal cache configured to
store two types of tokens - verbalized text tokens, which provide compressed
textual summaries of long-term observations, and visual tokens, encoded with
DETR-QFormer to capture fine-grained details from short-term observations. This
design reduces token count by 22x over existing methods in representing one
hour of long-term observations while effectively encoding fine-granularity of
the present. By interleaving these tokens in our multimodal cache, ProVideLLM
ensures sub-linear scaling of memory and compute with video length, enabling
per-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with
a minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art
results on six procedural tasks across four datasets.

</details>


### [68] [Entropy Rectifying Guidance for Diffusion and Flow Models](https://arxiv.org/abs/2504.13987)
*Tariq Berrada Ifriqi,Adriana Romero-Soriano,Michal Drozdzal,Jakob Verbeek,Karteek Alahari*

Main category: cs.CV

TL;DR: 论文提出了一种名为熵修正引导（ERG）的新方法，通过调整扩散变换器架构中的注意力机制，在推理时同时提升图像质量、多样性和提示一致性，优于现有的分类器自由引导（CFG）技术。


<details>
  <summary>Details</summary>
Motivation: 现有的分类器自由引导（CFG）技术在提升图像质量、多样性和一致性时存在权衡，且需要额外模型或更多计算资源。本文旨在提出一种更通用且高效的引导方法。

Method: 提出熵修正引导（ERG），基于扩散变换器架构中注意力机制的推理时调整，无需额外模型或计算开销。

Result: ERG在文本到图像、类条件和无条件图像生成任务中显著提升了性能，并能与其他引导方法（如CADS和APG）结合使用。

Conclusion: ERG是一种简单有效的引导机制，能够同时优化图像质量、多样性和一致性，具有广泛的应用潜力。

Abstract: Guidance techniques are commonly used in diffusion and flow models to improve
image quality and consistency for conditional generative tasks such as
class-conditional and text-to-image generation. In particular, classifier-free
guidance (CFG) -- the most widely adopted guidance technique -- contrasts
conditional and unconditional predictions to improve the generated images. This
results, however, in trade-offs across quality, diversity and consistency,
improving some at the expense of others. While recent work has shown that it is
possible to disentangle these factors to some extent, such methods come with an
overhead of requiring an additional (weaker) model, or require more forward
passes per sampling step. In this paper, we propose Entropy Rectifying Guidance
(ERG), a simple and effective guidance mechanism based on inference-time
changes in the attention mechanism of state-of-the-art diffusion transformer
architectures, which allows for simultaneous improvements over image quality,
diversity and prompt consistency. ERG is more general than CFG and similar
guidance techniques, as it extends to unconditional sampling. ERG results in
significant improvements in various generation tasks such as text-to-image,
class-conditional and unconditional image generation. We also show that ERG can
be seamlessly combined with other recent guidance methods such as CADS and APG,
further boosting generation performance.

</details>


### [69] [Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training](https://arxiv.org/abs/2504.13995)
*Andrea Amaduzzi,Pierluigi Zama Ramirez,Giuseppe Lisanti,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TL;DR: LLaNA是首个能够直接处理NeRF权重并执行NeRF描述和问答任务的多模态大语言模型（MLLM），无需渲染图像或生成3D数据。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在理解图像和3D数据时存在局限性，而NeRF能更全面地编码几何和外观信息，因此研究将NeRF引入MLLM的可行性。

Method: 提出LLaNA模型，直接处理NeRF的MLP权重，并构建了一个包含30万NeRF的大规模数据集用于训练和评估。

Result: 直接处理NeRF权重在NeRF-语言任务上表现优于基于2D或3D表示的方法。

Conclusion: LLaNA展示了直接利用NeRF权重的有效性，为多模态理解开辟了新方向。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown
remarkable capabilities in understanding both images and 3D data, yet these
modalities face inherent limitations in comprehensively representing object
geometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a
promising alternative, encoding both geometric and photorealistic properties
within the weights of a simple Multi-Layer Perceptron (MLP). This work
investigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.
We introduce LLaNA, the first MLLM able to perform new tasks such as NeRF
captioning and Q\&A, by directly processing the weights of a NeRF's MLP.
Notably, LLaNA is able to extract information about the represented objects
without the need to render images or materialize 3D data structures. In
addition, we build the first large-scale NeRF-language dataset, composed by
more than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual
annotations that enable various NeRF-language tasks. Based on this dataset, we
develop a benchmark to evaluate the NeRF understanding capability of our
method. Results show that directly processing NeRF weights leads to better
performance on NeRF-Language tasks compared to approaches that rely on either
2D or 3D representations derived from NeRFs.

</details>


### [70] [Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation](https://arxiv.org/abs/2504.14011)
*Fulvio Sanguigni,Davide Morelli,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 论文提出Fashion-RAG方法，通过检索增强生成技术，基于用户文本输入定制时尚单品，解决了现有虚拟试衣方法依赖特定服装输入的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法通常需要特定服装输入，而用户可能仅提供文本描述，限制了实用性。

Method: 采用检索增强生成技术，结合文本反转方法，将检索到的服装图像嵌入Stable Diffusion文本编码空间，生成个性化图像。

Result: 在Dress Code数据集上，Fashion-RAG在质量和数量上均优于现有方法，能有效捕捉服装细节。

Conclusion: Fashion-RAG是首个针对多模态时尚图像编辑的检索增强生成方法，具有实用性和创新性。

Abstract: In recent years, the fashion industry has increasingly adopted AI
technologies to enhance customer experience, driven by the proliferation of
e-commerce platforms and virtual applications. Among the various tasks, virtual
try-on and multimodal fashion image editing -- which utilizes diverse input
modalities such as text, garment sketches, and body poses -- have become a key
area of research. Diffusion models have emerged as a leading approach for such
generative tasks, offering superior image quality and diversity. However, most
existing virtual try-on methods rely on having a specific garment input, which
is often impractical in real-world scenarios where users may only provide
textual specifications. To address this limitation, in this work we introduce
Fashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that
enables the customization of fashion items based on user preferences provided
in textual form. Our approach retrieves multiple garments that match the input
specifications and generates a personalized image by incorporating attributes
from the retrieved items. To achieve this, we employ textual inversion
techniques, where retrieved garment images are projected into the textual
embedding space of the Stable Diffusion text encoder, allowing seamless
integration of retrieved elements into the generative process. Experimental
results on the Dress Code dataset demonstrate that Fashion-RAG outperforms
existing methods both qualitatively and quantitatively, effectively capturing
fine-grained visual details from retrieved garments. To the best of our
knowledge, this is the first work to introduce a retrieval-augmented generation
approach specifically tailored for multimodal fashion image editing.

</details>


### [71] [LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models](https://arxiv.org/abs/2504.14032)
*Haiwen Huang,Anpei Chen,Volodymyr Havrylov,Andreas Geiger,Dan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种改进视觉基础模型（VFMs）特征上采样的方法，通过设计新的上采样器架构和训练目标，显著提升了像素级理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: VFMs（如DINOv2和CLIP）在像素级任务中表现受限，特征分辨率不足是主要挑战。

Method: 引入基于坐标的交叉注意力Transformer架构，结合高分辨率图像和低分辨率VFM特征；提出利用类无关掩码和自蒸馏构建高分辨率伪真值特征的训练目标。

Result: 实验表明，该方法在多种下游任务中显著优于现有特征上采样技术。

Conclusion: 该方法有效捕捉细节并适应不同分辨率，代码已开源。

Abstract: Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved
impressive results on various downstream tasks, but their limited feature
resolution hampers performance in applications requiring pixel-level
understanding. Feature upsampling offers a promising direction to address this
challenge. In this work, we identify two critical factors for enhancing feature
upsampling: the upsampler architecture and the training objective. For the
upsampler architecture, we introduce a coordinate-based cross-attention
transformer that integrates the high-resolution images with coordinates and
low-resolution VFM features to generate sharp, high-quality features. For the
training objective, we propose constructing high-resolution pseudo-groundtruth
features by leveraging class-agnostic masks and self-distillation. Our approach
effectively captures fine-grained details and adapts flexibly to various input
and feature resolutions. Through experiments, we demonstrate that our approach
significantly outperforms existing feature upsampling techniques across various
downstream tasks. Our code is released at https://github.com/andrehuang/loftup.

</details>


### [72] [Occlusion-Ordered Semantic Instance Segmentation](https://arxiv.org/abs/2504.14054)
*Soroosh Baselizadeh,Cheuk-To Yu,Olga Veksler,Yuri Boykov*

Main category: cs.CV

TL;DR: 论文提出了一种结合相对深度排序和实例分割的任务OOSIS，通过遮挡边界实现更可靠的3D信息提取。


<details>
  <summary>Details</summary>
Motivation: 传统的2D实例分割缺乏3D信息，而单目深度估计困难，因此利用遮挡关系提供更简单的3D信息。

Method: 提出OOSIS任务，结合遮挡边界和语义分割同时提取实例及其遮挡顺序，并开发了一种新的定向遮挡边界方法。

Result: 在KINS和COCOA数据集上表现优于基线方法。

Conclusion: OOSIS通过遮挡关系提供了一种简单有效的3D信息提取方法，优于传统深度估计。

Abstract: Standard semantic instance segmentation provides useful, but inherently 2D
information from a single image. To enable 3D analysis, one usually integrates
absolute monocular depth estimation with instance segmentation. However,
monocular depth is a difficult task. Instead, we leverage a simpler
single-image task, occlusion-based relative depth ordering, providing coarser
but useful 3D information. We show that relative depth ordering works more
reliably from occlusions than from absolute depth. We propose to solve the
joint task of relative depth ordering and segmentation of instances based on
occlusions. We call this task Occlusion-Ordered Semantic Instance Segmentation
(OOSIS). We develop an approach to OOSIS that extracts instances and their
occlusion order simultaneously from oriented occlusion boundaries and semantic
segmentation. Unlike popular detect-and-segment framework for instance
segmentation, combining occlusion ordering with instance segmentation allows a
simple and clean formulation of OOSIS as a labeling problem. As a part of our
solution for OOSIS, we develop a novel oriented occlusion boundaries approach
that significantly outperforms prior work. We also develop a new joint OOSIS
metric based both on instance mask accuracy and correctness of their occlusion
order. We achieve better performance than strong baselines on KINS and COCOA
datasets.

</details>


### [73] [Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design](https://arxiv.org/abs/2504.14075)
*Wei Dong,Yan Min,Han Zhou,Jun Chen*

Main category: cs.CV

TL;DR: SG-LLIE是一种基于结构先验的多尺度CNN-Transformer混合框架，用于低光图像增强，通过引入结构引导的特征提取模块，显著提升了极端低光环境下的增强效果。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法依赖直接映射或语义特征，但在极端低光环境下效果不佳，SG-LLIE旨在通过结构先验解决这一问题。

Method: 采用多尺度CNN-Transformer混合框架，结合光照不变边缘检测器提取结构先验，并在UNet架构中引入结构引导的Transformer模块。

Result: 在多个低光图像增强基准测试中取得最优性能，并在NTIRE 2025挑战赛中排名第二。

Conclusion: SG-LLIE通过结构先验和多尺度混合框架，显著提升了低光图像增强的效果，尤其在极端低光环境下表现突出。

Abstract: Current Low-light Image Enhancement (LLIE) techniques predominantly rely on
either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from
semantic features or illumination maps. Nonetheless, the intrinsic
ill-posedness of LLIE and the difficulty in retrieving robust semantics from
heavily corrupted images hinder their effectiveness in extremely low-light
environments. To tackle this challenge, we present SG-LLIE, a new multi-scale
CNN-Transformer hybrid framework guided by structure priors. Different from
employing pre-trained models for the extraction of semantics or illumination
maps, we choose to extract robust structure priors based on
illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer
Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in
the UNet encoder-decoder architecture. Besides the CNN blocks which excels in
multi-scale feature extraction and fusion, we introduce a Structure-Guided
Transformer Block (SGTB) in each HSGFE that incorporates structural priors to
modulate the enhancement process. Extensive experiments show that our method
achieves state-of-the-art performance on several LLIE benchmarks in both
quantitative metrics and visual quality. Our solution ranks second in the NTIRE
2025 Low-Light Enhancement Challenge. Code is released at
https://github.com/minyan8/imagine.

</details>


### [74] [Retinex-guided Histogram Transformer for Mask-free Shadow Removal](https://arxiv.org/abs/2504.14092)
*Wei Dong,Han Zhou,Seyed Amirreza Mousavi,Jun Chen*

Main category: cs.CV

TL;DR: ReHiT提出了一种基于混合CNN-Transformer架构的无掩模阴影去除框架，结合Retinex理论，通过双分支管道分别建模反射和光照分量，并利用IG-HCT模块进行恢复。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖难以获取的阴影掩模，限制了其在真实场景中的泛化能力。

Method: 采用双分支管道分别处理反射和光照分量，结合CNN块和IGHB模块处理非均匀光照和复杂阴影。

Result: 在多个基准数据集上表现优异，参数规模小且推理速度快，适用于计算资源有限的场景。

Conclusion: ReHiT是一种高效的无掩模阴影去除方法，具有实际应用的潜力。

Abstract: While deep learning methods have achieved notable progress in shadow removal,
many existing approaches rely on shadow masks that are difficult to obtain,
limiting their generalization to real-world scenes. In this work, we propose
ReHiT, an efficient mask-free shadow removal framework based on a hybrid
CNN-Transformer architecture guided by Retinex theory. We first introduce a
dual-branch pipeline to separately model reflectance and illumination
components, and each is restored by our developed Illumination-Guided Hybrid
CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are
capable of learning residual dense features and performing multi-scale semantic
fusion, multi-scale semantic fusion, we develop the Illumination-Guided
Histogram Transformer Block (IGHB) to effectively handle non-uniform
illumination and spatially complex shadows. Extensive experiments on several
benchmark datasets validate the effectiveness of our approach over existing
mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge
dataset, our solution delivers competitive results with one of the smallest
parameter sizes and fastest inference speeds among top-ranked entries,
highlighting its applicability for real-world applications with limited
computational resources. The code is available at
https://github.com/dongw22/oath.

</details>


### [75] [VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment](https://arxiv.org/abs/2504.14096)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: VideoPASTA通过偏好优化增强Video-LLMs，解决空间、时间和跨帧关系问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在空间关系、时间顺序和跨帧连续性方面表现不足。

Method: 使用偏好优化框架VideoPASTA，通过对抗样本训练模型区分准确和错误的视频表示。

Result: 在多个基准测试中性能显著提升（3.05%、1.97%、1.31%）。

Conclusion: VideoPASTA无需大规模预训练或架构修改，高效且可扩展。

Abstract: Video-language models (Video-LLMs) excel at understanding video content but
struggle with spatial relationships, temporal ordering, and cross-frame
continuity. To address these limitations, we introduce VideoPASTA (Preference
Alignment with Spatio-Temporal-Cross Frame Adversaries), a framework that
enhances Video-LLMs through targeted preference optimization. VideoPASTA trains
models to distinguish accurate video representations from carefully generated
adversarial examples that deliberately violate spatial, temporal, or
cross-frame relations. By applying Direct Preference Optimization to just 7,020
preference pairs, VideoPASTA learns robust representations that capture
fine-grained spatial relationships and long-range temporal dynamics.
Experiments on standard video benchmarks show significant relative performance
gains of 3.05% on VideoMME, 1.97% on NeXTQA, and 1.31% on LongVideoBench, over
the baseline Qwen2.5-VL model. These results demonstrate that targeted
alignment, rather than massive pretraining or architectural modifications,
effectively addresses core video-language challenges. Notably, VideoPASTA
achieves these improvements without human annotation or captioning, relying on
just 32-frame sampling, compared to the 96-frame, multi-GPU setups of prior
work. This efficiency makes our approach a scalable, plug-and-play solution
that seamlessly integrates with existing models while preserving their
capabilities.

</details>


### [76] [Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models](https://arxiv.org/abs/2504.14108)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang,Yuelong Xia*

Main category: cs.CV

TL;DR: DanceText是一个无需训练的多语言文本编辑框架，支持复杂几何变换并实现无缝的前景-背景融合。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散的生成模型在文本引导图像合成中缺乏可控性和布局一致性的问题。

Method: 采用分层编辑策略，分离文本与背景，引入深度感知模块以增强真实感和空间一致性。

Result: 在AnyWord-3M基准测试中表现出色，尤其在复杂变换场景下。

Conclusion: DanceText通过无需训练的设计，实现了高质量、可控的文本编辑。

Abstract: We present DanceText, a training-free framework for multilingual text editing
in images, designed to support complex geometric transformations and achieve
seamless foreground-background integration. While diffusion-based generative
models have shown promise in text-guided image synthesis, they often lack
controllability and fail to preserve layout consistency under non-trivial
manipulations such as rotation, translation, scaling, and warping. To address
these limitations, DanceText introduces a layered editing strategy that
separates text from the background, allowing geometric transformations to be
performed in a modular and controllable manner. A depth-aware module is further
proposed to align appearance and perspective between the transformed text and
the reconstructed background, enhancing photorealism and spatial consistency.
Importantly, DanceText adopts a fully training-free design by integrating
pretrained modules, allowing flexible deployment without task-specific
fine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that
our method achieves superior performance in visual quality, especially under
large-scale and complex transformation scenarios.

</details>


### [77] [Lightweight Road Environment Segmentation using Vector Quantization](https://arxiv.org/abs/2504.14113)
*Jiyong Kwag,Alper Yilmaz,Charles Toth*

Main category: cs.CV

TL;DR: 论文提出了一种基于向量量化的自动驾驶环境分割方法，通过离散化特征提升模型性能，实验结果表明其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于FCN和Transformer的方法依赖连续特征表示，限制了离散信息的提取能力，因此提出向量量化方法以解决这一问题。

Method: 结合向量量化与轻量级MobileUNETR模型，将连续特征映射为离散向量，优化特征表示。

Result: 在Cityscapes数据集上达到77.0% mIoU，比基线模型提升2.9%，且未增加模型复杂度。

Conclusion: 向量量化能有效提升自动驾驶环境分割的性能，为离散特征学习提供了新思路。

Abstract: Road environment segmentation plays a significant role in autonomous driving.
Numerous works based on Fully Convolutional Networks (FCNs) and Transformer
architectures have been proposed to leverage local and global contextual
learning for efficient and accurate semantic segmentation. In both
architectures, the encoder often relies heavily on extracting continuous
representations from the image, which limits the ability to represent
meaningful discrete information. To address this limitation, we propose
segmentation of the autonomous driving environment using vector quantization.
Vector quantization offers three primary advantages for road environment
segmentation. (1) Each continuous feature from the encoder is mapped to a
discrete vector from the codebook, helping the model discover distinct features
more easily than with complex continuous features. (2) Since a discrete feature
acts as compressed versions of the encoder's continuous features, they also
compress noise or outliers, enhancing the image segmentation task. (3) Vector
quantization encourages the latent space to form coarse clusters of continuous
features, forcing the model to group similar features, making the learned
representations more structured for the decoding process. In this work, we
combined vector quantization with the lightweight image segmentation model
MobileUNETR and used it as a baseline model for comparison to demonstrate its
efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,
outperforming the baseline by 2.9 % without increasing the model's initial size
or complexity.

</details>


### [78] [BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution](https://arxiv.org/abs/2504.14129)
*Yaning Zhang,Jiahe Zhang,Chunjie Ma,Weili Guan,Tian Gan,Zan Gao*

Main category: cs.CV

TL;DR: 提出了一种双模态引导的多视角表示学习框架（BMRL），用于零样本深度伪造溯源（ZS-DFA），通过视觉、解析和语言模态提升对未见生成器的溯源能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造溯源方法主要关注视觉模态，缺乏对其他模态（如文本和面部解析）的探索，且难以细粒度评估对未见生成器的泛化性能。

Method: 设计了多视角视觉编码器（MPVE）从图像、噪声和边缘三个视角提取特征；提出解析编码器和语言编码器分别学习全局面部属性和细粒度语言嵌入；引入深度伪造溯源对比中心（DFACC）损失优化模型。

Result: 实验表明，该方法在ZS-DFA任务上优于现有技术。

Conclusion: BMRL框架通过多模态融合和对比学习，显著提升了对未见生成器的溯源能力。

Abstract: The challenge of tracing the source attribution of forged faces has gained
significant attention due to the rapid advancement of generative models.
However, existing deepfake attribution (DFA) works primarily focus on the
interaction among various domains in vision modality, and other modalities such
as texts and face parsing are not fully explored. Besides, they tend to fail to
assess the generalization performance of deepfake attributors to unseen
generators in a fine-grained manner. In this paper, we propose a novel bi-modal
guided multi-perspective representation learning (BMRL) framework for zero-shot
deepfake attribution (ZS-DFA), which facilitates effective traceability to
unseen generators. Specifically, we design a multi-perspective visual encoder
(MPVE) to explore general deepfake attribution visual characteristics across
three views (i.e., image, noise, and edge). We devise a novel parsing encoder
to focus on global face attribute embeddings, enabling parsing-guided DFA
representation learning via vision-parsing matching. A language encoder is
proposed to capture fine-grained language embeddings, facilitating
language-guided general visual forgery representation learning through
vision-language alignment. Additionally, we present a novel deepfake
attribution contrastive center (DFACC) loss, to pull relevant generators closer
and push irrelevant ones away, which can be introduced into DFA models to
enhance traceability. Experimental results demonstrate that our method
outperforms the state-of-the-art on the ZS-DFA task through various protocols
evaluation.

</details>


### [79] [Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing](https://arxiv.org/abs/2504.14131)
*Ole-Christian Galbo Engstrøm,Michela Albano-Gaglio,Erik Schou Dreier,Yamine Bouzembrak,Maria Font-i-Furnols,Puneet Mishra,Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: 本研究提出了一种基于改进U-Net和自定义损失函数的深度学习方法来生成化学图谱，相比传统PLS回归方法，U-Net在预测精度和空间相关性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统化学图谱生成方法（如PLS回归）存在噪声高且忽略空间上下文的问题，本研究旨在通过深度学习改进这一过程。

Method: 使用改进的U-Net和自定义损失函数，直接从高光谱图像生成化学图谱，跳过传统像素级分析的中间步骤。

Result: U-Net在平均脂肪预测任务上的均方根误差比PLS低9%-13%，且99.91%的方差具有空间相关性，而PLS仅为2.53%。

Conclusion: U-Net在化学图谱生成中优于PLS回归，尤其在预测精度和空间相关性方面表现突出。

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>


### [80] [HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked Autoencoder for 3D Point Cloud Analysis](https://arxiv.org/abs/2504.14132)
*Xuanhua Yin,Dingxin Zhang,Jianhui Yu,Weidong Cai*

Main category: cs.CV

TL;DR: HFBRI-MAE是一种改进的自监督学习框架，通过引入旋转不变的手工特征，解决了现有MAE方法在旋转点云处理中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于MAE的自监督学习方法缺乏旋转不变性，导致在真实场景中处理任意旋转的点云时性能显著下降。

Method: HFBRI-MAE结合旋转不变的局部和全局特征进行令牌嵌入和位置嵌入，并重新定义重建目标为规范对齐版本。

Result: 在ModelNet40、ScanObjectNN和ShapeNetPart上的实验表明，HFBRI-MAE在分类、分割和少样本学习中优于现有方法。

Conclusion: HFBRI-MAE具有鲁棒性和强泛化能力，适用于真实世界的3D应用。

Abstract: Self-supervised learning (SSL) has demonstrated remarkable success in 3D
point cloud analysis, particularly through masked autoencoders (MAEs). However,
existing MAE-based methods lack rotation invariance, leading to significant
performance degradation when processing arbitrarily rotated point clouds in
real-world scenarios. To address this limitation, we introduce Handcrafted
Feature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel
framework that refines the MAE design with rotation-invariant handcrafted
features to ensure stable feature learning across different orientations. By
leveraging both rotation-invariant local and global features for token
embedding and position embedding, HFBRI-MAE effectively eliminates rotational
dependencies while preserving rich geometric structures. Additionally, we
redefine the reconstruction target to a canonically aligned version of the
input, mitigating rotational ambiguities. Extensive experiments on ModelNet40,
ScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently
outperforms existing methods in object classification, segmentation, and
few-shot learning, highlighting its robustness and strong generalization
ability in real-world 3D applications.

</details>


### [81] [Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach](https://arxiv.org/abs/2504.14137)
*Hangyu Liu,Bo Peng,Pengxiang Ding,Donglin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为2D-TGAF的多目标对抗攻击框架，利用扩散模型生成二维语义张量指导噪声生成，并通过实验验证其在攻击成功率上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法多从理论角度分析目标标签对噪声生成的影响，缺乏实践验证和全面总结，本文旨在填补这一空白。

Method: 提出2D-TGAF框架，利用扩散模型编码目标标签为二维语义张量，并设计新的掩码策略确保噪声保留目标类别的完整语义信息。

Result: 在ImageNet数据集上的实验表明，2D-TGAF在攻击成功率和对抗防御机制方面均优于现有方法。

Conclusion: 2D-TGAF通过提升语义特征的质量和数量，显著提高了多目标对抗攻击的效果。

Abstract: Compared to single-target adversarial attacks, multi-target attacks have
garnered significant attention due to their ability to generate adversarial
images for multiple target classes simultaneously. Existing generative
approaches for multi-target attacks mainly analyze the effect of the use of
target labels on noise generation from a theoretical perspective, lacking
practical validation and comprehensive summarization. To address this gap, we
first identify and validate that the semantic feature quality and quantity are
critical factors affecting the transferability of targeted attacks: 1) Feature
quality refers to the structural and detailed completeness of the implanted
target features, as deficiencies may result in the loss of key discriminative
information; 2) Feature quantity refers to the spatial sufficiency of the
implanted target features, as inadequacy limits the victim model's attention to
this feature. Based on these findings, we propose the 2D Tensor-Guided
Adversarial Fusion (2D-TGAF) framework, which leverages the powerful generative
capabilities of diffusion models to encode target labels into two-dimensional
semantic tensors for guiding adversarial noise generation. Additionally, we
design a novel masking strategy tailored for the training process, ensuring
that parts of the generated noise retain complete semantic information about
the target class. Extensive experiments on the standard ImageNet dataset
demonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in
attack success rates, both on normally trained models and across various
defense mechanisms.

</details>


### [82] [Segment Any Crack: Deep Semantic Segmentation Adaptation for Crack Detection](https://arxiv.org/abs/2504.14138)
*Ghodsiyeh Rostami,Po-Han Chen,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 提出了一种选择性微调策略，专注于调整归一化组件，以提高裂纹检测分割模型的适应性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有裂纹检测模型需要大量标注数据和计算资源，限制了其跨场景适应性。

Method: 采用选择性微调策略，仅调整归一化参数，应用于SAM及五种分割模型。

Result: 在OmniCrack30k数据集上，SAC模型达到61.22% F1-score和44.13% IoU，并在零样本数据集中表现最佳。

Conclusion: 选择性微调显著提升了分割精度并降低了计算开销。

Abstract: Image-based crack detection algorithms are increasingly in demand in
infrastructure monitoring, as early detection of cracks is of paramount
importance for timely maintenance planning. While deep learning has
significantly advanced crack detection algorithms, existing models often
require extensive labeled datasets and high computational costs for
fine-tuning, limiting their adaptability across diverse conditions. This study
introduces an efficient selective fine-tuning strategy, focusing on tuning
normalization components, to enhance the adaptability of segmentation models
for crack detection. The proposed method is applied to the Segment Anything
Model (SAM) and five well-established segmentation models. Experimental results
demonstrate that selective fine-tuning of only normalization parameters
outperforms full fine-tuning and other common fine-tuning techniques in both
performance and computational efficiency, while improving generalization. The
proposed approach yields a SAM-based model, Segment Any Crack (SAC), achieving
a 61.22\% F1-score and 44.13\% IoU on the OmniCrack30k benchmark dataset, along
with the highest performance across three zero-shot datasets and the lowest
standard deviation. The results highlight the effectiveness of the adaptation
approach in improving segmentation accuracy while significantly reducing
computational overhead.

</details>


### [83] [ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification](https://arxiv.org/abs/2504.14139)
*Hai Pham-Ngoc,De Nguyen-Van,Dung Vu-Tien,Phuong Le-Hong*

Main category: cs.CV

TL;DR: 开发了一种高效、可解释的深度学习系统，用于甲状腺细针穿刺活检图像的多类分类，实现了高准确率和低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺FNAB图像分类中的数据不足、观察者间差异和计算成本高的问题，为临床决策提供支持。

Method: 结合YOLOv10细胞簇检测、课程学习协议、轻量级EfficientNetB0和Transformer模块，进行多尺度多区域分析。

Result: 内部测试集宏F1为89.19%，外部验证AUCs为0.9495 (B2)、0.7436 (B5)、0.8396 (B6)，处理1000例仅需30秒。

Conclusion: 高准确率、可解释的甲状腺FNAB分类在低计算需求下可行。

Abstract: Background: Automated classification of thyroid fine needle aspiration biopsy
(FNAB) images faces challenges in limited data, inter-observer variability, and
computational cost. Efficient, interpretable models are crucial for clinical
support. Objective: To develop and externally validate a deep learning system
for the multi-class classification of thyroid FNAB images into three key
categories that directly guide post-biopsy treatment decisions in Vietnam:
benign (B2), suspicious for malignancy (B5), and malignant (B6), while
achieving high diagnostic accuracy with low computational overhead. Methods:
Our framework features: (1) YOLOv10-based cell cluster detection for
informative sub-region extraction and noise reduction; (2) a curriculum
learning-inspired protocol sequencing localized crops to full images for
multi-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4
millions parameters) selection balancing performance and efficiency; and (4) a
Transformer-inspired module for multi-scale, multi-region analysis. External
validation used 1,015 independent FNAB images. Results: ThyroidEffi Basic
achieved a macro F1 of 89.19\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)
on the internal test set. External validation yielded AUCs of 0.9495 (B2),
0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\%.
Grad-CAM highlighted key diagnostic regions, confirming interpretability. The
system processed 1000 cases in 30 seconds, demonstrating feasibility on widely
accessible hardware like a 12-core CPU. Conclusions: This work demonstrates
that high-accuracy, interpretable thyroid FNAB image classification is
achievable with minimal computational demands.

</details>


### [84] [Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D](https://arxiv.org/abs/2504.14151)
*Sergio Arnaud,Paul McVay,Ada Martin,Arjun Majumdar,Krishna Murthy Jatavallabhula,Phillip Thomas,Ruslan Partsey,Daniel Dugas,Abha Gejji,Alexander Sax,Vincent-Pierre Berges,Mikael Henaff,Ayush Jain,Ang Cao,Ishita Prasad,Mrinal Kalakrishnan,Michael Rabbat,Nicolas Ballas,Mido Assran,Oleksandr Maksymets,Aravind Rajeswaran,Franziska Meier*

Main category: cs.CV

TL;DR: LOCATE 3D是一个通过自然语言描述在3D场景中定位物体的模型，采用3D-JEPA自监督学习算法，结合2D基础模型处理3D点云，并在新数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景中基于自然语言描述的物体定位问题，提升机器人和AR设备的实际应用能力。

Method: 使用3D-JEPA自监督学习算法，结合2D基础模型（如CLIP、DINO）处理3D点云，通过掩码预测任务学习上下文特征，并微调编码器和语言条件解码器联合预测3D掩码和边界框。

Result: 在标准基准测试中达到新最优性能，展示了强大的泛化能力，并支持实时部署。

Conclusion: LOCATE 3D通过创新的自监督学习方法，在3D物体定位任务中表现出色，并提供了新数据集支持进一步研究。

Abstract: We present LOCATE 3D, a model for localizing objects in 3D scenes from
referring expressions like "the small coffee table between the sofa and the
lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding
benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D
operates directly on sensor observation streams (posed RGB-D frames), enabling
real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,
a novel self-supervised learning (SSL) algorithm applicable to sensor point
clouds. It takes as input a 3D pointcloud featurized using 2D foundation models
(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a
pretext task to aid the self-supervised learning of contextualized pointcloud
features. Once trained, the 3D-JEPA encoder is finetuned alongside a
language-conditioned decoder to jointly predict 3D masks and bounding boxes.
Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential
grounding, spanning multiple capture setups with over 130K annotations. This
enables a systematic study of generalization capabilities as well as a stronger
model.

</details>


### [85] [Segregation and Context Aggregation Network for Real-time Cloud Segmentation](https://arxiv.org/abs/2504.14178)
*Yijie Li,Hewei Wang,Jiayi Zhang,Jinjiang You,Jinfeng Xu,Puzhen Wu,Yunzhong Xiao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: SCANet是一种轻量级云分割模型，通过Segregation and Context Aggregation Module (SCAM)提升分割精度和计算效率，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分割精度和计算效率之间难以平衡，限制了在边缘设备上的实际应用。

Method: 提出SCANet模型，采用SCAM模块分别处理天空和云特征，并引入高效预训练策略。

Result: SCANet-large参数减少70.9%，性能接近最优；SCANet-lite速度达1390 fps。

Conclusion: SCANet在精度和效率上均表现优异，适合实时应用。

Abstract: Cloud segmentation from intensity images is a pivotal task in atmospheric
science and computer vision, aiding weather forecasting and climate analysis.
Ground-based sky/cloud segmentation extracts clouds from images for further
feature analysis. Existing methods struggle to balance segmentation accuracy
and computational efficiency, limiting real-world deployment on edge devices,
so we introduce SCANet, a novel lightweight cloud segmentation model featuring
Segregation and Context Aggregation Module (SCAM), which refines rough
segmentation maps into weighted sky and cloud features processed separately.
SCANet achieves state-of-the-art performance while drastically reducing
computational complexity. SCANet-large (4.29M) achieves comparable accuracy to
state-of-the-art methods with 70.9% fewer parameters. Meanwhile, SCANet-lite
(90K) delivers 1390 fps in FP16, surpassing real-time standards. Additionally,
we propose an efficient pre-training strategy that enhances performance even
without ImageNet pre-training.

</details>


### [86] [Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization](https://arxiv.org/abs/2504.14200)
*Huiyi Chen,Jiawei Peng,Kaihua Tang,Xin Geng,Xu Yang*

Main category: cs.CV

TL;DR: KeCO是一种基于视觉特征的紧凑核心集优化框架，显著提升图像分类任务中的上下文学习性能，计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法在图像分类中效率低且信息损失严重，需改进。

Method: 利用未利用数据构建核心集，通过视觉特征作为键，动态更新核心集样本。

Result: 在粗粒度和细粒度图像分类基准上，性能平均提升超过20%。

Conclusion: KeCO在资源受限场景中具有实用价值，性能显著优于现有方法。

Abstract: In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to
adapt to new tasks without parameter updates, using a few demonstrations from a
large support set. However, selecting informative demonstrations leads to high
computational and memory costs. While some methods explore selecting a small
and representative coreset in the text classification, evaluating all support
set samples remains costly, and discarded samples lead to unnecessary
information loss. These methods may also be less effective for image
classification due to differences in feature spaces. Given these limitations,
we propose Key-based Coreset Optimization (KeCO), a novel framework that
leverages untapped data to construct a compact and informative coreset. We
introduce visual features as keys within the coreset, which serve as the anchor
for identifying samples to be updated through different selection strategies.
By leveraging untapped samples from the support set, we update the keys of
selected coreset samples, enabling the randomly initialized coreset to evolve
into a more informative coreset under low computational cost. Through extensive
experiments on coarse-grained and fine-grained image classification benchmarks,
we demonstrate that KeCO effectively enhances ICL performance for image
classification task, achieving an average improvement of more than 20\%.
Notably, we evaluate KeCO under a simulated online scenario, and the strong
performance in this scenario highlights the practical value of our framework
for resource-constrained real-world scenarios.

</details>


### [87] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/abs/2504.14202)
*Zichuan Liu,Liming Jiang,Qing Yan,Yumin Jia,Hao Kang,Xin Lu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态编码策略的ID保留生成框架，通过FaceCLIP联合嵌入身份和文本语义，结合扩散模型生成身份一致且文本对齐的图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过适配器注入身份特征，而本文提出统一处理身份和文本作为条件输入，以提升生成效果。

Method: 引入FaceCLIP多模态编码器，学习身份与文本的联合嵌入空间，并结合扩散模型生成图像。使用多模态对齐算法训练FaceCLIP。

Result: FaceCLIP-SDXL在身份保留和文本相关性上优于现有方法，实验证明其定量和定性优势。

Conclusion: FaceCLIP-SDXL实现了更逼真的肖像生成，身份保留和文本对齐效果显著。

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>


### [88] [Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2504.14221)
*Wenbing Zhu,Lidong Wang,Ziqing Zhou,Chengjie Wang,Yurui Pan,Ruoyi Zhang,Zhuhao Chen,Linjie Cheng,Bin-Bin Gao,Jiangning Zhang,Zhenye Gan,Yuxie Wang,Yulong Chen,Shuguang Qian,Mingmin Chi,Bo Peng,Lizhuang Ma*

Main category: cs.CV

TL;DR: 论文提出了Real-IAD D3数据集，结合RGB、3D点云和伪3D模态，用于工业异常检测，并展示了多模态方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测的复杂性增加，但缺乏专门的多模态数据集，现有数据集如MVTec 3D在规模和分辨率上存在不足。

Method: 引入Real-IAD D3数据集，包含RGB、高精度3D点云和伪3D模态，并提出多模态融合方法。

Result: 实验表明多模态方法提升了检测鲁棒性和性能。

Conclusion: Real-IAD D3为多模态工业异常检测提供了新基准，数据集和代码已公开。

Abstract: The increasing complexity of industrial anomaly detection (IAD) has
positioned multimodal detection methods as a focal area of machine vision
research. However, dedicated multimodal datasets specifically tailored for IAD
remain limited. Pioneering datasets like MVTec 3D have laid essential
groundwork in multimodal IAD by incorporating RGB+3D data, but still face
challenges in bridging the gap with real industrial environments due to
limitations in scale and resolution. To address these challenges, we introduce
Real-IAD D3, a high-precision multimodal dataset that uniquely incorporates an
additional pseudo3D modality generated through photometric stereo, alongside
high-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3
features finer defects, diverse anomalies, and greater scale across 20
categories, providing a challenging benchmark for multimodal IAD Additionally,
we introduce an effective approach that integrates RGB, point cloud, and
pseudo-3D depth information to leverage the complementary strengths of each
modality, enhancing detection performance. Our experiments highlight the
importance of these modalities in boosting detection robustness and overall IAD
performance. The dataset and code are publicly accessible for research purposes
at https://realiad4ad.github.io/Real-IAD D3

</details>


### [89] [Revisiting CLIP for SF-OSDA: Unleashing Zero-Shot Potential with Adaptive Threshold and Training-Free Feature Filtering](https://arxiv.org/abs/2504.14224)
*Yongguang Li,Jindong Li,Qi Wang,Qianli Xing,Runliang Niu,Shengsheng Wang,Menglin Yang*

Main category: cs.CV

TL;DR: CLIPXpert提出了一种新的SF-OSDA方法，通过自适应阈值和未知类特征过滤模块，解决了现有方法依赖固定阈值和忽略类倾向性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SF-OSDA方法依赖固定阈值且忽略类倾向性，导致CLIP的零样本潜力未被充分利用。

Method: CLIPXpert结合了BGAT模块（动态确定阈值）和SUFF模块（过滤未知类特征），无需训练且源自由。

Result: 在DomainNet上优于UOTA 1.92%，在Office-Home等数据集上达到SOTA水平。

Conclusion: CLIPXpert验证了CLIP在SF-OSDA任务中的零样本潜力，并展示了方法的有效性。

Abstract: Source-Free Unsupervised Open-Set Domain Adaptation (SF-OSDA) methods using
CLIP face significant issues: (1) while heavily dependent on domain-specific
threshold selection, existing methods employ simple fixed thresholds,
underutilizing CLIP's zero-shot potential in SF-OSDA scenarios; and (2)
overlook intrinsic class tendencies while employing complex training to enforce
feature separation, incurring deployment costs and feature shifts that
compromise CLIP's generalization ability. To address these issues, we propose
CLIPXpert, a novel SF-OSDA approach that integrates two key components: an
adaptive thresholding strategy and an unknown class feature filtering module.
Specifically, the Box-Cox GMM-Based Adaptive Thresholding (BGAT) module
dynamically determines the optimal threshold by estimating sample score
distributions, balancing known class recognition and unknown class sample
detection. Additionally, the Singular Value Decomposition (SVD)-Based
Unknown-Class Feature Filtering (SUFF) module reduces the tendency of unknown
class samples towards known classes, improving the separation between known and
unknown classes. Experiments show that our source-free and training-free method
outperforms state-of-the-art trained approach UOTA by 1.92% on the DomainNet
dataset, achieves SOTA-comparable performance on datasets such as Office-Home,
and surpasses other SF-OSDA methods. This not only validates the effectiveness
of our proposed method but also highlights CLIP's strong zero-shot potential
for SF-OSDA tasks.

</details>


### [90] [Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA in 3D Semantic Segmentation](https://arxiv.org/abs/2504.14231)
*Johannes Spoecklberger,Wei Lin,Pedro Hermosilla,Sivan Doveh,Horst Possegger,M. Jehanzeb Mirza*

Main category: cs.CV

TL;DR: 本文探讨了视觉基础模型（VFMs）在LiDAR 3D语义分割任务中的应用，通过融合2D-3D数据提升性能。


<details>
  <summary>Details</summary>
Motivation: VFMs在多模态任务中潜力巨大，但尚未充分用于3D任务。本文旨在利用VFMs的跨模态信息提升3D语义分割性能。

Method: 提出一种融合网络，结合2D图像和3D点云数据，动态调整两者贡献，利用VFMs的鲁棒特征训练3D主干网络。

Result: 在多个实验中显著优于现有方法，平均提升6.5 mIoU。

Conclusion: VFMs在3D任务中具有显著潜力，融合2D-3D数据能有效提升性能。

Abstract: Vision Foundation Models (VFMs) have become a de facto choice for many
downstream vision tasks, like image classification, image segmentation, and
object localization. However, they can also provide significant utility for
downstream 3D tasks that can leverage the cross-modal information (e.g., from
paired image data). In our work, we further explore the utility of VFMs for
adapting from a labeled source to unlabeled target data for the task of
LiDAR-based 3D semantic segmentation. Our method consumes paired 2D-3D (image
and point cloud) data and relies on the robust (cross-domain) features from a
VFM to train a 3D backbone on a mix of labeled source and unlabeled target
data. At the heart of our method lies a fusion network that is guided by both
the image and point cloud streams, with their relative contributions adjusted
based on the target domain. We extensively compare our proposed methodology
with different state-of-the-art methods in several settings and achieve strong
performance gains. For example, achieving an average improvement of 6.5 mIoU
(over all tasks), when compared with the previous state-of-the-art.

</details>


### [91] [Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network](https://arxiv.org/abs/2504.14238)
*Lu Pan,Yu-Hsuan Huang,Hongxia Xie,Cheng Zhang,Hongwei Zhao,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 论文提出了DocHR14K数据集和L2HRNet网络，用于解决文档图像中的高光问题，显著提升了高光去除效果。


<details>
  <summary>Details</summary>
Motivation: 文档图像在环境光下易产生高光，影响文本可读性和视觉质量，现有深度学习方法因缺乏专用数据集和针对性设计而效果不佳。

Method: 提出DocHR14K数据集和基于高光位置先验（HLP）的L2HRNet网络，利用残差图估计高光区域，并通过扩散模块恢复细节。

Result: DocHR14K提升了高光去除效果，L2HRNet在多个基准数据集上达到最优性能，PSNR提升5.01%，RMSE降低13.17%。

Conclusion: DocHR14K和L2HRNet为文档高光去除提供了有效解决方案，显著提升了性能。

Abstract: Reflective documents often suffer from specular highlights under ambient
lighting, severely hindering text readability and degrading overall visual
quality. Although recent deep learning methods show promise in highlight
removal, they remain suboptimal for document images, primarily due to the lack
of dedicated datasets and tailored architectural designs. To tackle these
challenges, we present DocHR14K, a large-scale real-world dataset comprising
14,902 high-resolution image pairs across six document categories and various
lighting conditions. To the best of our knowledge, this is the first
high-resolution dataset for document highlight removal that captures a wide
range of real-world lighting conditions. Additionally, motivated by the
observation that the residual map between highlighted and clean images
naturally reveals the spatial structure of highlight regions, we propose a
simple yet effective Highlight Location Prior (HLP) to estimate highlight masks
without human annotations. Building on this prior, we present the
Location-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which
effectively removes highlights by leveraging estimated priors and incorporates
diffusion module to restore details. Extensive experiments demonstrate that
DocHR14K improves highlight removal under diverse lighting conditions. Our
L2HRNet achieves state-of-the-art performance across three benchmark datasets,
including a 5.01\% increase in PSNR and a 13.17\% reduction in RMSE on
DocHR14K.

</details>


### [92] [ROI-Guided Point Cloud Geometry Compression Towards Human and Machine Vision](https://arxiv.org/abs/2504.14240)
*Xie Liang,Gao Wei,Zhenghui Ming,Li Ge*

Main category: cs.CV

TL;DR: 论文提出了一种ROI引导的点云几何压缩方法（RPCGC），通过双分支并行结构优化压缩性能，同时保留语义细节，提升下游任务准确性。


<details>
  <summary>Details</summary>
Motivation: 点云数据在存储和传输中面临体积大的挑战，传统压缩方法会损害语义细节，影响下游任务准确性。

Method: 采用双分支并行结构：基础层简化点云，增强层聚焦几何细节；通过ROI预测网络优化残差信息，并结合RD优化和检测损失。

Result: 在ScanNet和SUN RGB-D数据集上，RPCGC在高比特率下实现了优异的压缩性能和10%的检测精度提升。

Conclusion: RPCGC方法有效平衡了压缩比和语义细节保留，提升了机器视觉任务的准确性。

Abstract: Point cloud data is pivotal in applications like autonomous driving, virtual
reality, and robotics. However, its substantial volume poses significant
challenges in storage and transmission. In order to obtain a high compression
ratio, crucial semantic details usually confront severe damage, leading to
difficulties in guaranteeing the accuracy of downstream tasks. To tackle this
problem, we are the first to introduce a novel Region of Interest (ROI)-guided
Point Cloud Geometry Compression (RPCGC) method for human and machine vision.
Our framework employs a dual-branch parallel structure, where the base layer
encodes and decodes a simplified version of the point cloud, and the
enhancement layer refines this by focusing on geometry details. Furthermore,
the residual information of the enhancement layer undergoes refinement through
an ROI prediction network. This network generates mask information, which is
then incorporated into the residuals, serving as a strong supervision signal.
Additionally, we intricately apply these mask details in the Rate-Distortion
(RD) optimization process, with each point weighted in the distortion
calculation. Our loss function includes RD loss and detection loss to better
guide point cloud encoding for the machine. Experiment results demonstrate that
RPCGC achieves exceptional compression performance and better detection
accuracy (10% gain) than some learning-based compression methods at high
bitrates in ScanNet and SUN RGB-D datasets.

</details>


### [93] [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
*Yikun Ji,Yan Hong,Jiahui Zhan,Haoxing Chen,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 论文探讨了利用多模态大语言模型（MLLMs）进行AI生成图像检测的潜力，提出了一种结合六种提示的框架，旨在实现更强泛化性和透明性。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成技术带来的公共安全问题，强调检测方法需具备泛化性和透明性。

Method: 评估MLLMs与传统检测方法及人类评估者的能力，设计六种提示并整合为框架。

Result: 展示了MLLMs在检测中的优势和局限性，提出了更鲁棒、可解释的检测系统。

Conclusion: MLLMs为AI生成图像检测提供了新思路，结合提示的框架有望提升检测效果。

Abstract: Progress in image generation raises significant public security concerns. We
argue that fake image detection should not operate as a "black box". Instead,
an ideal approach must ensure both strong generalization and transparency.
Recent progress in Multi-modal Large Language Models (MLLMs) offers new
opportunities for reasoning-based AI-generated image detection. In this work,
we evaluate the capabilities of MLLMs in comparison to traditional detection
methods and human evaluators, highlighting their strengths and limitations.
Furthermore, we design six distinct prompts and propose a framework that
integrates these prompts to develop a more robust, explainable, and
reasoning-driven detection system. The code is available at
https://github.com/Gennadiyev/mllm-defake.

</details>


### [94] [Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation](https://arxiv.org/abs/2504.14249)
*Bin Ren,Eduard Zamfir,Zongwei Wu,Yawei Li,Yidi Li,Danda Pani Paudel,Radu Timofte,Ming-Hsuan Yang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: AnyIR提出了一种统一的图像修复方法，通过联合嵌入机制高效处理多种退化，无需增加模型规模或依赖大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法需为每种退化训练专用模型，效率低下；近期方法则增加模块复杂度或依赖大型模型。AnyIR旨在解决这些问题。

Method: 利用输入的子潜在空间识别关键组件并加权，提出空间-频率并行融合策略，增强局部-全局交互和频率细节。

Result: 在多项基准测试中表现最佳，模型参数减少82%，计算量减少85%。

Conclusion: AnyIR提供了一种高效、统一的图像修复方案，显著降低了模型复杂度和计算成本。

Abstract: Restoring any degraded image efficiently via just one model has become
increasingly significant and impactful, especially with the proliferation of
mobile devices. Traditional solutions typically involve training dedicated
models per degradation, resulting in inefficiency and redundancy. More recent
approaches either introduce additional modules to learn visual prompts,
significantly increasing model size, or incorporate cross-modal transfer from
large language models trained on vast datasets, adding complexity to the system
architecture. In contrast, our approach, termed AnyIR, takes a unified path
that leverages inherent similarity across various degradations to enable both
efficient and comprehensive restoration through a joint embedding mechanism,
without scaling up the model or relying on large language models.Specifically,
we examine the sub-latent space of each input, identifying key components and
reweighting them first in a gated manner. To fuse the intrinsic degradation
awareness and the contextualized attention, a spatial-frequency parallel fusion
strategy is proposed for enhancing spatial-aware local-global interactions and
enriching the restoration details from the frequency perspective. Extensive
benchmarking in the all-in-one restoration setting confirms AnyIR's SOTA
performance, reducing model complexity by around 82\% in parameters and 85\% in
FLOPs. Our code will be available at our Project page
(https://amazingren.github.io/AnyIR/)

</details>


### [95] [ColorVein: Colorful Cancelable Vein Biometrics](https://arxiv.org/abs/2504.14253)
*Yifan Wang,Jie Gui,Xinli Shi,Linqing Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TL;DR: 本文提出了一种名为ColorVein的可取消静脉生物特征生成方案，通过引入颜色信息增强图像信息密度，并优化特征提取模型以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对静脉生物特征的可取消模板生成方案，生物信息泄露可能威胁用户隐私。

Method: ColorVein通过交互式着色将静态灰度信息转换为动态可控的颜色表示，并引入安全中心损失优化特征提取模型。

Result: ColorVein在识别性能、不可链接性、不可逆性和可撤销性方面表现优异，与现有方法竞争力相当。

Conclusion: ColorVein为静脉生物特征提供了一种安全且高效的可取消模板生成方案。

Abstract: Vein recognition technologies have become one of the primary solutions for
high-security identification systems. However, the issue of biometric
information leakage can still pose a serious threat to user privacy and
anonymity. Currently, there is no cancelable biometric template generation
scheme specifically designed for vein biometrics. Therefore, this paper
proposes an innovative cancelable vein biometric generation scheme: ColorVein.
Unlike previous cancelable template generation schemes, ColorVein does not
destroy the original biometric features and introduces additional color
information to grayscale vein images. This method significantly enhances the
information density of vein images by transforming static grayscale information
into dynamically controllable color representations through interactive
colorization. ColorVein allows users/administrators to define a controllable
pseudo-random color space for grayscale vein images by editing the position,
number, and color of hint points, thereby generating protected cancelable
templates. Additionally, we propose a new secure center loss to optimize the
training process of the protected feature extraction model, effectively
increasing the feature distance between enrolled users and any potential
impostors. Finally, we evaluate ColorVein's performance on all types of vein
biometrics, including recognition performance, unlinkability, irreversibility,
and revocability, and conduct security and privacy analyses. ColorVein achieves
competitive performance compared with state-of-the-art methods.

</details>


### [96] [Visual Consensus Prompting for Co-Salient Object Detection](https://arxiv.org/abs/2504.14254)
*Jie Wang,Nana Yu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: 本文提出了一种交互高效且参数简洁的共显著目标检测（CoSOD）架构，解决了现有方法的两大局限性：共识提取与编码阶段缺乏及时交互，以及全局参数更新效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CoSOD方法依赖三阶段架构和全微调范式，存在共识提取与编码阶段缺乏交互、参数更新效率低的问题。本文旨在通过参数高效的提示调优范式改进这些问题。

Method: 提出了一种基于视觉共识提示（VCP）的架构，包括共识提示生成器（CPG）和共识提示分散器（CPD），通过最小化可调参数实现任务特定的共识提取。

Result: 在最具挑战性的CoCA数据集上，VCP在F_m指标上提升了6.8%，优于13种前沿全微调模型。

Conclusion: VCP通过参数高效的提示调优和任务特定共识提取，显著提升了CoSOD任务的性能，成为新的state-of-the-art方法。

Abstract: Existing co-salient object detection (CoSOD) methods generally employ a
three-stage architecture (i.e., encoding, consensus extraction & dispersion,
and prediction) along with a typical full fine-tuning paradigm. Although they
yield certain benefits, they exhibit two notable limitations: 1) This
architecture relies on encoded features to facilitate consensus extraction, but
the meticulously extracted consensus does not provide timely guidance to the
encoding stage. 2) This paradigm involves globally updating all parameters of
the model, which is parameter-inefficient and hinders the effective
representation of knowledge within the foundation model for this task.
Therefore, in this paper, we propose an interaction-effective and
parameter-efficient concise architecture for the CoSOD task, addressing two key
limitations. It introduces, for the first time, a parameter-efficient prompt
tuning paradigm and seamlessly embeds consensus into the prompts to formulate
task-specific Visual Consensus Prompts (VCP). Our VCP aims to induce the frozen
foundation model to perform better on CoSOD tasks by formulating task-specific
visual consensus prompts with minimized tunable parameters. Concretely, the
primary insight of the purposeful Consensus Prompt Generator (CPG) is to
enforce limited tunable parameters to focus on co-salient representations and
generate consensus prompts. The formulated Consensus Prompt Disperser (CPD)
leverages consensus prompts to form task-specific visual consensus prompts,
thereby arousing the powerful potential of pre-trained models in addressing
CoSOD tasks. Extensive experiments demonstrate that our concise VCP outperforms
13 cutting-edge full fine-tuning models, achieving the new state of the art
(with 6.8% improvement in F_m metrics on the most challenging CoCA dataset).
Source code has been available at https://github.com/WJ-CV/VCP.

</details>


### [97] [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
*Liu Xiao,Li Zhiyuan,Lin Yueyu*

Main category: cs.CV

TL;DR: CrossWKV是一种新型的跨注意力机制，用于增强RWKV-7模型在文本到图像生成中的表达能力，结合了线性复杂度的WKV架构和LoRA技术，实现了卓越的跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 提升文本到图像生成的表达能力，同时保持线性复杂度和低内存占用。

Method: 利用RWKV-7的WKV架构，结合广义delta规则、向量门控和LoRA技术，实现跨模态对齐。

Result: 在DIR-7框架下，CrossWKV在ImageNet 256x256上取得了FID 2.88和CLIP分数0.33的优异表现。

Conclusion: CrossWKV在跨模态任务中表现出色，具有高分辨率和动态状态操作的潜力。

Abstract: We introduce CrossWKV, a novel cross-attention mechanism for the state-based
RWKV-7 model, designed to enhance the expressive power of text-to-image
generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)
architecture, CrossWKV integrates text and image modalities in a single pass,
utilizing a generalized delta rule with vector-valued gating and low-rank
adaptations (LoRA) to achieve superior cross-modal alignment. Unlike
Transformer-based models, CrossWKV's non-diagonal, input-dependent transition
matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$
complexity class, including all regular languages, as demonstrated by its
ability to perform state-tracking tasks like $S_5$ permutation modeling.
Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B
and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and
a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance
while offering robust generalization across diverse prompts. The model's
enhanced expressivity, combined with constant memory usage and linear scaling,
positions it as a powerful solution for advanced cross-modal tasks, with
potential applications in high-resolution generation and dynamic state
manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention

</details>


### [98] [Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction](https://arxiv.org/abs/2504.14267)
*Li Yu,Xuanzhe Sun,Wei Zhou,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本-音频-视觉的多模态扩散模型TAVDiff，用于视频显著性预测，通过解耦条件信息和时间步长，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 视频显著性预测对下游应用至关重要，而多模态学习的发展促使研究者探索结合视觉、听觉和文本信息的方法，以提高预测准确性。

Method: TAVDiff将视频显著性预测视为基于文本、音频和视觉输入的图像生成任务，通过逐步去噪预测显著性图。采用大型多模态模型生成文本描述，并引入SITR机制和Saliency-DiT结构。

Result: 实验结果表明，TAVDiff在SIM、CC、NSS和AUC-J指标上分别提升了1.03%、2.35%、2.71%和0.33%，优于现有方法。

Conclusion: TAVDiff通过结合多模态信息和改进的扩散模型结构，显著提升了视频显著性预测的准确性。

Abstract: Video saliency prediction is crucial for downstream applications, such as
video compression and human-computer interaction. With the flourishing of
multimodal learning, researchers started to explore multimodal video saliency
prediction, including audio-visual and text-visual approaches. Auditory cues
guide the gaze of viewers to sound sources, while textual cues provide semantic
guidance for understanding video content. Integrating these complementary cues
can improve the accuracy of saliency prediction. Therefore, we attempt to
simultaneously analyze visual, auditory, and textual modalities in this paper,
and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video
saliency prediction. TAVDiff treats video saliency prediction as an image
generation task conditioned on textual, audio, and visual inputs, and predicts
saliency maps through stepwise denoising. To effectively utilize text, a large
multimodal model is used to generate textual descriptions for video frames and
introduce a saliency-oriented image-text response (SITR) mechanism to generate
image-text response maps. It is used as conditional information to guide the
model to localize the visual regions that are semantically related to the
textual description. Regarding the auditory modality, it is used as another
conditional information for directing the model to focus on salient regions
indicated by sounds. At the same time, since the diffusion transformer (DiT)
directly concatenates the conditional information with the timestep, which may
affect the estimation of the noise level. To achieve effective conditional
guidance, we propose Saliency-DiT, which decouples the conditional information
from the timestep. Experimental results show that TAVDiff outperforms existing
methods, improving 1.03\%, 2.35\%, 2.71\% and 0.33\% on SIM, CC, NSS and AUC-J
metrics, respectively.

</details>


### [99] [RAMCT: Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking](https://arxiv.org/abs/2504.14278)
*Shang Zhang,Yuke Hou,Guoqiang Gong,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TL;DR: RAMCT是一种区域自适应的稀疏相关滤波器跟踪器，通过多通道特征优化和自适应正则化策略，解决了热红外目标跟踪中的低分辨率、遮挡、背景干扰和目标变形等问题。


<details>
  <summary>Details</summary>
Motivation: 现有相关滤波器跟踪器在热红外目标跟踪中面临低分辨率、遮挡、背景干扰和目标变形等挑战，影响了跟踪性能。

Method: RAMCT引入空间自适应二值掩码优化CF学习过程，提出基于GSVD的区域自适应迭代Tikhonov正则化方法，并采用动态参数调整的在线优化策略。

Result: 在多个基准测试中，RAMCT在准确性和鲁棒性上优于其他先进跟踪器。

Conclusion: RAMCT通过自适应和优化策略显著提升了热红外目标跟踪的性能。

Abstract: Correlation filter (CF)-based trackers have gained significant attention for
their computational efficiency in thermal infrared (TIR) target tracking.
However, ex-isting methods struggle with challenges such as low-resolution
imagery, occlu-sion, background clutter, and target deformation, which severely
impact tracking performance. To overcome these limitations, we propose RAMCT, a
region-adaptive sparse correlation filter tracker that integrates multi-channel
feature opti-mization with an adaptive regularization strategy. Firstly, we
refine the CF learn-ing process by introducing a spatially adaptive binary
mask, which enforces spar-sity in the target region while dynamically
suppressing background interference. Secondly, we introduce generalized
singular value decomposition (GSVD) and propose a novel GSVD-based
region-adaptive iterative Tikhonov regularization method. This enables flexible
and robust optimization across multiple feature channels, improving resilience
to occlusion and background variations. Thirdly, we propose an online
optimization strategy with dynamic discrepancy-based pa-rameter adjustment.
This mechanism facilitates real time adaptation to target and background
variations, thereby improving tracking accuracy and robustness. Ex-tensive
experiments on LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks
demonstrate that RAMCT outperforms other state-of-the-art trackers in terms of
accuracy and robustness.

</details>


### [100] [CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey](https://arxiv.org/abs/2504.14280)
*Jindong Li,Yongguang Li,Yali Fu,Jiahong Liu,Yixin Liu,Menglin Yang,Irwin King*

Main category: cs.CV

TL;DR: 该论文综述了CLIP在领域泛化（DG）和领域适应（DA）中的应用，填补了现有文献的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对CLIP在DG和DA中应用的系统性综述，作者希望通过此综述填补这一空白，并为研究者和实践者提供指导。

Method: 在DG中，方法分为优化提示学习和利用CLIP作为特征提取主干；在DA中，分为基于源数据的方法和基于目标数据的源无关方法。

Result: 综述总结了CLIP在DG和DA中的关键应用、挑战（如过拟合和计算效率）及未来机会。

Conclusion: 该工作为提升模型在多样化环境中的鲁棒性提供了方向，旨在推动更可靠的机器学习模型的发展。

Abstract: As machine learning evolves, domain generalization (DG) and domain adaptation
(DA) have become crucial for enhancing model robustness across diverse
environments. Contrastive Language-Image Pretraining (CLIP) plays a significant
role in these tasks, offering powerful zero-shot capabilities that allow models
to perform effectively in unseen domains. However, there remains a significant
gap in the literature, as no comprehensive survey currently exists that
systematically explores the applications of CLIP in DG and DA, highlighting the
necessity for this review. This survey presents a comprehensive review of
CLIP's applications in DG and DA. In DG, we categorize methods into optimizing
prompt learning for task alignment and leveraging CLIP as a backbone for
effective feature extraction, both enhancing model adaptability. For DA, we
examine both source-available methods utilizing labeled source data and
source-free approaches primarily based on target domain data, emphasizing
knowledge transfer mechanisms and strategies for improved performance across
diverse contexts. Key challenges, including overfitting, domain diversity, and
computational efficiency, are addressed, alongside future research
opportunities to advance robustness and efficiency in practical applications.
By synthesizing existing literature and pinpointing critical gaps, this survey
provides valuable insights for researchers and practitioners, proposing
directions for effectively leveraging CLIP to enhance methodologies in domain
generalization and adaptation. Ultimately, this work aims to foster innovation
and collaboration in the quest for more resilient machine learning models that
can perform reliably across diverse real-world scenarios. A more up-to-date
version of the papers is maintained at:
https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.

</details>


### [101] [ISTD-YOLO: A Multi-Scale Lightweight High-Performance Infrared Small Target Detection Algorithm](https://arxiv.org/abs/2504.14289)
*Shang Zhang,Yujie Cui,Ruoyan Xiong,Huanbin Zhang*

Main category: cs.CV

TL;DR: ISTD-YOLO是一种基于改进YOLOv7的轻量级红外小目标检测算法，通过轻量化重构、引入无参数注意力机制和优化指标NWD，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 针对红外图像检测中背景复杂、信噪比低、目标小且亮度弱等难点，提出改进算法以提高检测精度。

Method: 1. 轻量化重构YOLOv7网络结构；2. 用VoV-GSCSP替换ELAN-W模块；3. 引入无参数注意力机制；4. 使用NWD优化IoU指标。

Result: 实验表明，ISTD-YOLO相比YOLOv7及其他主流算法，检测效果显著提升，各项指标均有改善。

Conclusion: ISTD-YOLO能有效实现红外小目标的高质量检测。

Abstract: Aiming at the detection difficulties of infrared images such as complex
background, low signal-to-noise ratio, small target size and weak brightness, a
lightweight infrared small target detection algorithm ISTD-YOLO based on
improved YOLOv7 was proposed. Firstly, the YOLOv7 network structure was
lightweight reconstructed, and a three-scale lightweight network architecture
was designed. Then, the ELAN-W module of the model neck network is replaced by
VoV-GSCSP to reduce the computational cost and the complexity of the network
structure. Secondly, a parameter-free attention mechanism was introduced into
the neck network to enhance the relevance of local con-text information.
Finally, the Normalized Wasserstein Distance (NWD) was used to optimize the
commonly used IoU index to enhance the localization and detection accuracy of
small targets. Experimental results show that compared with YOLOv7 and the
current mainstream algorithms, ISTD-YOLO can effectively improve the detection
effect, and all indicators are effectively improved, which can achieve
high-quality detection of infrared small targets.

</details>


### [102] [Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization](https://arxiv.org/abs/2504.14290)
*Shouwei Ruan,Zhenyu Wu,Yao Huang,Ruochen Zhang,Yitong Sun,Caixin Kang,Xingxing Wei*

Main category: cs.CV

TL;DR: SC-DPO是一个新颖的框架，用于解决文本到图像生成中的安全问题，通过结合安全约束和人类偏好校准，有效平衡安全性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法完全保证有害概念下的安全性或难以平衡安全性与生成质量，因此需要一种更有效的方法。

Method: 提出SC-DPO框架，包括安全成本模型、对比学习与成本锚定目标，并构建SCP-10K数据集和动态聚焦机制（DFM）。

Result: SC-DPO在实验中表现优于现有方法，能有效防御NSFW内容并保持高质量生成。

Conclusion: SC-DPO成功解决了安全性与生成质量的平衡问题，并具备对抗恶意提示的鲁棒性。

Abstract: Ensuring the safety of generated content remains a fundamental challenge for
Text-to-Image (T2I) generation. Existing studies either fail to guarantee
complete safety under potentially harmful concepts or struggle to balance
safety with generation quality. To address these issues, we propose
Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework
for safety alignment in T2I models. SC-DPO integrates safety constraints into
the general human preference calibration, aiming to maximize the likelihood of
generating human-preferred samples while minimizing the safety cost of the
generated outputs. In SC-DPO, we introduce a safety cost model to accurately
quantify harmful levels for images, and train it effectively using the proposed
contrastive learning and cost anchoring objectives. To apply SC-DPO for
effective T2I safety alignment, we constructed SCP-10K, a safety-constrained
preference dataset containing rich harmful concepts, which blends
safety-constrained preference pairs under both harmful and clean instructions,
further mitigating the trade-off between safety and sample quality.
Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO,
promoting the model's learning of difficult preference pair samples. Extensive
experiments demonstrate that SC-DPO outperforms existing methods, effectively
defending against various NSFW content while maintaining optimal sample quality
and human preference alignment. Additionally, SC-DPO exhibits resilience
against adversarial prompts designed to generate harmful content.

</details>


### [103] [From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion](https://arxiv.org/abs/2504.14294)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Huiyu Zhou,Michael Felsberg,Dacheng Tao,Xuelong Li*

Main category: cs.CV

TL;DR: ConFill通过引入上下文自适应差异模型和动态采样机制，解决了扩散模型在图像补全中空间和语义对齐不足的问题，显著提升了补全效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像补全中因缺乏显式的空间和语义对齐，导致生成内容与原图不连贯。

Method: 提出ConFill框架，包含上下文自适应差异模型（CAD）和动态采样机制，逐步减少生成与原始图像的差异。

Result: 实验表明ConFill优于现有方法，成为图像补全的新标杆。

Conclusion: ConFill通过改进对齐和采样机制，显著提升了图像补全的质量和连贯性。

Abstract: Image completion is a challenging task, particularly when ensuring that
generated content seamlessly integrates with existing parts of an image. While
recent diffusion models have shown promise, they often struggle with
maintaining coherence between known and unknown (missing) regions. This issue
arises from the lack of explicit spatial and semantic alignment during the
diffusion process, resulting in content that does not smoothly integrate with
the original image. Additionally, diffusion models typically rely on global
learned distributions rather than localized features, leading to
inconsistencies between the generated and existing image parts. In this work,
we propose ConFill, a novel framework that introduces a Context-Adaptive
Discrepancy (CAD) model to ensure that intermediate distributions of known and
unknown regions are closely aligned throughout the diffusion process. By
incorporating CAD, our model progressively reduces discrepancies between
generated and original images at each diffusion step, leading to contextually
aligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism
that adaptively increases the sampling rate in regions with high reconstruction
complexity. This approach enables precise adjustments, enhancing detail and
integration in restored areas. Extensive experiments demonstrate that ConFill
outperforms current methods, setting a new benchmark in image completion.

</details>


### [104] [Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization](https://arxiv.org/abs/2504.14301)
*Nazia Aslam,Kamal Nasrollahi*

Main category: cs.CV

TL;DR: 论文提出了一种隐私保护的图像匿名化技术，通过优化匿名化器以平衡隐私保护和动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频监控系统中隐私保护与动作识别性能之间的权衡问题，确保符合欧盟AI法案和GDPR标准。

Method: 采用基于特征的惩罚方案，优化匿名化器以减少隐私泄漏并保持动作识别性能。

Result: 实验表明，该方法在保持隐私泄漏一致的同时，显著提升了动作识别性能。

Conclusion: 该方法有效平衡了隐私保护与性能需求，为隐私保护技术提供了新思路。

Abstract: The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.

</details>


### [105] [Exploring Generalizable Pre-training for Real-world Change Detection via Geometric Estimation](https://arxiv.org/abs/2504.14306)
*Yitao Zhao,Sen Lei,Nanqing Liu,Heng-Chao Li,Turgay Celik,Qing Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为MatchCD的自监督变化检测框架，通过几何估计解决多时相图像未对齐和对象变化问题，无需手动配准，可直接处理大尺度图像。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测算法需手动配准多时相图像，增加了工作流程的复杂性。

Method: 利用自监督对比表示优化编码器，结合几何估计，同时处理图像未对齐和对象变化问题。

Result: 在几何畸变显著的复杂场景中表现优异，可直接处理高分辨率图像。

Conclusion: MatchCD框架有效简化了变化检测流程，提升了性能。

Abstract: As an essential procedure in earth observation system, change detection (CD)
aims to reveal the spatial-temporal evolution of the observation regions. A key
prerequisite for existing change detection algorithms is aligned geo-references
between multi-temporal images by fine-grained registration. However, in the
majority of real-world scenarios, a prior manual registration is required
between the original images, which significantly increases the complexity of
the CD workflow. In this paper, we proposed a self-supervision motivated CD
framework with geometric estimation, called "MatchCD". Specifically, the
proposed MatchCD framework utilizes the zero-shot capability to optimize the
encoder with self-supervised contrastive representation, which is reused in the
downstream image registration and change detection to simultaneously handle the
bi-temporal unalignment and object change issues. Moreover, unlike the
conventional change detection requiring segmenting the full-frame image into
small patches, our MatchCD framework can directly process the original
large-scale image (e.g., 6K*4K resolutions) with promising performance. The
performance in multiple complex scenarios with significant geometric distortion
demonstrates the effectiveness of our proposed framework.

</details>


### [106] [FGSGT: Saliency-Guided Siamese Network Tracker Based on Key Fine-Grained Feature Information for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14309)
*Ruoyan Xiong,Huanbin Zhang,Shentao Wang,Hui He,Yuke Hou,Yue Zhang,Yujie Cui,Huipan Guan,Shang Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于显著性引导的Siamese网络跟踪器，通过细粒度特征并行学习模块和多层特征融合模块，解决了TIR图像特征提取的挑战，显著提升了跟踪精度。


<details>
  <summary>Details</summary>
Motivation: TIR图像特征细节少、对比度低，传统特征提取模型难以捕捉目标特征，导致跟踪器易受干扰和漂移。

Method: 设计了细粒度特征并行学习卷积块和多层特征融合模块，结合Siamese残差细化块和显著性损失函数，优化特征提取和预测。

Result: 在PTB-TIR、LSOTB-TIR和VOT-TIR基准测试中取得了最高精度和成功率。

Conclusion: 所提方法显著提升了TIR图像跟踪的性能，适用于复杂场景。

Abstract: Thermal infrared (TIR) images typically lack detailed features and have low
contrast, making it challenging for conventional feature extraction models to
capture discriminative target characteristics. As a result, trackers are often
affected by interference from visually similar objects and are susceptible to
tracking drift. To address these challenges, we propose a novel saliency-guided
Siamese network tracker based on key fine-grained feature infor-mation. First,
we introduce a fine-grained feature parallel learning convolu-tional block with
a dual-stream architecture and convolutional kernels of varying sizes. This
design captures essential global features from shallow layers, enhances feature
diversity, and minimizes the loss of fine-grained in-formation typically
encountered in residual connections. In addition, we propose a multi-layer
fine-grained feature fusion module that uses bilinear matrix multiplication to
effectively integrate features across both deep and shallow layers. Next, we
introduce a Siamese residual refinement block that corrects saliency map
prediction errors using residual learning. Combined with deep supervision, this
mechanism progressively refines predictions, ap-plying supervision at each
recursive step to ensure consistent improvements in accuracy. Finally, we
present a saliency loss function to constrain the sali-ency predictions,
directing the network to focus on highly discriminative fi-ne-grained features.
Extensive experiment results demonstrate that the pro-posed tracker achieves
the highest precision and success rates on the PTB-TIR and LSOTB-TIR
benchmarks. It also achieves a top accuracy of 0.78 on the VOT-TIR 2015
benchmark and 0.75 on the VOT-TIR 2017 benchmark.

</details>


### [107] [DCFG: Diverse Cross-Channel Fine-Grained Feature Learning and Progressive Fusion Siamese Tracker for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14311)
*Ruoyan Xiong,Yuke Hou,Princess Retor Torboh,Hui He,Huanbin Zhang,Yue Zhang,Yanpin Wang,Huipan Guan,Shang Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于跨通道细粒度特征学习和渐进融合的新型Siamese跟踪器，用于热红外（TIR）跟踪，显著提升了目标表示的细节捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 解决热红外跟踪中难以捕捉高判别性特征的挑战。

Method: 1. 跨通道细粒度特征学习网络，通过掩码和抑制系数抑制主导目标特征；2. 引入通道重排和均衡机制减少参数；3. 逐层组合单元优化特征提取与融合；4. 设计跨通道细粒度损失函数，提升特征多样性。

Result: 在VOT-TIR 2015和2017基准测试中分别达到0.81和0.78的准确率，并在其他基准测试中全面优于现有方法。

Conclusion: 该方法通过细粒度特征学习和渐进融合，显著提升了热红外跟踪的性能和效率。

Abstract: To address the challenge of capturing highly discriminative features in
ther-mal infrared (TIR) tracking, we propose a novel Siamese tracker based on
cross-channel fine-grained feature learning and progressive fusion. First, we
introduce a cross-channel fine-grained feature learning network that employs
masks and suppression coefficients to suppress dominant target features,
en-abling the tracker to capture more detailed and subtle information. The
net-work employs a channel rearrangement mechanism to enhance efficient
in-formation flow, coupled with channel equalization to reduce parameter count.
Additionally, we incorporate layer-by-layer combination units for ef-fective
feature extraction and fusion, thereby minimizing parameter redun-dancy and
computational complexity. The network further employs feature redirection and
channel shuffling strategies to better integrate fine-grained details. Second,
we propose a specialized cross-channel fine-grained loss function designed to
guide feature groups toward distinct discriminative re-gions of the target,
thus improving overall target representation. This loss function includes an
inter-channel loss term that promotes orthogonality be-tween channels,
maximizing feature diversity and facilitating finer detail capture. Extensive
experiments demonstrate that our proposed tracker achieves the highest
accuracy, scoring 0.81 on the VOT-TIR 2015 and 0.78 on the VOT-TIR 2017
benchmark, while also outperforming other methods across all evaluation metrics
on the LSOTB-TIR and PTB-TIR benchmarks.

</details>


### [108] [Visual Prompting for One-shot Controllable Video Editing without Inversion](https://arxiv.org/abs/2504.14335)
*Zhengbo Zhang,Yuxi Zhou,Duo Peng,Joo-Hwee Lim,Zhigang Tu,De Wen Soh,Lin Geng Foo*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉提示和内容一致性采样的方法，用于一次性可控视频编辑（OCVE），避免了DDIM反演带来的误差，并确保了内容一致性和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因DDIM反演误差导致的内容一致性问题，提升视频编辑的准确性和一致性。

Method: 采用视觉提示替代DDIM反演，提出内容一致性采样（CCS）和时间一致性采样（TCS）以确保内容一致性和时间一致性。

Result: 实验验证了方法的有效性，能够生成内容一致且时间一致的编辑视频。

Conclusion: 该方法通过消除DDIM反演误差并引入一致性采样，显著提升了OCVE任务的效果。

Abstract: One-shot controllable video editing (OCVE) is an important yet challenging
task, aiming to propagate user edits that are made -- using any image editing
tool -- on the first frame of a video to all subsequent frames, while ensuring
content consistency between edited frames and source frames. To achieve this,
prior methods employ DDIM inversion to transform source frames into latent
noise, which is then fed into a pre-trained diffusion model, conditioned on the
user-edited first frame, to generate the edited video. However, the DDIM
inversion process accumulates errors, which hinder the latent noise from
accurately reconstructing the source frames, ultimately compromising content
consistency in the generated edited frames. To overcome it, our method
eliminates the need for DDIM inversion by performing OCVE through a novel
perspective based on visual prompting. Furthermore, inspired by consistency
models that can perform multi-step consistency sampling to generate a sequence
of content-consistent images, we propose a content consistency sampling (CCS)
to ensure content consistency between the generated edited frames and the
source frames. Moreover, we introduce a temporal-content consistency sampling
(TCS) based on Stein Variational Gradient Descent to ensure temporal
consistency across the edited frames. Extensive experiments validate the
effectiveness of our approach.

</details>


### [109] [Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms](https://arxiv.org/abs/2504.14337)
*Josef Taher,Eric Hyyppä,Matti Hyyppä,Klaara Salolahti,Xiaowei Yu,Leena Matikainen,Antero Kukko,Matti Lehtomäki,Harri Kaartinen,Sopitta Thurachen,Paula Litkey,Ville Luoma,Markus Holopainen,Gefei Kong,Hongchao Fan,Petri Rönnholm,Antti Polvivaara,Samuli Junttila,Mikko Vastaranta,Stefano Puliti,Rasmus Astrup,Joel Kostensalo,Mari Myllymäki,Maksymilian Kulicki,Krzysztof Stereńczak,Raul de Paula Pires,Ruben Valbuena,Juan Pedro Carbonell-Rivera,Jesús Torralba,Yi-Chen Chen,Lukas Winiwarter,Markus Hollaus,Gottfried Mandlburger,Narges Takhtkeshha,Fabio Remondino,Maciej Lisiewicz,Bartłomiej Kraszewski,Xinlian Liang,Jianchang Chen,Eero Ahokas,Kirsi Karila,Eugeniu Vezeteu,Petri Manninen,Roope Näsi,Heikki Hyyti,Siiri Pyykkönen,Peilun Hu,Juha Hyyppä*

Main category: cs.CV

TL;DR: 该研究通过对比机器学习和深度学习方法，评估了多光谱ALS数据在树种分类中的表现，发现点变换器模型在高密度数据上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决多光谱ALS数据在树种分类中的挑战，尤其是稀有树种的识别和深度学习技术的应用。

Method: 收集高密度多光谱ALS数据，对比点变换器模型、传统机器学习和图像深度学习方法在树种分类中的准确性。

Result: 点变换器模型在高密度数据上表现最优，整体准确率达87.9%，光谱信息显著提升分类性能。

Conclusion: 点变换器模型结合多光谱ALS数据是树种分类的高效方法，光谱信息对提升准确性至关重要。

Abstract: Climate-smart and biodiversity-preserving forestry demands precise
information on forest resources, extending to the individual tree level.
Multispectral airborne laser scanning (ALS) has shown promise in automated
point cloud processing and tree segmentation, but challenges remain in
identifying rare tree species and leveraging deep learning techniques. This
study addresses these gaps by conducting a comprehensive benchmark of machine
learning and deep learning methods for tree species classification. For the
study, we collected high-density multispectral ALS data (>1000 pts/m$^2$) at
three wavelengths using the FGI-developed HeliALS system, complemented by
existing Optech Titan data (35 pts/m$^2$), to evaluate the species
classification accuracy of various algorithms in a test site located in
Southern Finland. Based on 5261 test segments, our findings demonstrate that
point-based deep learning methods, particularly a point transformer model,
outperformed traditional machine learning and image-based deep learning
approaches on high-density multispectral point clouds. For the high-density ALS
dataset, a point transformer model provided the best performance reaching an
overall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065
segments and 92.0% (85.1%) with 5000 training segments. The best image-based
deep learning method, DetailView, reached an overall (macro-average) accuracy
of 84.3% (63.9%), whereas a random forest (RF) classifier achieved an overall
(macro-average) accuracy of 83.2% (61.3%). Importantly, the overall
classification accuracy of the point transformer model on the HeliALS data
increased from 73.0% with no spectral information to 84.7% with single-channel
reflectance, and to 87.9% with spectral information of all the three channels.

</details>


### [110] [Manipulating Multimodal Agents via Cross-Modal Prompt Injection](https://arxiv.org/abs/2504.14348)
*Le Wang,Zonghao Ying,Tianyuan Zhang,Siyuan Liang,Shengshan Hu,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种针对多模态代理的新型安全漏洞攻击方法CrossInject，通过跨模态提示注入攻击，利用视觉和文本模态的对抗扰动，成功劫持代理的决策过程。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的兴起为代理提供了更强的任务执行能力，但也暴露了安全漏洞。本文旨在揭示并利用这种跨模态提示注入漏洞。

Method: 提出CrossInject攻击框架，包括视觉潜在对齐和文本引导增强两部分，分别优化对抗图像和生成恶意文本指令。

Result: 实验表明，该方法在攻击成功率上比现有方法至少提高26.4%，并在真实多模态自主代理中验证了有效性。

Conclusion: CrossInject展示了多模态代理的安全隐患，对安全关键应用具有潜在影响。

Abstract: The emergence of multimodal large language models has redefined the agent
paradigm by integrating language and vision modalities with external data
sources, enabling agents to better interpret human instructions and execute
increasingly complex tasks. However, in this work, we identify a critical yet
previously overlooked security vulnerability in multimodal agents: cross-modal
prompt injection attacks. To exploit this vulnerability, we propose
CrossInject, a novel attack framework in which attackers embed adversarial
perturbations across multiple modalities to align with target malicious
content, allowing external instructions to hijack the agent's decision-making
process and execute unauthorized tasks. Our approach consists of two key
components. First, we introduce Visual Latent Alignment, where we optimize
adversarial features to the malicious instructions in the visual embedding
space based on a text-to-image generative model, ensuring that adversarial
images subtly encode cues for malicious task execution. Subsequently, we
present Textual Guidance Enhancement, where a large language model is leveraged
to infer the black-box defensive system prompt through adversarial meta
prompting and generate an malicious textual command that steers the agent's
output toward better compliance with attackers' requests. Extensive experiments
demonstrate that our method outperforms existing injection attacks, achieving
at least a +26.4% increase in attack success rates across diverse tasks.
Furthermore, we validate our attack's effectiveness in real-world multimodal
autonomous agents, highlighting its potential implications for safety-critical
applications.

</details>


### [111] [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
*Kyle Buettner,Jacob Emmerson,Adriana Kovashka*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的多模态重标注策略，通过修改英文标注以提升多语言视觉语言模型对感知多样性的理解，显著提升了德日文本图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型多依赖英语数据，导致感知偏见和灵活性不足，需解决多语言和文化多样性问题。

Method: 采用LLM驱动的多模态重标注策略，修改英文标注后翻译，并结合母语数据指导的多模态机制。

Result: 德日文本图像检索性能显著提升（平均召回率提升3.5，非母语错误案例提升4.7）。

Conclusion: 通过数据高效框架和重标注策略，增强了模型对感知多样性的理解，并提供了跨数据集和跨语言泛化的分析机制。

Abstract: There are many ways to describe, name, and group objects when captioning an
image. Differences are evident when speakers come from diverse cultures due to
the unique experiences that shape perception. Machine translation of captions
has pushed multilingual capabilities in vision-language models (VLMs), but data
comes mainly from English speakers, indicating a perceptual bias and lack of
model flexibility. In this work, we address this challenge and outline a
data-efficient framework to instill multilingual VLMs with greater
understanding of perceptual diversity. We specifically propose an LLM-based,
multimodal recaptioning strategy that alters the object descriptions of English
captions before translation. The greatest benefits are demonstrated in a
targeted multimodal mechanism guided by native speaker data. By adding produced
rewrites as augmentations in training, we improve on German and Japanese
text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on
non-native error cases). We further propose a mechanism to analyze the specific
object description differences across datasets, and we offer insights into
cross-dataset and cross-language generalization.

</details>


### [112] [Efficient Spiking Point Mamba for Point Cloud Analysis](https://arxiv.org/abs/2504.14371)
*Peixi Wu,Bosong Chai,Menghua Zheng,Wei Li,Zhangchi Hu,Jie Chen,Zheyu Zhang,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的3D脉冲神经网络（SPM），结合Mamba的序列建模能力和SNN的时间特征提取，显著提升了性能并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D SNN在长程依赖建模上表现不佳，Mamba的高效计算和序列建模能力为解决这一问题提供了可能。

Method: 设计了Hierarchical Dynamic Encoding（HDE）和Spiking Mamba Block（SMB），并采用非对称SNN-ANN架构进行预训练和微调。

Result: 在ScanObjectNN和ShapeNetPart数据集上性能显著提升，能耗比ANN低至少3.5倍。

Conclusion: SPM是首个基于Mamba的3D SNN，有效结合了两者的优势，实现了高性能和低能耗。

Abstract: Bio-inspired Spiking Neural Networks (SNNs) provide an energy-efficient way
to extract 3D spatio-temporal features. However, existing 3D SNNs have
struggled with long-range dependencies until the recent emergence of Mamba,
which offers superior computational efficiency and sequence modeling
capability. In this work, we propose Spiking Point Mamba (SPM), the first
Mamba-based SNN in the 3D domain. Due to the poor performance of simply
transferring Mamba to 3D SNNs, SPM is designed to utilize both the sequence
modeling capabilities of Mamba and the temporal feature extraction of SNNs.
Specifically, we first introduce Hierarchical Dynamic Encoding (HDE), an
improved direct encoding method that effectively introduces dynamic temporal
mechanism, thereby facilitating temporal interactions. Then, we propose a
Spiking Mamba Block (SMB), which builds upon Mamba while learning
inter-time-step features and minimizing information loss caused by spikes.
Finally, to further enhance model performance, we adopt an asymmetric SNN-ANN
architecture for spike-based pre-training and finetune. Compared with the
previous state-of-the-art SNN models, SPM improves OA by +6.2%, +6.1%, and
+7.4% on three variants of ScanObjectNN, and boosts instance mIOU by +1.9% on
ShapeNetPart. Meanwhile, its energy consumption is at least 3.5x lower than
that of its ANN counterpart. The code will be made publicly available.

</details>


### [113] [LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers](https://arxiv.org/abs/2504.14386)
*Md Abtahi Majeed Chowdhury,Md Rifat Ur Rahman,Akil Ahmad Taki*

Main category: cs.CV

TL;DR: 论文提出了一种名为LOOPE的可学习补丁排序方法，优化了位置嵌入的空间表示，并通过实验验证了其在ViT架构中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了补丁排序对位置嵌入的影响，导致2D网格到1D序列映射的挑战。

Method: 提出LOOPE方法，优化补丁排序以增强空间表示，并引入“Three Cell Experiment”评估框架。

Result: LOOPE显著提升了分类准确率，实验显示其能更敏感地评估位置嵌入效果，性能差距达30-35%。

Conclusion: LOOPE在保留相对和绝对位置信息方面表现优异，为位置嵌入提供了更有效的解决方案。

Abstract: Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)
by providing spatial information otherwise lost due to the permutation
invariant nature of self attention. While absolute positional embeddings (APE)
have shown theoretical advantages over relative positional embeddings (RPE),
particularly due to the ability of sinusoidal functions to preserve spatial
inductive biases like monotonicity and shift invariance, a fundamental
challenge arises when mapping a 2D grid to a 1D sequence. Existing methods have
mostly overlooked or never explored the impact of patch ordering in positional
embeddings. To address this, we propose LOOPE, a learnable patch-ordering
method that optimizes spatial representation for a given set of frequencies,
providing a principled approach to patch order optimization. Empirical results
show that our PE significantly improves classification accuracy across various
ViT architectures. To rigorously evaluate the effectiveness of positional
embeddings, we introduce the "Three Cell Experiment", a novel benchmarking
framework that assesses the ability of PEs to retain relative and absolute
positional information across different ViT architectures. Unlike standard
evaluations, which typically report a performance gap of 4 to 6% between models
with and without PE, our method reveals a striking 30 to 35% difference,
offering a more sensitive diagnostic tool to measure the efficacy of PEs. Our
experimental analysis confirms that the proposed LOOPE demonstrates enhanced
effectiveness in retaining both relative and absolute positional information.

</details>


### [114] [How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?](https://arxiv.org/abs/2504.14391)
*Rahul Thapa,Andrew Li,Qingyang Wu,Bryan He,Yuki Sahashi,Christina Binder,Angela Zhang,Ben Athiwaratkun,Shuaiwen Leon Song,David Ouyang,James Zou*

Main category: cs.CV

TL;DR: OpenBiomedVi数据集利用生物医学教育视频训练视觉语言模型，显著提升性能，并引入新基准测试验证模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索非标准化、教学性强的生物医学视频是否能有效训练通用视觉语言模型。

Method: 构建OpenBiomedVi数据集，包含1031小时视频-字幕和Q/A对，通过多步人工参与流程筛选。

Result: 微调后的模型在视频和图像任务上表现显著提升，2B模型在视频任务上提升98.7%，7B模型在视频任务上提升37.09%。

Conclusion: 教育视频为生物医学视觉语言模型提供了有效的训练信号。

Abstract: Publicly available biomedical videos, such as those on YouTube, serve as
valuable educational resources for medical students. Unlike standard machine
learning datasets, these videos are designed for human learners, often mixing
medical imagery with narration, explanatory diagrams, and contextual framing.
In this work, we investigate whether such pedagogically rich, yet
non-standardized and heterogeneous videos can effectively teach general-domain
vision-language models biomedical knowledge. To this end, we introduce
OpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031
hours of video-caption and Q/A pairs, curated through a multi-step
human-in-the-loop pipeline. Diverse biomedical video datasets are rare, and
OpenBiomedVid fills an important gap by providing instruction-style supervision
grounded in real-world educational content. Surprisingly, despite the informal
and heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models
exhibit substantial performance improvements across most benchmarks. The 2B
model achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on
text tasks. The 7B model shows improvements of 37.09% on video and 11.2% on
image tasks, with a slight degradation of 2.7% on text tasks compared to their
respective base models. To address the lack of standardized biomedical video
evaluation datasets, we also introduce two new expert curated benchmarks,
MIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves
gains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%,
respectively, demonstrating the models' ability to generalize and perform
biomedical video understanding on cleaner and more standardized datasets than
those seen during training. These results suggest that educational videos
created for human learning offer a surprisingly effective training signal for
biomedical VLMs.

</details>


### [115] [Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models](https://arxiv.org/abs/2504.14395)
*Chung-En,Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Hydra是一个自适应代理框架，通过迭代推理、结构化批判和跨模型验证提升视觉语言模型（VLMs）的鲁棒性，同时解决对抗性攻击和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中（如国防和医疗），视觉语言模型的对抗鲁棒性和幻觉问题影响事实准确性，现有方法缺乏统一的解决方案。

Method: Hydra采用Action-Critique循环，结合Chain-of-Thought和In-Context Learning技术，动态优化输出。

Result: 在四个VLMs、三个幻觉基准和两种对抗攻击策略上的评估显示，Hydra优于现有插件VLMs和去幻觉方法。

Conclusion: Hydra为提升VLMs在现实应用中的可靠性提供了一种无需训练的解决方案。

Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to
address adversarial robustness and hallucination mitigation, both of which
impact factual accuracy in high-stakes applications such as defense and
healthcare. Existing methods primarily focus on either adversarial defense or
hallucination post-hoc correction, leaving a gap in unified robustness
strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that
enhances plug-in VLMs through iterative reasoning, structured critiques, and
cross-model verification, improving both resilience to adversarial
perturbations and intrinsic model errors. Hydra employs an Action-Critique
Loop, where it retrieves and critiques visual information, leveraging
Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine
outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to
both adversarial manipulations and intrinsic model errors, making it robust to
malicious perturbations and hallucination-related inaccuracies. We evaluate
Hydra on four VLMs, three hallucination benchmarks, two adversarial attack
strategies, and two adversarial defense methods, assessing performance on both
clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs
and state-of-the-art (SOTA) dehallucination methods, even without explicit
adversarial defenses, demonstrating enhanced robustness and factual
consistency. By bridging adversarial resistance and hallucination mitigation,
Hydra provides a scalable, training-free solution for improving the reliability
of VLMs in real-world applications.

</details>


### [116] [SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation](https://arxiv.org/abs/2504.14396)
*Minho Park,Taewoong Kang,Jooyeol Yun,Sungwon Hwang,Jaegul Choo*

Main category: cs.CV

TL;DR: SphereDiff提出了一种基于扩散模型的无缝360度全景图像和视频生成方法，解决了传统ERP投影的失真问题。


<details>
  <summary>Details</summary>
Motivation: AR/VR应用对高质量360度全景内容的需求增加，但现有方法因ERP投影的严重失真而受限。

Method: 定义球形潜在表示，扩展MultiDiffusion到球形空间，提出球形潜在采样和失真感知加权平均。

Result: SphereDiff在生成360度全景内容时优于现有方法，保持高保真度。

Conclusion: SphereDiff为AR/VR应用提供了高质量的360度全景内容生成解决方案。

Abstract: The increasing demand for AR/VR applications has highlighted the need for
high-quality 360-degree panoramic content. However, generating high-quality
360-degree panoramic images and videos remains a challenging task due to the
severe distortions introduced by equirectangular projection (ERP). Existing
approaches either fine-tune pretrained diffusion models on limited ERP datasets
or attempt tuning-free methods that still rely on ERP latent representations,
leading to discontinuities near the poles. In this paper, we introduce
SphereDiff, a novel approach for seamless 360-degree panoramic image and video
generation using state-of-the-art diffusion models without additional tuning.
We define a spherical latent representation that ensures uniform distribution
across all perspectives, mitigating the distortions inherent in ERP. We extend
MultiDiffusion to spherical latent space and propose a spherical latent
sampling method to enable direct use of pretrained diffusion models. Moreover,
we introduce distortion-aware weighted averaging to further improve the
generation quality in the projection process. Our method outperforms existing
approaches in generating 360-degree panoramic content while maintaining high
fidelity, making it a robust solution for immersive AR/VR applications. The
code is available here. https://github.com/pmh9960/SphereDiff

</details>


### [117] [Adversarial Attack for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2504.14423)
*Qiang Chen,Xiao Wang,Haowen Wang,Bo Jiang,Lin Zhu,Dawei Zhang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了一种针对RGB-Event视觉跟踪的跨模态对抗攻击算法，研究了Event体素和帧两种表示形式，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管RGB-Event流跟踪算法在精度和效率上取得了进展，但对其对抗攻击和防御的研究较少，本文旨在填补这一空白。

Method: 针对RGB-Event体素，通过对抗损失优化扰动生成对抗样本；对离散Event体素提出两步攻击策略；对RGB-Event帧跟踪，通过多模态数据梯度信息优化跨模态通用扰动。

Result: 在COESOT、FE108和VisEvent数据集上的实验表明，该方法显著降低了跟踪器在单模态和多模态场景下的性能。

Conclusion: 本文提出的跨模态对抗攻击算法有效，未来可进一步研究防御策略。

Abstract: Visual object tracking is a crucial research topic in the fields of computer
vision and multi-modal fusion. Among various approaches, robust visual tracking
that combines RGB frames with Event streams has attracted increasing attention
from researchers. While striving for high accuracy and efficiency in tracking,
it is also important to explore how to effectively conduct adversarial attacks
and defenses on RGB-Event stream tracking algorithms, yet research in this area
remains relatively scarce. To bridge this gap, in this paper, we propose a
cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because
of the diverse representations of Event streams, and given that Event voxels
and frames are more commonly used, this paper will focus on these two
representations for an in-depth study. Specifically, for the RGB-Event voxel,
we first optimize the perturbation by adversarial loss to generate RGB frame
adversarial examples. For discrete Event voxel representations, we propose a
two-step attack strategy, more in detail, we first inject Event voxels into the
target region as initialized adversarial examples, then, conduct a
gradient-guided optimization by perturbing the spatial location of the Event
voxels. For the RGB-Event frame based tracking, we optimize the cross-modal
universal perturbation by integrating the gradient information from multimodal
data. We evaluate the proposed approach against attacks on three widely used
RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive
experiments show that our method significantly reduces the performance of the
tracker across numerous datasets in both unimodal and multimodal scenarios. The
source code will be released on
https://github.com/Event-AHU/Adversarial_Attack_Defense

</details>


### [118] [ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations](https://arxiv.org/abs/2504.14429)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: 论文提出了一种针对视频语言模型ResNetVLLM的幻觉问题解决方案，通过两步策略（检测与缓解）显著提升了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和视频语言模型（VideoLLMs）存在幻觉问题，生成的内容可能事实错误。本文旨在解决这一问题。

Method: 提出两步策略：1）使用改进的Lynx模型检测生成内容与真实视频的语义对齐；2）采用检索增强生成（RAG）动态构建知识库以缓解幻觉。

Result: 改进后的ResNetVLLM-2模型在ActivityNet-QA基准测试中准确率从54.8%提升至65.3%。

Conclusion: 两步策略有效减少了多模态幻觉，提升了视频语言模型的可靠性。

Abstract: Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

</details>


### [119] [ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](https://arxiv.org/abs/2504.14432)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: ResNetVLLM是一个结合ResNet视觉编码器和大型语言模型的跨模态框架，用于零样本视频理解，无需依赖预训练视频模型，表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决零样本视频理解中依赖预训练模型的挑战，提出统一架构学习视觉和语义表示。

Method: 使用非预训练的ResNet提取视觉特征，结合LLM生成文本描述。

Result: 在多个基准测试（如MSRVTT-QA、MSVD-QA等）中达到最先进性能。

Conclusion: ResNetVLLM通过统一架构显著提升了零样本视频理解的准确性和上下文相关性。

Abstract: In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel
cross-modal framework for zero-shot video understanding that integrates a
ResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM
addresses the challenges associated with zero-shot video models by avoiding
reliance on pre-trained video understanding models and instead employing a
non-pretrained ResNet to extract visual features. This design ensures the model
learns visual and semantic representations within a unified architecture,
enhancing its ability to generate accurate and contextually relevant textual
descriptions from video inputs. Our experimental results demonstrate that
ResNetVLLM achieves state-of-the-art performance in zero-shot video
understanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,
TGIF-QA FrameQA, and ActivityNet-QA.

</details>


### [120] [WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2504.14445)
*Mingya Zhang,Liang Wang,Limei Gu,Tingsheng Ling,Xianping Tao*

Main category: cs.CV

TL;DR: 提出了一种基于小波变换的双向复制粘贴半监督医学图像分割框架（WT-BCP），解决了标注与未标注数据分布不匹配、人工扰动导致的训练偏差以及低频和高频信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 半监督医学图像分割（SSMIS）依赖稀缺的标注数据，但面临标注与未标注数据分布不匹配、人工扰动偏差和低频/高频信息利用不足的挑战。

Method: WT-BCP框架通过双向复制粘贴增强未标注数据的理解，利用小波变换提取低频和高频信息，并设计多输入多输出模型XNet-Plus进行信息融合。通过一致性训练减少人工扰动偏差。

Result: 在2D和3D数据集上的实验验证了模型的有效性。

Conclusion: WT-BCP框架显著提升了半监督医学图像分割的性能，解决了现有方法的局限性。

Abstract: Semi-supervised medical image segmentation (SSMIS) shows promise in reducing
reliance on scarce labeled medical data. However, SSMIS field confronts
challenges such as distribution mismatches between labeled and unlabeled data,
artificial perturbations causing training biases, and inadequate use of raw
image information, especially low-frequency (LF) and high-frequency (HF)
components.To address these challenges, we propose a Wavelet Transform based
Bidirectional Copy-Paste SSMIS framework, named WT-BCP, which improves upon the
Mean Teacher approach. Our method enhances unlabeled data understanding by
copying random crops between labeled and unlabeled images and employs WT to
extract LF and HF details.We propose a multi-input and multi-output model named
XNet-Plus, to receive the fused information after WT. Moreover, consistency
training among multiple outputs helps to mitigate learning biases introduced by
artificial perturbations. During consistency training, the mixed images
resulting from WT are fed into both models, with the student model's output
being supervised by pseudo-labels and ground-truth. Extensive experiments
conducted on 2D and 3D datasets confirm the effectiveness of our model.Code:
https://github.com/simzhangbest/WT-BCP.

</details>


### [121] [Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability](https://arxiv.org/abs/2504.14446)
*Carlos Caetano,Gabriel O. dos Santos,Caio Petrucci,Artur Barros,Camila Laranjeira,Leo S. F. Ribeiro,Júlia F. de Mendonça,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.CV

TL;DR: 论文探讨了在AI数据集中使用儿童图像的伦理问题，并提出了一种检测和移除这些图像的流程。


<details>
  <summary>Details</summary>
Motivation: 儿童图像的公开使用涉及隐私、同意和数据保护等伦理问题，但目前缺乏有效的解决方法。

Method: 提出了一种基于视觉语言模型的流程，并在Visual Question Answering任务和#PraCegoVer数据集上测试，同时在Open Images V7数据集的子集上评估其有效性。

Result: 流程为未来研究提供了基线，呼吁社区反思并采取行动保护儿童权利。

Conclusion: 研究旨在鼓励社区在创建数据集时更加谨慎，并开发工具保护弱势群体权利。

Abstract: Including children's images in datasets has raised ethical concerns,
particularly regarding privacy, consent, data protection, and accountability.
These datasets, often built by scraping publicly available images from the
Internet, can expose children to risks such as exploitation, profiling, and
tracking. Despite the growing recognition of these issues, approaches for
addressing them remain limited. We explore the ethical implications of using
children's images in AI datasets and propose a pipeline to detect and remove
such images. As a use case, we built the pipeline on a Vision-Language Model
under the Visual Question Answering task and tested it on the #PraCegoVer
dataset. We also evaluate the pipeline on a subset of 100,000 images from the
Open Images V7 dataset to assess its effectiveness in detecting and removing
images of children. The pipeline serves as a baseline for future research,
providing a starting point for more comprehensive tools and methodologies.
While we leverage existing models trained on potentially problematic data, our
goal is to expose and address this issue. We do not advocate for training or
deploying such models, but instead call for urgent community reflection and
action to protect children's rights. Ultimately, we aim to encourage the
research community to exercise - more than an additional - care in creating new
datasets and to inspire the development of tools to protect the fundamental
rights of vulnerable groups, particularly children.

</details>


### [122] [Causal Disentanglement for Robust Long-tail Medical Image Generation](https://arxiv.org/abs/2504.14450)
*Weizhi Nie,Zichun Zhang,Weijie Wang,Bruno Lepri,Anan Liu,Nicu Seb*

Main category: cs.CV

TL;DR: 论文提出了一种基于因果解耦和文本引导的医学图像生成框架，旨在解决数据稀缺和类别不平衡问题，生成高质量且多样化的医学图像。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据稀缺且类别分布不平衡，生成高质量且多样化的图像具有挑战性。同时，需要利用有限数据中的解剖结构信息，避免生成图像的结构失真或不一致。

Method: 通过因果解耦实现病理和结构特征分离，并引入组监督确保独立性；利用扩散模型和文本引导建模病理特征；通过大语言模型提取病变信息优化生成。

Result: 生成具有临床相关性和结构稳定性的多样化医学图像，同时通过初始噪声优化提升长尾类别性能。

Conclusion: 该框架有效提升了医学图像生成的多样性和质量，增强了模型的解释性和临床实用性。

Abstract: Counterfactual medical image generation effectively addresses data scarcity
and enhances the interpretability of medical images. However, due to the
complex and diverse pathological features of medical images and the imbalanced
class distribution in medical data, generating high-quality and diverse medical
images from limited data is significantly challenging. Additionally, to fully
leverage the information in limited data, such as anatomical structure
information and generate more structurally stable medical images while avoiding
distortion or inconsistency. In this paper, in order to enhance the clinical
relevance of generated data and improve the interpretability of the model, we
propose a novel medical image generation framework, which generates independent
pathological and structural features based on causal disentanglement and
utilizes text-guided modeling of pathological features to regulate the
generation of counterfactual images. First, we achieve feature separation
through causal disentanglement and analyze the interactions between features.
Here, we introduce group supervision to ensure the independence of pathological
and identity features. Second, we leverage a diffusion model guided by
pathological findings to model pathological features, enabling the generation
of diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging
a large language model to extract lesion severity and location from medical
reports. Additionally, we improve the performance of the latent diffusion model
on long-tailed categories through initial noise optimization.

</details>


### [123] [Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding](https://arxiv.org/abs/2504.14460)
*Junyan Su,Baozhu Zhao,Xiaohan Zhang,Qi Liu*

Main category: cs.CV

TL;DR: Metamon-GS通过方差引导的密集化策略和多级哈希网格，解决了3D高斯泼溅（3DGS）在渲染性能和颜色表示上的问题，显著提升了新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS在渲染性能和颜色表示上存在挑战，如颜色失真和模糊问题，需要改进。

Method: 提出方差引导的密集化策略和多级哈希网格，分别优化高斯点生长和颜色表示。

Result: 实验表明，Metamon-GS在公开数据集上表现优于基线模型和先前版本。

Conclusion: Metamon-GS通过创新方法显著提升了3DGS的渲染质量。

Abstract: The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view
synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point
features with anchor embeddings has significantly enhanced the performance of
newer 3DGS variants. While significant advances have been made, it is still
challenging to boost rendering performance. Feature embeddings have difficulty
accurately representing colors from different perspectives under varying
lighting conditions, which leads to a washed-out appearance. Another reason is
the lack of a proper densification strategy that prevents Gaussian point growth
in thinly initialized areas, resulting in blurriness and needle-shaped
artifacts. To address them, we propose Metamon-GS, from innovative viewpoints
of variance-guided densification strategy and multi-level hash grid. The
densification strategy guided by variance specifically targets Gaussians with
high gradient variance in pixels and compensates for the importance of regions
with extra Gaussians to improve reconstruction. The latter studies implicit
global lighting conditions and accurately interprets color from different
perspectives and feature embeddings. Our thorough experiments on publicly
available datasets show that Metamon-GS surpasses its baseline model and
previous versions, delivering superior quality in rendering novel views.

</details>


### [124] [LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation](https://arxiv.org/abs/2504.14467)
*Jiachen Li,Qing Xie,Xiaohan Yu,Hongyun Wang,Jinyu Xu,Yongjian Liu,Yongsheng Gao*

Main category: cs.CV

TL;DR: LGD框架利用多模态大语言模型生成描述，提升视觉语言模型在零样本参考图像分割中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决自由形式参考表达在视觉和文本模态对齐中的模糊性和多样性问题。

Method: 设计属性提示和周围提示，生成相关描述，并引入三种视觉-文本匹配分数。

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上取得新SOTA，oIoU和mIoU分别提升9.97%和11.29%。

Conclusion: LGD通过生成描述显著提升了区域-文本匹配性能。

Abstract: Zero-shot referring image segmentation aims to locate and segment the target
region based on a referring expression, with the primary challenge of aligning
and matching semantics across visual and textual modalities without training.
Previous works address this challenge by utilizing Vision-Language Models and
mask proposal networks for region-text matching. However, this paradigm may
lead to incorrect target localization due to the inherent ambiguity and
diversity of free-form referring expressions. To alleviate this issue, we
present LGD (Leveraging Generative Descriptions), a framework that utilizes the
advanced language generation capabilities of Multi-Modal Large Language Models
to enhance region-text matching performance in Vision-Language Models.
Specifically, we first design two kinds of prompts, the attribute prompt and
the surrounding prompt, to guide the Multi-Modal Large Language Models in
generating descriptions related to the crucial attributes of the referent
object and the details of surrounding objects, referred to as attribute
description and surrounding description, respectively. Secondly, three
visual-text matching scores are introduced to evaluate the similarity between
instance-level visual features and textual features, which determines the mask
most associated with the referring expression. The proposed method achieves new
state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and
RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU
compared to previous methods.

</details>


### [125] [Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis](https://arxiv.org/abs/2504.14470)
*Jingjing Ren,Wenbo Li,Zhongdao Wang,Haoze Sun,Bangzhen Liu,Haoyu Chen,Jiaqi Xu,Aoxue Li,Shifeng Zhang,Bin Shao,Yong Guo,Lei Zhu*

Main category: cs.CV

TL;DR: Turbo2K是一个高效框架，用于生成2K分辨率视频，通过压缩潜在空间和知识蒸馏技术显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着消费者对超清晰视觉的需求增加，2K视频合成的需求上升，但现有方法在计算成本上难以扩展。

Method: Turbo2K采用压缩潜在空间和知识蒸馏策略，并通过分层两阶段合成框架优化生成过程。

Result: Turbo2K在效率上达到最新水平，生成5秒24fps的2K视频，推理速度比现有方法快20倍。

Conclusion: Turbo2K为高分辨率视频生成提供了更高效和实用的解决方案。

Abstract: Demand for 2K video synthesis is rising with increasing consumer expectations
for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated
remarkable capabilities in high-quality video generation, scaling them to 2K
resolution remains computationally prohibitive due to quadratic growth in
memory and processing costs. In this work, we propose Turbo2K, an efficient and
practical framework for generating detail-rich 2K videos while significantly
improving training and inference efficiency. First, Turbo2K operates in a
highly compressed latent space, reducing computational complexity and memory
footprint, making high-resolution video synthesis feasible. However, the high
compression ratio of the VAE and limited model size impose constraints on
generative quality. To mitigate this, we introduce a knowledge distillation
strategy that enables a smaller student model to inherit the generative
capacity of a larger, more powerful teacher model. Our analysis reveals that,
despite differences in latent spaces and architectures, DiTs exhibit structural
similarities in their internal representations, facilitating effective
knowledge transfer. Second, we design a hierarchical two-stage synthesis
framework that first generates multi-level feature at lower resolutions before
guiding high-resolution video generation. This approach ensures structural
coherence and fine-grained detail refinement while eliminating redundant
encoding-decoding overhead, further enhancing computational efficiency.Turbo2K
achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos
with significantly reduced computational cost. Compared to existing methods,
Turbo2K is up to 20$\times$ faster for inference, making high-resolution video
generation more scalable and practical for real-world applications.

</details>


### [126] [Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space](https://arxiv.org/abs/2504.14471)
*Yichi Zhang,Qianqian Yang*

Main category: cs.CV

TL;DR: PICO是一个基于隐式神经表示（INR）的点云压缩框架，通过几何和属性压缩两阶段分解任务，并引入LeAFNet网络提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统点云压缩方法效率不足，PICO旨在通过INR和新型网络架构提升压缩性能。

Method: 采用两阶段压缩（几何和属性），引入LeAFNet网络（基于KANs的可学习激活函数），结合量化和熵编码。

Result: LeAFNet优于传统MLPs，PICO在几何压缩上比MPEG标准提升4.92 dB D1 PSNR，联合压缩PCQM增益2.7×10⁻³。

Conclusion: PICO通过INR和LeAFNet显著提升点云压缩效率，性能优于现有标准。

Abstract: Implicit Neural Representations (INRs), also known as neural fields, have
emerged as a powerful paradigm in deep learning, parameterizing continuous
spatial fields using coordinate-based neural networks. In this paper, we
propose \textbf{PICO}, an INR-based framework for static point cloud
compression. Unlike prevailing encoder-decoder paradigms, we decompose the
point cloud compression task into two separate stages: geometry compression and
attribute compression, each with distinct INR optimization objectives. Inspired
by Kolmogorov-Arnold Networks (KANs), we introduce a novel network
architecture, \textbf{LeAFNet}, which leverages learnable activation functions
in the latent space to better approximate the target signal's implicit
function. By reformulating point cloud compression as neural parameter
compression, we further improve compression efficiency through quantization and
entropy coding. Experimental results demonstrate that \textbf{LeAFNet}
outperforms conventional MLPs in INR-based point cloud compression.
Furthermore, \textbf{PICO} achieves superior geometry compression performance
compared to the current MPEG point cloud compression standard, yielding an
average improvement of $4.92$ dB in D1 PSNR. In joint geometry and attribute
compression, our approach exhibits highly competitive results, with an average
PCQM gain of $2.7 \times 10^{-3}$.

</details>


### [127] [Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation](https://arxiv.org/abs/2504.14481)
*Guoyi Zhang,Siyang Chen,Guangsheng Xu,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: 提出LSR-ST框架，通过形状偏差先验增强视觉基础模型的鲁棒性，适用于复杂场景。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在复杂场景（如伪装和红外图像）中表现不佳，主要由于其纹理偏差限制了泛化能力。

Method: LSR-ST框架通过HDConv Block捕获形状感知特征，结合大核注意力和残差学习，满足形状偏差的三个关键条件。

Result: 在17个数据集和6个任务上表现优异，仅需4.719M可训练参数。

Conclusion: LSR-ST展示了表示效率在复杂视觉环境中的潜力，为视觉任务提供了更鲁棒和通用的解决方案。

Abstract: Foreground segmentation is crucial for scene understanding, yet
parameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often
fails in complex scenarios, such as camouflage and infrared imagery. We
attribute this challenge to the inherent texture bias in VFMs, which is
exacerbated during fine-tuning and limits generalization in texture-sparse
environments. To address this, we propose Ladder Shape-bias Representation
Side-tuning (LSR-ST), a lightweight PEFT framework that enhances model
robustness by introducing shape-biased inductive priors. LSR-ST captures
shape-aware features using a simple HDConv Block, which integrates large-kernel
attention and residual learning. The method satisfies three key conditions for
inducing shape bias: large receptive fields, multi-order feature interactions,
and sparse connectivity. Our analysis reveals that these improvements stem from
representation efficiency-the ability to extract task-relevant, structurally
grounded features while minimizing redundancy. We formalize this concept via
Information Bottleneck theory and advocate for it as a key PEFT objective.
Unlike traditional NLP paradigms that focus on optimizing parameters and
memory, visual tasks require models that extract task-defined semantics, rather
than just relying on pre-encoded features. This shift enables our approach to
move beyond conventional trade-offs, offering more robust and generalizable
solutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves
consistent improvements across 17 datasets and 6 tasks using only 4.719M
trainable parameters. These results highlight the potential of representation
efficiency for robust and adaptable VFMs within complex visual environments.

</details>


### [128] [STARS: Sparse Learning Correlation Filter with Spatio-temporal Regularization and Super-resolution Reconstruction for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14491)
*Shang Zhang,Xiaobo Ding,Huanbin Zhang,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TL;DR: STARS是一种基于稀疏学习的相关滤波器跟踪器，结合时空正则化和超分辨率重建，显著提升了热红外目标跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 热红外（TIR）图像分辨率低且易受干扰，限制了跟踪器的性能。

Method: 采用自适应稀疏滤波和时域滤波提取目标特征，引入边缘保持稀疏正则化稳定特征，并提出梯度增强超分辨率方法提升图像分辨率。

Result: 在多个基准测试中，STARS表现优于现有最先进的跟踪器。

Conclusion: STARS首次将超分辨率方法融入稀疏学习框架，显著提升了TIR目标跟踪的鲁棒性。

Abstract: Thermal infrared (TIR) target tracking methods often adopt the correlation
filter (CF) framework due to its computational efficiency. However, the low
resolution of TIR images, along with tracking interference, significantly
limits the perfor-mance of TIR trackers. To address these challenges, we
introduce STARS, a novel sparse learning-based CF tracker that incorporates
spatio-temporal regulari-zation and super-resolution reconstruction. First, we
apply adaptive sparse filter-ing and temporal domain filtering to extract key
features of the target while reduc-ing interference from background clutter and
noise. Next, we introduce an edge-preserving sparse regularization method to
stabilize target features and prevent excessive blurring. This regularization
integrates multiple terms and employs the alternating direction method of
multipliers to optimize the solution. Finally, we propose a gradient-enhanced
super-resolution method to extract fine-grained TIR target features and improve
the resolution of TIR images, addressing performance degradation in tracking
caused by low-resolution sequences. To the best of our knowledge, STARS is the
first to integrate super-resolution methods within a sparse learning-based CF
framework. Extensive experiments on the LSOTB-TIR, PTB-TIR, VOT-TIR2015, and
VOT-TIR2017 benchmarks demonstrate that STARS outperforms state-of-the-art
trackers in terms of robustness.

</details>


### [129] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye,Miao Hua,Pengze Zhang,Xinghui Li,Qichao Sun,Songtao Zhao,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: DreamID是一种基于扩散模型的人脸交换方法，通过显式监督和Triplet ID Group数据提升身份相似性和属性保留，结合SD Turbo加速模型实现高效训练，在512*512分辨率下仅需0.6秒完成高质量交换。


<details>
  <summary>Details</summary>
Motivation: 传统人脸交换方法依赖隐式监督，效果不佳。DreamID旨在通过显式监督和高效架构解决这一问题。

Method: 使用Triplet ID Group数据建立显式监督，结合SD Turbo加速模型，提出SwapNet、FaceNet和ID Adapter的改进架构。

Result: 在身份相似性、姿态表情保留和图像保真度上优于现有方法，0.6秒内完成高质量交换。

Conclusion: DreamID通过显式监督和高效架构实现了快速、高质量的人脸交换，适用于复杂场景。

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>


### [130] [Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction](https://arxiv.org/abs/2504.14516)
*Weirong Chen,Ganlin Zhang,Felix Wimbauer,Rui Wang,Nikita Araslanov,Andrea Vedaldi,Daniel Cremers*

Main category: cs.CV

TL;DR: BA-Track框架通过3D点追踪器分离相机运动与动态物体运动，结合传统SLAM的束调整与轻量级后处理，显著提升了动态场景下的相机位姿估计和3D重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统依赖静态环境假设，难以处理动态场景。现有方法要么过滤动态元素，要么独立建模其运动，导致重建不完整或运动估计不一致。

Method: 利用3D点追踪器分解相机运动与动态物体运动，结合束调整和基于尺度图的轻量级后处理，确保深度一致性。

Result: 在挑战性数据集上，BA-Track显著提升了相机位姿估计和3D重建的准确性。

Conclusion: BA-Track通过统一框架有效处理动态场景，实现了静态与动态元素的兼容性，为SLAM系统提供了新思路。

Abstract: Traditional SLAM systems, which rely on bundle adjustment, struggle with
highly dynamic scenes commonly found in casual videos. Such videos entangle the
motion of dynamic elements, undermining the assumption of static environments
required by traditional systems. Existing techniques either filter out dynamic
elements or model their motion independently. However, the former often results
in incomplete reconstructions, whereas the latter can lead to inconsistent
motion estimates. Taking a novel approach, this work leverages a 3D point
tracker to separate the camera-induced motion from the observed motion of
dynamic objects. By considering only the camera-induced component, bundle
adjustment can operate reliably on all scene elements as a result. We further
ensure depth consistency across video frames with lightweight post-processing
based on scale maps. Our framework combines the core of traditional SLAM --
bundle adjustment -- with a robust learning-based 3D tracker front-end.
Integrating motion decomposition, bundle adjustment and depth refinement, our
unified framework, BA-Track, accurately tracks the camera motion and produces
temporally coherent and scale-consistent dense reconstructions, accommodating
both static and dynamic elements. Our experiments on challenging datasets
reveal significant improvements in camera pose estimation and 3D reconstruction
accuracy.

</details>


### [131] [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
*Tong Zeng,Longfeng Wu,Liang Shi,Dawei Zhou,Feng Guo*

Main category: cs.CV

TL;DR: DVBench是一个新的基准测试，用于评估视觉大语言模型（VLLMs）在安全关键驾驶场景中的表现，揭示了现有模型的局限性，并通过领域特定微调展示了改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有VLLMs在通用视觉任务中表现优异，但在安全关键领域（如自动驾驶）的效果尚未充分探索，需要专门评估其复杂场景理解能力。

Method: 开发了DVBench，包含10,000个人工标注的多选题，基于分层能力分类法，评估14种VLLMs在驾驶视频中的感知与推理能力，并进行微调实验。

Result: 现有VLLMs在DVBench上表现不佳（最高准确率<40%），但微调后准确率提升5.24-10.94个百分点，相对改进最高达43.59%。

Conclusion: DVBench为VLLMs在自动驾驶领域的应用提供了评估框架，强调领域适应的重要性，以提升安全性和鲁棒性。

Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive
capabilities in general visual tasks such as image captioning and visual
question answering. However, their effectiveness in specialized,
safety-critical domains like autonomous driving remains largely unexplored.
Autonomous driving systems require sophisticated scene understanding in complex
environments, yet existing multimodal benchmarks primarily focus on normal
driving conditions, failing to adequately assess VLLMs' performance in
safety-critical scenarios. To address this, we introduce DVBench, a pioneering
benchmark designed to evaluate the performance of VLLMs in understanding
safety-critical driving videos. Built around a hierarchical ability taxonomy
that aligns with widely adopted frameworks for describing driving scenarios
used in assessing highly automated driving systems, DVBench features 10,000
multiple-choice questions with human-annotated ground-truth answers, enabling a
comprehensive evaluation of VLLMs' capabilities in perception and reasoning.
Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal
significant performance gaps, with no model achieving over 40% accuracy,
highlighting critical limitations in understanding complex driving scenarios.
To probe adaptability, we fine-tuned selected models using domain-specific data
from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage
points, with relative improvements of up to 43.59%. This improvement
underscores the necessity of targeted adaptation to bridge the gap between
general-purpose VLLMs and mission-critical driving applications. DVBench
establishes an essential evaluation framework and research roadmap for
developing VLLMs that meet the safety and robustness requirements for
real-world autonomous systems. We released the benchmark toolbox and the
fine-tuned model at: https://github.com/tong-zeng/DVBench.git.

</details>


### [132] [SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization](https://arxiv.org/abs/2504.14534)
*Liang Peng,Boxi Wu,Haoran Cheng,Yibo Zhao,Xiaofei He*

Main category: cs.CV

TL;DR: SUDO是一种自监督直接偏好优化方法，用于优化文本到图像扩散模型，同时关注像素级细节和全局图像质量，无需昂贵的数据标注。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法主要优化像素级MSE损失，忽略了全局图像质量的优化，导致感知质量和结构连贯性不足。

Method: SUDO通过自监督生成偏好图像对，结合直接偏好优化，平衡像素级和全局级学习。

Result: 实验表明，SUDO显著提升了Stable Diffusion等模型的全局和局部图像质量。

Conclusion: SUDO是一种高效且无需标注的替代方案，适用于任何文本到图像扩散模型。

Abstract: Previous text-to-image diffusion models typically employ supervised
fine-tuning (SFT) to enhance pre-trained base models. However, this approach
primarily minimizes the loss of mean squared error (MSE) at the pixel level,
neglecting the need for global optimization at the image level, which is
crucial for achieving high perceptual quality and structural coherence. In this
paper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a
novel paradigm that optimizes both fine-grained details at the pixel level and
global image quality. By integrating direct preference optimization into the
model, SUDO generates preference image pairs in a self-supervised manner,
enabling the model to prioritize global-level learning while complementing the
pixel-level MSE loss. As an effective alternative to supervised fine-tuning,
SUDO can be seamlessly applied to any text-to-image diffusion model.
Importantly, it eliminates the need for costly data collection and annotation
efforts typically associated with traditional direct preference optimization
methods. Through extensive experiments on widely-used models, including Stable
Diffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both
global and local image quality. The codes are provided at
\href{https://github.com/SPengLiang/SUDO}{this link}.

</details>


### [133] [FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models](https://arxiv.org/abs/2504.14535)
*Kuanting Wu,Kei Ota,Asako Kanezaki*

Main category: cs.CV

TL;DR: FlowLoss通过直接比较生成视频和真实视频的光流场，结合噪声感知权重方案，提升了视频扩散模型的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决视频扩散模型在生成时间一致运动时的困难。

Method: 提出FlowLoss，直接比较光流场，并引入噪声感知权重方案。

Result: 实验表明FlowLoss提高了运动稳定性并加速早期训练收敛。

Conclusion: 为噪声条件生成模型引入运动监督提供了实用见解。

Abstract: Video Diffusion Models (VDMs) can generate high-quality videos, but often
struggle with producing temporally coherent motion. Optical flow supervision is
a promising approach to address this, with prior works commonly employing
warping-based strategies that avoid explicit flow matching. In this work, we
explore an alternative formulation, FlowLoss, which directly compares flow
fields extracted from generated and ground-truth videos. To account for the
unreliability of flow estimation under high-noise conditions in diffusion, we
propose a noise-aware weighting scheme that modulates the flow loss across
denoising steps. Experiments on robotic video datasets suggest that FlowLoss
improves motion stability and accelerates convergence in early training stages.
Our findings offer practical insights for incorporating motion-based
supervision into noise-conditioned generative models.

</details>


### [134] [VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control](https://arxiv.org/abs/2504.14548)
*Lifeng Lin,Rongfeng Lu,Quan Chen,Haofan Ren,Ming Lu,Yaoqi Sun,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: VGNC提出了一种基于生成新视角合成模型的验证引导高斯数量控制方法，用于减少稀疏视角3D高斯泼溅（3DGS）的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角3D重建在实际应用中存在过拟合问题，现有基于3DGS的方法虽有所进展但仍未解决。

Method: VGNC通过生成验证图像并基于此控制高斯数量，以减少过拟合。

Result: 实验表明VGNC不仅减少了过拟合，还提高了渲染质量并降低了高斯点数量，从而节省存储和加速训练与渲染。

Conclusion: VGNC是首个利用生成验证图像解决稀疏视角3DGS过拟合问题的方法，效果显著。

Abstract: Sparse-view 3D reconstruction is a fundamental yet challenging task in
practical 3D reconstruction applications. Recently, many methods based on the
3D Gaussian Splatting (3DGS) framework have been proposed to address
sparse-view 3D reconstruction. Although these methods have made considerable
advancements, they still show significant issues with overfitting. To reduce
the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number
Control (VGNC) approach based on generative novel view synthesis (NVS) models.
To the best of our knowledge, this is the first attempt to alleviate the
overfitting issue of sparse-view 3DGS with generative validation images.
Specifically, we first introduce a validation image generation method based on
a generative NVS model. We then propose a Gaussian number control strategy that
utilizes generated validation images to determine the optimal Gaussian numbers,
thereby reducing the issue of overfitting. We conducted detailed experiments on
various sparse-view 3DGS baselines and datasets to evaluate the effectiveness
of VGNC. Extensive experiments show that our approach not only reduces
overfitting but also improves rendering quality on the test set while
decreasing the number of Gaussian points. This reduction lowers storage demands
and accelerates both training and rendering. The code will be released.

</details>


### [135] [Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection](https://arxiv.org/abs/2504.14553)
*Weijun Zhuang,Qizhang Li,Xin Li,Ming Liu,Xiaopeng Hong,Feng Gao,Fan Yang,Wangmeng Zuo*

Main category: cs.CV

TL;DR: Grounding-MD是一个创新的视频-语言预训练框架，用于开放世界时刻检测，通过结构化提示机制支持灵活的自然语言查询，并在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于封闭场景，无法适应开放世界需求，因此需要一种能够处理多样化查询的通用框架。

Method: 采用Cross-Modality Fusion Encoder和Text-Guided Fusion Decoder，结合大规模预训练，实现视频-文本对齐和跨任务协作。

Result: 在ActivityNet、THUMOS14等数据集上，Grounding-MD在零样本和监督设置下均达到最优性能。

Conclusion: Grounding-MD为开放世界时刻检测提供了高效解决方案，并展示了强大的语义表示学习能力。

Abstract: Temporal Action Detection and Moment Retrieval constitute two pivotal tasks
in video understanding, focusing on precisely localizing temporal segments
corresponding to specific actions or events. Recent advancements introduced
Moment Detection to unify these two tasks, yet existing approaches remain
confined to closed-set scenarios, limiting their applicability in open-world
contexts. To bridge this gap, we present Grounding-MD, an innovative, grounded
video-language pre-training framework tailored for open-world moment detection.
Our framework incorporates an arbitrary number of open-ended natural language
queries through a structured prompt mechanism, enabling flexible and scalable
moment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a
Text-Guided Fusion Decoder to facilitate comprehensive video-text alignment and
enable effective cross-task collaboration. Through large-scale pre-training on
temporal action detection and moment retrieval datasets, Grounding-MD
demonstrates exceptional semantic representation learning capabilities,
effectively handling diverse and complex query conditions. Comprehensive
evaluations across four benchmark datasets including ActivityNet, THUMOS14,
ActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD
establishes new state-of-the-art performance in zero-shot and supervised
settings in open-world moment detection scenarios. All source code and trained
models will be released.

</details>


### [136] [SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14566)
*Shang Zhang,HuiPan Guan,XiaoBo Ding,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为SMTT的新型热红外目标跟踪器，通过多任务学习、联合稀疏表示和自适应图正则化，有效解决了噪声、遮挡和目标快速运动等常见问题。


<details>
  <summary>Details</summary>
Motivation: 热红外目标跟踪在监控、自动驾驶和军事行动中至关重要，但面临噪声、遮挡和快速目标运动等挑战。

Method: 将跟踪任务重新定义为多任务学习问题，采用加权混合范数正则化策略独立优化每个粒子的表示，并利用加速近端梯度方法实现实时性能。

Result: 在VOT-TIR、PTB-TIR和LSOTB-TIR等基准数据集上的实验表明，SMTT在准确性、鲁棒性和计算效率方面表现优异。

Conclusion: SMTT是复杂环境中热红外目标跟踪的可靠高性能解决方案。

Abstract: Thermal infrared target tracking is crucial in applications such as
surveillance, autonomous driving, and military operations. In this paper, we
propose a novel tracker, SMTT, which effectively addresses common challenges in
thermal infrared imagery, such as noise, occlusion, and rapid target motion, by
leveraging multi-task learning, joint sparse representation, and adaptive graph
regularization. By reformulating the tracking task as a multi-task learning
problem, the SMTT tracker independently optimizes the representation of each
particle while dynamically capturing spatial and feature-level similarities
using a weighted mixed-norm regularization strategy. To ensure real-time
performance, we incorporate the Accelerated Proximal Gradient method for
efficient optimization. Extensive experiments on benchmark datasets - including
VOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior
accuracy, robustness, and computational efficiency. These results highlight
SMTT as a reliable and high-performance solution for thermal infrared target
tracking in complex environments.

</details>


### [137] [NTIRE 2025 Challenge on Image Super-Resolution ($\times$4): Methods and Results](https://arxiv.org/abs/2504.14582)
*Zheng Chen,Kai Liu,Jue Gong,Jingkai Wang,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Xiangyu Kong,Xiaoxuan Yu,Hyunhee Park,Suejin Han,Hakjae Jeon,Dafeng Zhang,Hyung-Ju Chun,Donghun Ryou,Inju Ha,Bohyung Han,Lu Zhao,Yuyi Zhang,Pengyu Yan,Jiawei Hu,Pengwei Liu,Fengjun Guo,Hongyuan Yu,Pufan Xu,Zhijuan Huang,Shuyuan Cui,Peng Guo,Jiahui Liu,Dongkai Zhang,Heng Zhang,Huiyuan Fu,Huadong Ma,Yanhui Guo,Sisi Tian,Xin Liu,Jinwen Liang,Jie Liu,Jie Tang,Gangshan Wu,Zeyu Xiao,Zhuoyuan Li,Yinxiang Zhang,Wenxuan Cai,Vijayalaxmi Ashok Aralikatti,Nikhil Akalwadi,G Gyaneshwar Rao,Chaitra Desai,Ramesh Ashok Tabib,Uma Mudenagudi,Marcos V. Conde,Alejandro Merino,Bruno Longarela,Javier Abad,Weijun Yuan,Zhan Li,Zhanglu Chen,Boyang Yao,Aagam Jain,Milan Kumar Singh,Ankit Kumar,Shubh Kawa,Divyavardhan Singh,Anjali Sarvaiya,Kishor Upla,Raghavendra Ramachandra,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu,Risheek V Hiremath,Yashaswini Palani,Yuxuan Jiang,Qiang Zhu,Siyue Teng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull,Jingwei Liao,Yuqing Yang,Wenda Shao,Junyi Zhao,Qisheng Xu,Kele Xu,Sunder Ali Khowaja,Ik Hyun Lee,Snehal Singh Tomar,Rajarshi Ray,Klaus Mueller,Sachin Chaudhary,Surya Vashisth,Akshay Dudhane,Praful Hambarde,Satya Naryan Tazi,Prashant Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Bilel Benjdira,Anas M. Ali,Wadii Boulila,Zahra Moammeri,Ahmad Mahmoudi-Aznaveh,Ali Karbasi,Hossein Motamednia,Liangyan Li,Guanhua Zhao,Kevin Le,Yimo Ning,Haoxuan Huang,Jun Chen*

Main category: cs.CV

TL;DR: NTIRE 2025图像超分辨率挑战赛旨在通过双三次下采样恢复高分辨率图像，分为恢复和感知两个子赛道，共有286名参与者。


<details>
  <summary>Details</summary>
Motivation: 推动图像超分辨率技术的发展，提供基准测试以促进研究进步。

Method: 挑战赛包括两个子赛道：恢复赛道（基于PSNR）和感知赛道（基于感知分数），参与者提交网络设计或解决方案。

Result: 286名参与者注册，25支团队提交有效作品，报告总结了挑战设计、数据集和结果。

Conclusion: 挑战赛作为基准推动了图像超分辨率技术的进步。

Abstract: This paper presents the NTIRE 2025 image super-resolution ($\times$4)
challenge, one of the associated competitions of the 10th NTIRE Workshop at
CVPR 2025. The challenge aims to recover high-resolution (HR) images from
low-resolution (LR) counterparts generated through bicubic downsampling with a
$\times$4 scaling factor. The objective is to develop effective network designs
or solutions that achieve state-of-the-art SR performance. To reflect the dual
objectives of image SR research, the challenge includes two sub-tracks: (1) a
restoration track, emphasizes pixel-wise accuracy and ranks submissions based
on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by
a perceptual score. A total of 286 participants registered for the competition,
with 25 teams submitting valid entries. This report summarizes the challenge
design, datasets, evaluation protocol, the main results, and methods of each
team. The challenge serves as a benchmark to advance the state of the art and
foster progress in image SR.

</details>


### [138] [Using street view imagery and deep generative modeling for estimating the health of urban forests](https://arxiv.org/abs/2504.14583)
*Akshit Gupta,Remko Uijlenhoet*

Main category: cs.CV

TL;DR: 提出了一种基于街景图像、树木清单和气象条件的城市森林健康监测方法，利用图像到图像转换网络估算NDVI和CTD参数，并与地面实测数据对比验证。


<details>
  <summary>Details</summary>
Motivation: 传统城市森林健康监测方法依赖人工且难以扩展，现有遥感技术也存在部署或分辨率限制，需要一种更高效、可扩展的解决方案。

Method: 使用街景图像、树木清单和气象数据，通过图像到图像转换网络估算NDVI和CTD参数，并与手持多光谱和热成像传感器的实测数据对比。

Result: 该方法有望利用现有街景平台（如Google Street View）实现大规模城市森林健康管理。

Conclusion: 提出的方法为城市森林健康监测提供了一种低成本、可扩展的替代方案。

Abstract: Healthy urban forests comprising of diverse trees and shrubs play a crucial
role in mitigating climate change. They provide several key advantages such as
providing shade for energy conservation, and intercepting rainfall to reduce
flood runoff and soil erosion. Traditional approaches for monitoring the health
of urban forests require instrumented inspection techniques, often involving a
high amount of human labor and subjective evaluations. As a result, they are
not scalable for cities which lack extensive resources. Recent approaches
involving multi-spectral imaging data based on terrestrial sensing and
satellites, are constrained respectively with challenges related to dedicated
deployments and limited spatial resolutions. In this work, we propose an
alternative approach for monitoring the urban forests using simplified inputs:
street view imagery, tree inventory data and meteorological conditions. We
propose to use image-to-image translation networks to estimate two urban forest
health parameters, namely, NDVI and CTD. Finally, we aim to compare the
generated results with ground truth data using an onsite campaign utilizing
handheld multi-spectral and thermal imaging sensors. With the advent and
expansion of street view imagery platforms such as Google Street View and
Mapillary, this approach should enable effective management of urban forests
for the authorities in cities at scale.

</details>


### [139] [NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results](https://arxiv.org/abs/2504.14600)
*Zheng Chen,Jingkai Wang,Kai Liu,Jue Gong,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Jianxing Zhang,Jinlong Wu,Jun Wang,Zheng Xie,Hakjae Jeon,Suejin Han,Hyung-Ju Chun,Hyunhee Park,Zhicun Yin,Junjie Chen,Ming Liu,Xiaoming Li,Chao Zhou,Wangmeng Zuo,Weixia Zhang,Dingquan Li,Kede Ma,Yun Zhang,Zhuofan Zheng,Yuyue Liu,Shizhen Tang,Zihao Zhang,Yi Ning,Hao Jiang,Wenjie An,Kangmeng Yu,Chenyang Wang,Kui Jiang,Xianming Liu,Junjun Jiang,Yingfu Zhang,Gang He,Siqi Wang,Kepeng Xu,Zhenyang Liu,Changxin Zhou,Shanlan Shen,Yubo Duan,Yiang Chen,Jin Guo,Mengru Yang,Jen-Wei Lee,Chia-Ming Lee,Chih-Chung Hsu,Hu Peng,Chunming He*

Main category: cs.CV

TL;DR: NTIRE 2025挑战赛综述，聚焦真实人脸恢复，强调自然输出与身份一致性，推动了感知质量与真实性的前沿解决方案。


<details>
  <summary>Details</summary>
Motivation: 提升真实人脸恢复的感知质量与真实性，同时不限制计算资源或训练数据。

Method: 使用加权图像质量评估（IQA）分数和AdaFace模型进行身份验证。

Result: 141名注册者中，13支团队提交有效模型，10支团队在最终排名中获有效分数。

Conclusion: 该挑战赛推动了真实人脸恢复的性能，并提供了该领域最新趋势的深入概述。

Abstract: This paper provides a review of the NTIRE 2025 challenge on real-world face
restoration, highlighting the proposed solutions and the resulting outcomes.
The challenge focuses on generating natural, realistic outputs while
maintaining identity consistency. Its goal is to advance state-of-the-art
solutions for perceptual quality and realism, without imposing constraints on
computational resources or training data. The track of the challenge evaluates
performance using a weighted image quality assessment (IQA) score and employs
the AdaFace model as an identity checker. The competition attracted 141
registrants, with 13 teams submitting valid models, and ultimately, 10 teams
achieved a valid score in the final ranking. This collaborative effort advances
the performance of real-world face restoration while offering an in-depth
overview of the latest trends in the field.

</details>


### [140] [MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation](https://arxiv.org/abs/2504.14606)
*Siyi Jiao,Wenzheng Zeng,Yerong Li,Huayu Zhang,Changxin Gao,Nong Sang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: MP-Mat是一种新型的3D感知和实例感知的抠图框架，通过多平面表示解决复杂场景中多实例抠图的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决复杂场景中多实例抠图的困难，尤其是在边界模糊和遮挡区域的表现。

Method: 构建特征级多平面表示（基于深度差异）和实例级多平面表示（包括背景），提升3D感知和边界处理能力。

Result: MP-Mat在抠图任务中表现优越，同时在零样本推理的图像编辑任务中超越专业方法。

Conclusion: MP-Mat通过多平面表示显著提升了复杂场景的抠图效果，并展示了在图像编辑中的潜力。

Abstract: Human instance matting aims to estimate an alpha matte for each human
instance in an image, which is challenging as it easily fails in complex cases
requiring disentangling mingled pixels belonging to multiple instances along
hairy and thin boundary structures. In this work, we address this by
introducing MP-Mat, a novel 3D-and-instance-aware matting framework with
multiplane representation, where the multiplane concept is designed from two
different perspectives: scene geometry level and instance level. Specifically,
we first build feature-level multiplane representations to split the scene into
multiple planes based on depth differences. This approach makes the scene
representation 3D-aware, and can serve as an effective clue for splitting
instances in different 3D positions, thereby improving interpretability and
boundary handling ability especially in occlusion areas. Then, we introduce
another multiplane representation that splits the scene in an instance-level
perspective, and represents each instance with both matte and color. We also
treat background as a special instance, which is often overlooked by existing
methods. Such an instance-level representation facilitates both foreground and
background content awareness, and is useful for other down-stream tasks like
image editing. Once built, the representation can be reused to realize
controllable instance-level image editing with high efficiency. Extensive
experiments validate the clear advantage of MP-Mat in matting task. We also
demonstrate its superiority in image editing tasks, an area under-explored by
existing matting-focused methods, where our approach under zero-shot inference
even outperforms trained specialized image editing techniques by large margins.
Code is open-sourced at https://github.com/JiaoSiyi/MPMat.git}.

</details>


### [141] [VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image](https://arxiv.org/abs/2504.14618)
*Han Bi,Ge Yu,Yu He,Wenzhuo Liu,Zijie Zheng*

Main category: cs.CV

TL;DR: VM-BHINet利用状态空间模型改进双手交互重建，显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡、模糊外观和计算效率方面表现不佳，需改进双手交互建模。

Method: 提出VM-BHINet，结合状态空间模型与局部全局特征操作，核心为VM-IFEBlock。

Result: 在InterHand2.6M数据集上，MPJPE和MPVPE降低2-3%，优于现有方法。

Conclusion: VM-BHINet通过SSMs有效提升双手交互重建的准确性和效率。

Abstract: Understanding bimanual hand interactions is essential for realistic 3D pose
and shape reconstruction. However, existing methods struggle with occlusions,
ambiguous appearances, and computational inefficiencies. To address these
challenges, we propose Vision Mamba Bimanual Hand Interaction Network
(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to
enhance interaction modeling while improving computational efficiency. The core
component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),
combines SSMs with local and global feature operations, enabling deep
understanding of hand interactions. Experiments on the InterHand2.6M dataset
show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean
per-vertex position error (MPVPE) by 2-3%, significantly surpassing
state-of-the-art methods.

</details>


### [142] [Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts](https://arxiv.org/abs/2504.14621)
*Zhenkui Yang,Zeyi Huang,Ge Wang,Han Ding,Tony Xiao Han,Fei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种文本增强的无线传感框架WiTalk，通过三种层次化的提示策略整合语义知识，显著提升了无线信号感知技术在动作识别和定位任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无线信号感知技术虽具有非接触和环境适应性优势，但未能充分利用数据集中的文本信息。

Method: 提出WiTalk框架，采用标签、简要描述和详细动作描述三种提示策略，无需修改架构或增加数据成本。

Result: 在三个公开数据集上验证，性能显著提升，如XRF55上WiFi、RFID和mmWave的准确率分别提高3.9%、2.59%和0.46%。

Conclusion: WiTalk框架有效整合语义知识，为无线信号感知任务提供了性能提升的新途径。

Abstract: Wireless signal-based human sensing technologies, such as WiFi,
millimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),
enable the detection and interpretation of human presence, posture, and
activities, thereby providing critical support for applications in public
security, healthcare, and smart environments. These technologies exhibit
notable advantages due to their non-contact operation and environmental
adaptability; however, existing systems often fail to leverage the textual
information inherent in datasets. To address this, we propose an innovative
text-enhanced wireless sensing framework, WiTalk, that seamlessly integrates
semantic knowledge through three hierarchical prompt strategies-label-only,
brief description, and detailed action description-without requiring
architectural modifications or incurring additional data costs. We rigorously
validate this framework across three public benchmark datasets: XRF55 for human
action recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action
localization (TAL). Experimental results demonstrate significant performance
improvements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,
2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD
improves by 4.98%; and on XRFV2, the mean average precision gains across
various methods range from 4.02% to 13.68%. Our codes have been included in
https://github.com/yangzhenkui/WiTalk.

</details>


### [143] [MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification](https://arxiv.org/abs/2504.14626)
*Santanu Roy,Shweta Singh,Palak Sahu,Ashvath Suresh,Debashish Das*

Main category: cs.CV

TL;DR: 提出了一种新型CNN架构MSD-Net，通过改进的密集模块和多尺度特征提取，显著提升了肺癌检测的准确率，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期检测至关重要，但传统方法依赖人工分析，效率低且易受类别不平衡影响，因此需要自动化的深度学习解决方案。

Method: 设计了MSD-Net，包含改进的密集模块（DWSC层和1x1卷积）、跳跃连接和平行分支，利用空洞卷积提取多尺度特征。

Result: 实验表明，MSD-Net在性能上显著优于ConvNext-Tiny、ViT、PiT等最新模型。

Conclusion: MSD-Net通过结构创新有效解决了类别不平衡问题，为肺癌自动检测提供了高效工具。

Abstract: Lung cancer, a severe form of malignant tumor that originates in the tissues
of the lungs, can be fatal if not detected in its early stages. It ranks among
the top causes of cancer-related mortality worldwide. Detecting lung cancer
manually using chest X-Ray image or Computational Tomography (CT) scans image
poses significant challenges for radiologists. Hence, there is a need for
automatic diagnosis system of lung cancers from radiology images. With the
recent emergence of deep learning, particularly through Convolutional Neural
Networks (CNNs), the automated detection of lung cancer has become a much
simpler task. Nevertheless, numerous researchers have addressed that the
performance of conventional CNNs may be hindered due to class imbalance issue,
which is prevalent in medical images. In this research work, we have proposed a
novel CNN architecture ``Multi-Scale Dense Network (MSD-Net)''
(trained-from-scratch). The novelties we bring in the proposed model are (I) We
introduce novel dense modules in the 4th block and 5th block of the CNN model.
We have leveraged 3 depthwise separable convolutional (DWSC) layers, and one
1x1 convolutional layer in each dense module, in order to reduce complexity of
the model considerably. (II) Additionally, we have incorporated one skip
connection from 3rd block to 5th block and one parallel branch connection from
4th block to Global Average Pooling (GAP) layer. We have utilized dilated
convolutional layer (with dilation rate=2) in the last parallel branch in order
to extract multi-scale features. Extensive experiments reveal that our proposed
model has outperformed latest CNN model ConvNext-Tiny, recent trend Vision
Transformer (ViT), Pooling-based ViT (PiT), and other existing models by
significant margins.

</details>


### [144] [NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation](https://arxiv.org/abs/2504.14638)
*Junyuan Fang,Zihan Wang,Yejun Zhang,Shuzhe Wang,Iaroslav Melekhov,Juho Kannala*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅的硬视觉提示方法，通过相机插值生成多视角图像，无需2D-3D优化或微调，提升3D实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在3D实例级分割任务中表现不足，缺乏精确的定位和识别能力。

Method: 利用3D高斯泼溅和相机插值生成多视角图像，增强几何一致性，无需训练。

Result: 方法有效提升了视觉语言模型在3D实例分割中的鲁棒性和准确性。

Conclusion: 该训练免费策略能无缝集成现有硬视觉提示，适用于多样3D场景。

Abstract: Vision-language models (VLMs) have demonstrated impressive zero-shot transfer
capabilities in image-level visual perception tasks. However, they fall short
in 3D instance-level segmentation tasks that require accurate localization and
recognition of individual objects. To bridge this gap, we introduce a novel 3D
Gaussian Splatting based hard visual prompting approach that leverages camera
interpolation to generate diverse viewpoints around target objects without any
2D-3D optimization or fine-tuning. Our method simulates realistic 3D
perspectives, effectively augmenting existing hard visual prompts by enforcing
geometric consistency across viewpoints. This training-free strategy seamlessly
integrates with prior hard visual prompts, enriching object-descriptive
features and enabling VLMs to achieve more robust and accurate 3D instance
segmentation in diverse 3D scenes.

</details>


### [145] [Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension](https://arxiv.org/abs/2504.14642)
*Lin Li,Wei Chen,Jiahui Li,Long Chen*

Main category: cs.CV

TL;DR: Relation-R1是一个统一的关系理解框架，通过结合认知链式思维（CoT）和监督微调（SFT）以及强化学习（RL）中的组相对策略优化（GRPO），显著提升了多模态大语言模型（MLLMs）在视觉关系理解上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉关系理解（如场景图生成）上表现有限，尤其是在建模N元关系时缺乏语义依赖分析，导致输出不可靠和过度依赖语言先验。

Method: Relation-R1框架首先通过SFT建立基础推理能力，生成结构化输出；然后利用GRPO通过多奖励优化细化输出，优先考虑视觉语义基础而非语言偏见。

Result: 在PSG和SWiG数据集上的实验表明，Relation-R1在二元和N元关系理解上达到了最先进的性能。

Conclusion: Relation-R1通过结合SFT和GRPO，显著提升了MLLMs在复杂视觉关系理解上的能力，减少了幻觉和语言偏见的影响。

Abstract: Recent advances in multi-modal large language models (MLLMs) have
significantly improved object-level grounding and region captioning, but remain
limited in visual relation understanding (\eg, scene graph generation),
particularly in modeling \textit{N}-ary relationships that identify multiple
semantic roles among an action event. Such a lack of \textit{semantic
dependencies} modeling among multi-entities leads to unreliable outputs,
intensifying MLLMs' hallucinations and over-reliance on language priors. To
this end, we propose Relation-R1, the first unified relational comprehension
framework that explicitly integrates cognitive chain-of-thought (CoT)-guided
Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)
within a reinforcement learning (RL) paradigm. Specifically, we first establish
foundational reasoning capabilities via SFT, enforcing structured outputs with
thinking processes. Then, GRPO is utilized to refine these outputs via
multi-reward optimization, prioritizing visual-semantic grounding over
language-induced biases, thereby improving generalization capability. Extensive
experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1
achieves state-of-the-art performance in both binary and \textit{N}-ary
relation understanding.

</details>


### [146] [EmoSEM: Segment and Explain Emotion Stimuli in Visual Art](https://arxiv.org/abs/2504.14658)
*Jing Zhang,Dan Guo,Zhangbin Li,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出EmoSEM模型，解决艺术图像中像素级情感理解和语言解释生成的挑战，结合情感提示和轻量级前缀投影器实现端到端建模。


<details>
  <summary>Details</summary>
Motivation: 艺术图像的情感理解面临主观性和抽象性的双重挑战，现有模型难以适应情感导向的分割任务和情感推理。

Method: 引入情感提示和可学习掩码标记，设计情感投影器和前缀投影器，联合视觉、掩码和情感标记生成解释。

Result: 实验验证了模型的有效性，实现了从像素特征到情感解释的端到端建模。

Conclusion: EmoSEM为艺术情感计算提供了首个可解释的细粒度分析框架。

Abstract: This paper focuses on a key challenge in visual art understanding: given an
art image, the model pinpoints pixel regions that trigger a specific human
emotion, and generates linguistic explanations for the emotional arousal.
Despite recent advances in art understanding, pixel-level emotion understanding
still faces a dual challenge: first, the subjectivity of emotion makes it
difficult for general segmentation models like SAM to adapt to emotion-oriented
segmentation tasks; and second, the abstract nature of art expression makes it
difficult for captioning models to balance pixel-level semantic understanding
and emotion reasoning. To solve the above problems, this paper proposes the
Emotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the
segmentation model SAM with emotion comprehension capability. First, to enable
the model to perform segmentation under the guidance of emotional intent well,
we introduce an emotional prompt with a learnable mask token as the conditional
input for segmentation decoding. Then, we design an emotion projector to
establish the association between emotion and visual features. Next, more
importantly, to address emotion-visual stimuli alignment, we develop a
lightweight prefix projector, a module that fuses the learned emotional mask
with the corresponding emotion into a unified representation compatible with
the language model.Finally, we input the joint visual, mask, and emotional
tokens into the language model and output the emotional explanations. It
ensures that the generated interpretations remain semantically and emotionally
coherent with the visual stimuli. The method innovatively realizes end-to-end
modeling from low-level pixel features to high-level emotion interpretation,
providing the first interpretable fine-grained analysis framework for artistic
emotion computing. Extensive experiments validate the effectiveness of our
model.

</details>


### [147] [Frequency-domain Learning with Kernel Prior for Blind Image Deblurring](https://arxiv.org/abs/2504.14664)
*Jixiang Sun,Fei Lei,Jiawei Zhang,Wenxiu Sun,Yujiu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种结合核先验和频率域去模糊的深度学习方法，通过频率集成模块（FIM）提升泛化能力，并在多任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习图像去模糊方法依赖特定数据集，泛化能力有限，需引入与图像内容无关的核先验。

Method: 采用频率域去卷积的传统方法启发，设计频率集成模块（FIM）融合核先验，结合基于频率的去模糊Transformer网络。

Result: 实验表明，该方法在多个盲图像去模糊任务中优于现有技术，泛化能力更强。

Conclusion: 引入核先验并通过FIM模块融合，显著提升了去模糊方法的泛化性和性能。

Abstract: While achieving excellent results on various datasets, many deep learning
methods for image deblurring suffer from limited generalization capabilities
with out-of-domain data. This limitation is likely caused by their dependence
on certain domain-specific datasets. To address this challenge, we argue that
it is necessary to introduce the kernel prior into deep learning methods, as
the kernel prior remains independent of the image context. For effective fusion
of kernel prior information, we adopt a rational implementation method inspired
by traditional deblurring algorithms that perform deconvolution in the
frequency domain. We propose a module called Frequency Integration Module (FIM)
for fusing the kernel prior and combine it with a frequency-based deblurring
Transfomer network. Experimental results demonstrate that our method
outperforms state-of-the-art methods on multiple blind image deblurring tasks,
showcasing robust generalization abilities. Source code will be available soon.

</details>


### [148] [DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback Representations](https://arxiv.org/abs/2504.14665)
*A S M Sharifuzzaman Sagar,Yu Chen,Jun Hoong Chan*

Main category: cs.CV

TL;DR: 本文提出了一种混合预测误差反馈机制和动态调制的深度预测编码网络，解决了传统方法在局部和全局细节处理上的不足，并设计了专用损失函数以提高性能。


<details>
  <summary>Details</summary>
Motivation: 传统预测编码网络在局部和全局细节处理上表现不佳，且缺乏动态适应性和专用损失函数。

Method: 引入混合预测误差反馈机制，结合全局和局部信息，动态调整反馈；设计专用损失函数以优化预测误差最小化。

Result: 实验表明，模型在CIFAR-10、CIFAR-100、MNIST和FashionMNIST数据集上收敛更快且预测精度更高。

Conclusion: 提出的方法显著提升了预测编码网络的性能，适用于多样化场景。

Abstract: Traditional predictive coding networks, inspired by theories of brain
function, consistently achieve promising results across various domains,
extending their influence into the field of computer vision. However, the
performance of the predictive coding networks is limited by their error
feedback mechanism, which traditionally employs either local or global
recurrent updates, leading to suboptimal performance in processing both local
and broader details simultaneously. In addition, traditional predictive coding
networks face difficulties in dynamically adjusting to the complexity and
context of varying input data, which is crucial for achieving high levels of
performance in diverse scenarios. Furthermore, there is a gap in the
development and application of specific loss functions that could more
effectively guide the model towards optimal performance. To deal with these
issues, this paper introduces a hybrid prediction error feedback mechanism with
dynamic modulation for deep predictive coding networks by effectively combining
global contexts and local details while adjusting feedback based on input
complexity. Additionally, we present a loss function tailored to this framework
to improve accuracy by focusing on precise prediction error minimization.
Experimental results demonstrate the superiority of our model over other
approaches, showcasing faster convergence and higher predictive accuracy in
CIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.

</details>


### [149] [Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens](https://arxiv.org/abs/2504.14666)
*Kaihang Pan,Wang Lin,Zhongqi Yue,Tenglong Ao,Liyu Jia,Wei Zhao,Juncheng Li,Siliang Tang,Hanwang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新的视觉语言方法，通过利用扩散时间步来学习离散、递归的视觉标记，解决了现有空间标记缺乏递归结构的问题，实现了多模态理解与生成的统一。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖空间视觉标记，但这些标记缺乏语言的递归结构，难以被大型语言模型掌握。

Method: 利用扩散时间步学习离散、递归的视觉标记，通过递归补偿噪声图像中的属性损失，实现扩散模型在任何时间步重建原始图像。

Result: 实验表明，该方法在多模态理解和生成任务上均优于其他MLLMs。

Conclusion: 该方法成功整合了LLMs的自回归推理能力和扩散模型的精确图像生成能力，实现了多模态理解与生成的无缝统一。

Abstract: Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify
visual comprehension and generation by combining LLM and diffusion models, the
state-of-the-art in each task, respectively. Existing approaches rely on
spatial visual tokens, where image patches are encoded and arranged according
to a spatial order (e.g., raster scan). However, we show that spatial tokens
lack the recursive structure inherent to languages, hence form an impossible
language for LLM to master. In this paper, we build a proper visual language by
leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our
proposed tokens recursively compensate for the progressive attribute loss in
noisy images as timesteps increase, enabling the diffusion model to reconstruct
the original image at any timestep. This approach allows us to effectively
integrate the strengths of LLMs in autoregressive reasoning and diffusion
models in precise image generation, achieving seamless multimodal comprehension
and generation within a unified framework. Extensive experiments show that we
achieve superior performance for multimodal comprehension and generation
simultaneously compared with other MLLMs. Project Page:
https://DDT-LLaMA.github.io/.

</details>


### [150] [Seurat: From Moving Points to Depth](https://arxiv.org/abs/2504.14687)
*Seokju Cho,Jiahui Huang,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TL;DR: 提出一种基于2D轨迹时空关系的新方法，通过单目视频推断相对深度，表现优于基准测试。


<details>
  <summary>Details</summary>
Motivation: 单目视频深度估计因缺乏立体视觉线索而存在模糊性，但人类通过观察物体大小和间距变化能直观感知相对深度。

Method: 利用现有点跟踪模型捕获2D轨迹，通过时空变换器处理这些轨迹，直接推断时间上的深度变化。

Result: 在TAPVid-3D基准测试中表现出色，零样本性能强，能泛化到真实数据集，预测结果时间平滑且高精度。

Conclusion: 该方法通过时空分析2D轨迹，实现了高效的单目视频深度估计，具有广泛适用性。

Abstract: Accurate depth estimation from monocular videos remains challenging due to
ambiguities inherent in single-view geometry, as crucial depth cues like
stereopsis are absent. However, humans often perceive relative depth
intuitively by observing variations in the size and spacing of objects as they
move. Inspired by this, we propose a novel method that infers relative depth by
examining the spatial relationships and temporal evolution of a set of tracked
2D trajectories. Specifically, we use off-the-shelf point tracking models to
capture 2D trajectories. Then, our approach employs spatial and temporal
transformers to process these trajectories and directly infer depth changes
over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust
zero-shot performance, generalizing effectively from synthetic to real-world
datasets. Results indicate that our approach achieves temporally smooth,
high-accuracy depth predictions across diverse domains.

</details>


### [151] [Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/abs/2504.14693)
*Enxin Song,Wenhao Chai,Weili Xu,Jianwen Xie,Yuxuan Liu,Gaoang Wang*

Main category: cs.CV

TL;DR: Video-MMLU是一个用于评估语言多模态模型（LMMs）在多学科讲座理解能力的大规模基准测试，揭示了当前模型在感知与推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LMMs在多学科讲座理解中的潜力，填补该领域的研究空白。

Method: 评估了90多个开源和专有模型（参数范围0.5B至40B），并分析了视觉标记数量和大语言模型对性能的影响。

Result: 当前模型在多学科讲座的认知挑战中表现有限，尤其是在需要感知与推理的任务中。

Conclusion: 研究揭示了多模态感知与推理在讲座理解中的相互作用，为未来模型优化提供了方向。

Abstract: Recent advancements in language multimodal models (LMMs) for video have
demonstrated their potential for understanding video content, yet the task of
comprehending multi-discipline lectures remains largely unexplored. We
introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities
of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90
open-source and proprietary models, ranging from 0.5B to 40B parameters. Our
results highlight the limitations of current models in addressing the cognitive
challenges presented by these lectures, especially in tasks requiring both
perception and reasoning. Additionally, we explore how the number of visual
tokens and the large language models influence performance, offering insights
into the interplay between multimodal perception and reasoning in lecture
comprehension.

</details>


### [152] [IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays](https://arxiv.org/abs/2504.14699)
*Sascha Jecklin,Aidana Massalimova,Ruyi Zha,Lilian Calvet,Christoph J. Laux,Mazda Farshad,Philipp Fürnstahl*

Main category: cs.CV

TL;DR: 论文提出了一种基于实例学习的3D脊柱重建方法，通过扩展$R^2$-Gaussian splatting框架，解决了稀疏X射线数据下的解剖一致性重建问题，无需预训练即可适应新患者。


<details>
  <summary>Details</summary>
Motivation: 当前监督学习方法需要大量标注数据且泛化能力有限，而实例学习方法如Gaussian splatting在稀疏X射线下的应用尚未充分探索。

Method: 扩展$R^2$-Gaussian splatting框架，引入解剖引导的放射标准化步骤，通过风格迁移提升视图一致性。

Result: 在体外数据集上验证，专家评估确认了其临床实用性，定量指标（PSNR/SSIM）显示标准化提升了重建质量。

Conclusion: 该方法证明了从稀疏X射线进行实例化体积重建的可行性，推动了术中3D成像的发展。

Abstract: Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
sparse fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to sparse, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.

</details>


### [153] [Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features](https://arxiv.org/abs/2504.14708)
*Parshuram N. Aarotale,Ajita Rattani*

Main category: cs.CV

TL;DR: 论文提出了一种基于EMG的手势识别新方法XMANet，通过跨层互注意力结合局部和语义特征，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 提升EMG手势识别的准确性和鲁棒性，适用于假肢控制、康复和人机交互。

Method: 使用XMANet模型，结合短时傅里叶变换和小波变换生成的特征，进行细粒度分类。

Result: 在多个数据集和基准模型上，XMANet性能显著提升，最高提升9.36%。

Conclusion: XMANet通过细粒度特征和跨层注意力机制，为EMG分类提供了高效且鲁棒的解决方案。

Abstract: Electromyography (EMG) based hand gesture recognition converts forearm muscle
activity into control commands for prosthetics, rehabilitation, and human
computer interaction. This paper proposes a novel approach to EMG-based hand
gesture recognition that uses fine-grained classification and presents XMANet,
which unifies low-level local and high level semantic cues through cross layer
mutual attention among shallow to deep CNN experts. Using stacked spectrograms
and scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet
Transform (WT), we benchmark XMANet against ResNet50, DenseNet-121,
MobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset
indicate that, using STFT, the proposed XMANet model outperforms the baseline
ResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement
of approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing
the WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are
observed over the same baselines. Similarly, on the FORS EMG dataset, the
XMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the
baseline ResNet50. In comparison, the XMANet(DenseNet121) and
XMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,
respectively. Moreover, when using WT, the proposed XMANet achieves gains of
around 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,
MobileNetV3, and EfficientNetB0 models, respectively. These results confirm
that XMANet consistently improves performance across various architectures and
signal processing techniques, demonstrating the strong potential of fine
grained features for accurate and robust EMG classification.

</details>


### [154] [Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline](https://arxiv.org/abs/2504.14709)
*Hui Zhou,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: 该论文提出了一种结合模仿学习（IL）和强化学习（RL）的新框架，以解决模仿学习在自动驾驶规划中的局限性，包括过拟合和泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在自动驾驶规划中表现优异，但其是否真正理解驾驶原则仍存疑，且容易过拟合常见场景，难以应对罕见或未见情况。

Method: 论文提出：1）支持模仿和强化学习的闭环模拟器；2）基于Waymo数据集的新因果基准；3）结合IL和RL的新框架。

Result: 通过新框架和基准，论文旨在更严格评估和解决模仿学习的局限性。

Conclusion: 结合IL和RL的框架有望提升自动驾驶规划的性能和泛化能力。

Abstract: Machine learning (ML)-based planners have recently gained significant
attention. They offer advantages over traditional optimization-based planning
algorithms. These advantages include fewer manually selected parameters and
faster development. Within ML-based planning, imitation learning (IL) is a
common algorithm. It primarily learns driving policies directly from supervised
trajectory data. While IL has demonstrated strong performance on many open-loop
benchmarks, it remains challenging to determine if the learned policy truly
understands fundamental driving principles, rather than simply extrapolating
from the ego-vehicle's initial state. Several studies have identified this
limitation and proposed algorithms to address it. However, these methods often
use original datasets for evaluation. In these datasets, future trajectories
are heavily dependent on initial conditions. Furthermore, IL often overfits to
the most common scenarios. It struggles to generalize to rare or unseen
situations.
  To address these challenges, this work proposes: 1) a novel closed-loop
simulator supporting both imitation and reinforcement learning, 2) a causal
benchmark derived from the Waymo Open Dataset to rigorously assess the impact
of the copycat problem, and 3) a novel framework integrating imitation learning
and reinforcement learning to overcome the limitations of purely imitative
approaches. The code for this work will be released soon.

</details>


### [155] [Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation](https://arxiv.org/abs/2504.14715)
*Md. Sanaullah Chowdhury,Salauddin Tapu,Noyon Kumar Sarkar,Ferdous Bin Ali,Lameya Sabrin*

Main category: cs.CV

TL;DR: Med-2D SegNet是一种高效且精确的医学图像分割架构，通过紧凑的Med Block设计实现高性能，同时保持低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对临床诊断和手术规划至关重要，但现有方法在复杂性和效率上存在挑战。

Method: 提出Med-2D SegNet，采用Med Block设计，结合维度扩展和参数减少，实现高效特征提取。

Result: 在多个数据集上达到89.77%的平均DSC，参数仅2.07百万，展示优异的零样本学习能力。

Conclusion: Med-2D SegNet在精度和效率上取得平衡，为医学图像分析设定了新标准，推动了先进医疗技术的普及。

Abstract: Accurate and efficient medical image segmentation is crucial for advancing
clinical diagnostics and surgical planning, yet remains a complex challenge due
to the variability in anatomical structures and the demand for low-complexity
models. In this paper, we introduced Med-2D SegNet, a novel and highly
efficient segmentation architecture that delivers outstanding accuracy while
maintaining a minimal computational footprint. Med-2D SegNet achieves
state-of-the-art performance across multiple benchmark datasets, including
KVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient
(DSC) of 89.77% across 20 diverse datasets. Central to its success is the
compact Med Block, a specialized encoder design that incorporates dimension
expansion and parameter reduction, enabling precise feature extraction while
keeping model parameters to a low count of just 2.07 million. Med-2D SegNet
excels in cross-dataset generalization, particularly in polyp segmentation,
where it was trained on KVASIR-SEG and showed strong performance on unseen
datasets, demonstrating its robustness in zero-shot learning scenarios, even
though we acknowledge that further improvements are possible. With top-tier
performance in both binary and multi-class segmentation, Med-2D SegNet
redefines the balance between accuracy and efficiency, setting a new benchmark
for medical image analysis. This work paves the way for developing accessible,
high-performance diagnostic tools suitable for clinical environments and
resource-constrained settings, making it a step forward in the democratization
of advanced medical technology.

</details>


### [156] [TAPIP3D: Tracking Any Point in Persistent 3D Geometry](https://arxiv.org/abs/2504.14717)
*Bowei Zhang,Lei Ke,Adam W. Harley,Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: TAPIP3D是一种用于单目RGB和RGB-D视频中长期3D点跟踪的新方法，通过将视频表示为相机稳定的时空特征云，并利用深度和相机运动信息提升2D特征到3D世界空间，显著提高了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点跟踪方法在长期跟踪和准确性上存在不足，TAPIP3D旨在通过3D稳定表示和局部注意力机制解决这些问题。

Method: TAPIP3D将视频表示为相机稳定的时空特征云，利用深度和相机运动信息将2D特征提升到3D空间，并通过迭代优化多帧3D运动估计和局部注意力机制实现精确跟踪。

Result: TAPIP3D在3D点跟踪基准测试中显著优于现有方法，甚至在深度准确时提高了2D跟踪性能。

Conclusion: TAPIP3D通过3D稳定表示和局部注意力机制，显著提升了长期3D点跟踪的鲁棒性和准确性。

Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in
monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized
spatio-temporal feature clouds, leveraging depth and camera motion information
to lift 2D video features into a 3D world space where camera motion is
effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion
estimates within this stabilized representation, enabling robust tracking over
extended periods. To manage the inherent irregularities of 3D point
distributions, we propose a Local Pair Attention mechanism. This 3D
contextualization strategy effectively exploits spatial relationships in 3D,
forming informative feature neighborhoods for precise 3D trajectory estimation.
Our 3D-centric approach significantly outperforms existing 3D point tracking
methods and even enhances 2D tracking accuracy compared to conventional 2D
pixel trackers when accurate depth is available. It supports inference in both
camera coordinates (i.e., unstabilized) and world coordinates, and our results
demonstrate that compensating for camera motion improves tracking performance.
Our approach replaces the conventional 2D square correlation neighborhoods used
in prior 2D and 3D trackers, leading to more robust and accurate results across
various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io

</details>


### [157] [ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant Phenotyping](https://arxiv.org/abs/2504.14736)
*Nicolás Gaggion,Rodrigo Bonazzola,María Florencia Legascue,María Florencia Mammarella,Florencia Sol Rodriguez,Federico Emanuel Aballay,Florencia Belén Catulo,Andana Barrios,Franco Accavallo,Santiago Nahuel Villarreal,Martin Crespi,Martiniano María Ricardi,Ezequiel Petrillo,Thomas Blein,Federico Ariel,Enzo Ferrante*

Main category: cs.CV

TL;DR: ChronoRoot 2.0是一个开源平台，结合低成本硬件和AI技术，用于植物根系发育的时间动态表型分析，解决了现有技术的吞吐量和结构分析限制。


<details>
  <summary>Details</summary>
Motivation: 植物发育可塑性（如根系结构）的研究对理解植物适应性和农业可持续性至关重要，但现有技术难以实现全面的时间动态分析。

Method: ChronoRoot 2.0整合了多器官追踪、实时质量控制、全面结构测量和双用户界面，支持高通量筛选和详细架构分析。

Result: 通过拟南芥的三个用例验证了系统的多功能性，包括昼夜生长模式、转基因植物的向重力反应分析和高通量筛选。

Conclusion: ChronoRoot 2.0通过开源和模块化设计，使复杂的时间动态表型分析更易普及，促进植物科学社区的发展。

Abstract: The analysis of plant developmental plasticity, including root system
architecture, is fundamental to understanding plant adaptability and
development, particularly in the context of climate change and agricultural
sustainability. While significant advances have been made in plant phenotyping
technologies, comprehensive temporal analysis of root development remains
challenging, with most existing solutions providing either limited throughput
or restricted structural analysis capabilities. Here, we present ChronoRoot
2.0, an integrated open-source platform that combines affordable hardware with
advanced artificial intelligence to enable sophisticated temporal plant
phenotyping. The system introduces several major advances, offering an integral
perspective of seedling development: (i) simultaneous multi-organ tracking of
six distinct plant structures, (ii) quality control through real-time
validation, (iii) comprehensive architectural measurements including novel
gravitropic response parameters, and (iv) dual specialized user interfaces for
both architectural analysis and high-throughput screening. We demonstrate the
system's capabilities through three use cases for Arabidopsis thaliana:
characterization of circadian growth patterns under different light conditions,
detailed analysis of gravitropic responses in transgenic plants, and
high-throughput screening of etiolation responses across multiple genotypes.
ChronoRoot 2.0 maintains its predecessor's advantages of low cost and
modularity while significantly expanding its capabilities, making sophisticated
temporal phenotyping more accessible to the broader plant science community.
The system's open-source nature, combined with extensive documentation and
containerized deployment options, ensures reproducibility and enables
community-driven development of new analytical capabilities.

</details>


### [158] [SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training](https://arxiv.org/abs/2504.14737)
*Shuang Zeng,Lei Zhu,Xinliang Zhang,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为SuperCL的新型对比学习方法，用于医学图像分割预训练，通过利用图像的结构先验和像素相关性，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临高质量标注数据稀缺的问题，现有对比学习方法忽略图像内相似像素组的特征，且对比对生成依赖人工阈值设置，效率低且泛化性差。

Method: 提出SuperCL方法，引入两种对比对生成策略（ILCP和IGCP），利用超像素图生成伪掩码指导对比学习，并设计了ASP和CCL模块以更好地利用结构信息。

Result: 在8个医学图像数据集上，SuperCL优于12种现有方法，DSC指标分别提升3.15%、5.44%、7.89%。

Conclusion: SuperCL通过创新的对比学习策略和结构信息利用，显著提升了医学图像分割的精度和效率。

Abstract: Medical image segmentation is a critical yet challenging task, primarily due
to the difficulty of obtaining extensive datasets of high-quality,
expert-annotated images. Contrastive learning presents a potential but still
problematic solution to this issue. Because most existing methods focus on
extracting instance-level or pixel-to-pixel representation, which ignores the
characteristics between intra-image similar pixel groups. Moreover, when
considering contrastive pairs generation, most SOTA methods mainly rely on
manually setting thresholds, which requires a large number of gradient
experiments and lacks efficiency and generalization. To address these issues,
we propose a novel contrastive learning approach named SuperCL for medical
image segmentation pre-training. Specifically, our SuperCL exploits the
structural prior and pixel correlation of images by introducing two novel
contrastive pairs generation strategies: Intra-image Local Contrastive Pairs
(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.
Considering superpixel cluster aligns well with the concept of contrastive
pairs generation, we utilize the superpixel map to generate pseudo masks for
both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also
propose two modules named Average SuperPixel Feature Map Generation (ASP) and
Connected Components Label Generation (CCL) to better exploit the prior
structural information for IGCP. Finally, experiments on 8 medical image
datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL
achieves a superior performance with more precise predictions from
visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best
results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released
after acceptance.

</details>


### [159] [Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches](https://arxiv.org/abs/2504.14753)
*Guodong Shen,Yuqi Ouyang,Junru Lu,Yixuan Yang,Victor Sanchez*

Main category: cs.CV

TL;DR: 论文提出了一种混合框架，结合视觉Transformer和ConvLSTM，通过双向结构优化视频异常检测的单任务框架，并验证了其在单任务和多任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 观察到现有视频异常检测方法在多任务框架中对单任务框架的优化不足，提出优化单任务框架可以提升整体性能。

Method: 利用中间帧预测作为代理任务，设计双向结构结合视觉Transformer和ConvLSTM，通过卷积时间Transformer和层交互ConvLSTM桥增强预测。

Result: 在公开基准测试中验证了混合框架的有效性，无论是单任务还是多任务分支均表现优异。

Conclusion: 结合视觉Transformer和ConvLSTM的混合框架显著提升了视频异常检测的性能。

Abstract: Despite the prevailing transition from single-task to multi-task approaches
in video anomaly detection, we observe that many adopt sub-optimal frameworks
for individual proxy tasks. Motivated by this, we contend that optimizing
single-task frameworks can advance both single- and multi-task approaches.
Accordingly, we leverage middle-frame prediction as the primary proxy task, and
introduce an effective hybrid framework designed to generate accurate
predictions for normal frames and flawed predictions for abnormal frames. This
hybrid framework is built upon a bi-directional structure that seamlessly
integrates both vision transformers and ConvLSTMs. Specifically, we utilize
this bi-directional structure to fully analyze the temporal dimension by
predicting frames in both forward and backward directions, significantly
boosting the detection stability. Given the transformer's capacity to model
long-range contextual dependencies, we develop a convolutional temporal
transformer that efficiently associates feature maps from all context frames to
generate attention-based predictions for target frames. Furthermore, we devise
a layer-interactive ConvLSTM bridge that facilitates the smooth flow of
low-level features across layers and time-steps, thereby strengthening
predictions with fine details. Anomalies are eventually identified by
scrutinizing the discrepancies between target frames and their corresponding
predictions. Several experiments conducted on public benchmarks affirm the
efficacy of our hybrid framework, whether used as a standalone single-task
approach or integrated as a branch in a multi-task approach. These experiments
also underscore the advantages of merging vision transformers and ConvLSTMs for
video anomaly detection.

</details>


### [160] [How Effective Can Dropout Be in Multiple Instance Learning ?](https://arxiv.org/abs/2504.14783)
*Wenhui Zhu,Peijie Qiu,Xiwen Chen,Zhangsihao Yang,Aristeidis Sotiras,Abolfazl Razi,Yalin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种针对多实例学习（MIL）的新型dropout方法MIL-Dropout，通过丢弃bag中最重要的实例来提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统MIL在WSI分类中因两阶段训练方案导致特征嵌入噪声和弱监督问题，而常用的dropout技术尚未在MIL中充分探索。

Method: 提出MIL-Dropout，系统性地决定丢弃bag中最重要的实例，以提升性能和泛化能力。

Result: 在五个MIL基准数据集和两个WSI数据集上验证了MIL-Dropout的有效性，显著提升了现有MIL方法的性能。

Conclusion: MIL-Dropout是一种高效且计算成本低的方法，能够显著提升MIL的性能和泛化能力。

Abstract: Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.

</details>


### [161] [When Cloud Removal Meets Diffusion Model in Remote Sensing](https://arxiv.org/abs/2504.14785)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang*

Main category: cs.CV

TL;DR: DC4CR是一种基于多模态扩散的云去除框架，通过提示驱动控制和低秩适应等技术，高效去除遥感图像中的云层，无需预生成云掩码，性能优越。


<details>
  <summary>Details</summary>
Motivation: 云遮挡严重阻碍遥感应用，传统方法依赖预生成云掩码且效率低，亟需一种高效、适应性强的云去除方法。

Method: 提出DC4CR框架，结合提示驱动控制、低秩适应、主题驱动生成和分组学习，实现高效云去除。

Result: 在RICE和CUHK-CR数据集上表现优异，达到最先进的云去除效果。

Conclusion: DC4CR为遥感图像处理提供了一种实用且高效的解决方案，具有广泛的实际应用前景。

Abstract: Cloud occlusion significantly hinders remote sensing applications by
obstructing surface information and complicating analysis. To address this, we
propose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal
diffusion-based framework for cloud removal in remote sensing imagery. Our
method introduces prompt-driven control, allowing selective removal of thin and
thick clouds without relying on pre-generated cloud masks, thereby enhancing
preprocessing efficiency and model adaptability. Additionally, we integrate
low-rank adaptation for computational efficiency, subject-driven generation for
improved generalization, and grouped learning to enhance performance on small
datasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into
existing cloud removal models, providing a scalable and robust solution.
Extensive experiments on the RICE and CUHK-CR datasets demonstrate
state-of-the-art performance, achieving superior cloud removal across diverse
conditions. This work presents a practical and efficient approach for remote
sensing image processing with broad real-world applications.

</details>


### [162] [Real-Time Sleepiness Detection for Driver State Monitoring System](https://arxiv.org/abs/2504.14807)
*Deepak Ghimire,Sunghwan Jeong,Sunhong Yoon,Sanghyun Park,Juhwan Choi*

Main category: cs.CV

TL;DR: 提出了一种实时驾驶员眼睛状态检测方法，结合动态模板匹配和Kalman滤波跟踪，使用SVM分类器判断眼睛开闭状态，并在检测到疲劳时触发警报。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是许多事故的重要因素，通过计算机视觉技术实时监测驾驶员眼睛状态可以有效预防疲劳驾驶。

Method: 1. 检测人脸并定位眼睛区域；2. 使用动态模板匹配和Kalman滤波跟踪眼睛位置；3. 基于HOG特征的SVM分类器判断眼睛开闭状态；4. 若眼睛长时间闭合则触发警报。

Result: 实现了实时驾驶员眼睛状态检测，能够准确判断疲劳状态并触发警报。

Conclusion: 该方法能有效监测驾驶员疲劳，减少因疲劳驾驶引发的事故。

Abstract: A driver face monitoring system can detect driver fatigue, which is a
significant factor in many accidents, using computer vision techniques. In this
paper, we present a real-time technique for driver eye state detection. First,
the face is detected, and the eyes are located within the face region for
tracking. A normalized cross-correlation-based online dynamic template matching
technique, combined with Kalman filter tracking, is proposed to track the
detected eye positions in subsequent image frames. A support vector machine
with histogram of oriented gradients (HOG) features is used to classify the
state of the eyes as open or closed. If the eyes remain closed for a specified
period, the driver is considered to be asleep, and an alarm is triggered.

</details>


### [163] [ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages](https://arxiv.org/abs/2504.14825)
*Zhoujie Qian*

Main category: cs.CV

TL;DR: ECViT是一种结合CNN和Transformer优势的高效混合架构，解决了ViTs的高计算成本和数据需求问题。


<details>
  <summary>Details</summary>
Motivation: ViTs因自注意力的二次计算复杂性和大数据需求面临挑战，ECViT旨在解决这些问题。

Method: ECViT通过引入CNN的局部性和平移不变性，结合局部注意力和金字塔结构，实现高效多尺度特征提取。

Result: ECViT在图像分类任务中性能优于现有模型，同时保持低计算和存储需求。

Conclusion: ECViT为高效且高性能的应用提供了理想解决方案。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision by leveraging
self-attention to model long-range dependencies. However, ViTs face challenges
such as high computational costs due to the quadratic scaling of self-attention
and the requirement of a large amount of training data. To address these
limitations, we propose the Efficient Convolutional Vision Transformer (ECViT),
a hybrid architecture that effectively combines the strengths of CNNs and
Transformers. ECViT introduces inductive biases such as locality and
translation invariance, inherent to Convolutional Neural Networks (CNNs) into
the Transformer framework by extracting patches from low-level features and
enhancing the encoder with convolutional operations. Additionally, it
incorporates local-attention and a pyramid structure to enable efficient
multi-scale feature extraction and representation. Experimental results
demonstrate that ECViT achieves an optimal balance between performance and
efficiency, outperforming state-of-the-art models on various image
classification tasks while maintaining low computational and storage
requirements. ECViT offers an ideal solution for applications that prioritize
high efficiency without compromising performance.

</details>


### [164] [Distribution-aware Dataset Distillation for Efficient Image Restoration](https://arxiv.org/abs/2504.14826)
*Zhuoran Zheng,Xin Su,Chen Wu,Xiuyi Jia*

Main category: cs.CV

TL;DR: 论文提出了一种名为TripleD的分布感知数据集蒸馏方法，用于图像修复任务，显著减少了训练时间和计算资源。


<details>
  <summary>Details</summary>
Motivation: 随着图像数据的爆炸式增长，训练图像修复模型变得耗时且资源密集。现有数据集蒸馏技术在图像修复领域尚未应用，因此需要填补这一空白。

Method: TripleD利用预训练的视觉Transformer提取图像特征以评估复杂度，基于复杂度选择子集，并通过轻量级CNN调整图像分布。训练分为两阶段：早期关注简单样本，后期选择复杂样本。

Result: TripleD在多项图像修复任务中表现优异，包括多任务、全功能和超高清图像修复，且仅需一台消费级GPU在8小时内完成训练，节省大量资源。

Conclusion: TripleD为图像修复领域提供了一种高效的数据集蒸馏方法，显著降低了训练成本，同时保持了高性能。

Abstract: With the exponential increase in image data, training an image restoration
model is laborious. Dataset distillation is a potential solution to this
problem, yet current distillation techniques are a blank canvas in the field of
image restoration. To fill this gap, we propose the Distribution-aware Dataset
Distillation method (TripleD), a new framework that extends the principles of
dataset distillation to image restoration. Specifically, TripleD uses a
pre-trained vision Transformer to extract features from images for complexity
evaluation, and the subset (the number of samples is much smaller than the
original training set) is selected based on complexity. The selected subset is
then fed through a lightweight CNN that fine-tunes the image distribution to
align with the distribution of the original dataset at the feature level. To
efficiently condense knowledge, the training is divided into two stages. Early
stages focus on simpler, low-complexity samples to build foundational
knowledge, while later stages select more complex and uncertain samples as the
model matures. Our method achieves promising performance on multiple image
restoration tasks, including multi-task image restoration, all-in-one image
restoration, and ultra-high-definition image restoration tasks. Note that we
can train a state-of-the-art image restoration model on an
ultra-high-definition (4K resolution) dataset using only one consumer-grade GPU
in less than 8 hours (500 savings in computing resources and immeasurable
training time).

</details>


### [165] [Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning](https://arxiv.org/abs/2504.14847)
*Xixi Wan,Aihua Zheng,Zi Wang,Bo Jiang,Jin Tang,Jixin Ma*

Main category: cs.CV

TL;DR: 提出了一种名为MGRNet的图推理模型，用于解决多模态ReID任务中局部特征质量差异和模态互补信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了局部特征的质量差异，未能充分利用多模态的互补信息，尤其是在低质量特征情况下。

Method: 构建模态感知图以增强细粒度局部细节提取，采用选择性图节点交换操作缓解低质量特征影响，并通过局部感知图推理模块传播多模态信息。

Result: 在四个基准测试（RGBNT201、Market1501-MM、RGBNT100、MSVR310）上实现了最先进的性能。

Conclusion: MGRNet通过图推理有效提升了多模态ReID任务的性能，并能重构缺失模态信息。

Abstract: Multi-modal data provides abundant and diverse object information, crucial
for effective modal interactions in Re-Identification (ReID) tasks. However,
existing approaches often overlook the quality variations in local features and
fail to fully leverage the complementary information across modalities,
particularly in the case of low-quality features. In this paper, we propose to
address this issue by leveraging a novel graph reasoning model, termed the
Modality-aware Graph Reasoning Network (MGRNet). Specifically, we first
construct modality-aware graphs to enhance the extraction of fine-grained local
details by effectively capturing and modeling the relationships between
patches. Subsequently, the selective graph nodes swap operation is employed to
alleviate the adverse effects of low-quality local features by considering both
local and global information, enhancing the representation of discriminative
information. Finally, the swapped modality-aware graphs are fed into the
local-aware graph reasoning module, which propagates multi-modal information to
yield a reliable feature representation. Another advantage of the proposed
graph reasoning approach is its ability to reconstruct missing modal
information by exploiting inherent structural relationships, thereby minimizing
disparities between different modalities. Experimental results on four
benchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the
proposed method achieves state-of-the-art performance in multi-modal object
ReID. The code for our method will be available upon acceptance.

</details>


### [166] [Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation](https://arxiv.org/abs/2504.14848)
*Yunpu Zhao,Rui Zhang,Junbin Xiao,Ruibo Hou,Jiaming Guo,Zihao Zhang,Yifan Hao,Yunji Chen*

Main category: cs.CV

TL;DR: 提出了一种名为CSP的新框架，通过语义扰动校准视觉语言模型的置信度，提升其可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在多模态任务中表现优异，但其置信度校准不佳，导致用户信任问题。

Method: 通过高斯噪声扰动关键对象区域模拟视觉不确定性，结合两阶段训练（监督微调和偏好优化）提升校准效果。

Result: 实验表明，CSP显著改善了置信度与响应正确性的对齐，同时保持或提升了任务性能。

Conclusion: 语义扰动是提升VLMs可靠性和可解释性的实用工具。

Abstract: Vision-language models (VLMs) excel in various multimodal tasks but
frequently suffer from poor calibration, resulting in misalignment between
their verbalized confidence and response correctness. This miscalibration
undermines user trust, especially when models confidently provide incorrect or
fabricated information. In this work, we propose a novel Confidence Calibration
through Semantic Perturbation (CSP) framework to improve the calibration of
verbalized confidence for VLMs in response to object-centric queries. We first
introduce a perturbed dataset where Gaussian noise is applied to the key object
regions to simulate visual uncertainty at different confidence levels,
establishing an explicit mapping between visual ambiguity and confidence
levels. We further enhance calibration through a two-stage training process
combining supervised fine-tuning on the perturbed dataset with subsequent
preference optimization. Extensive experiments on popular benchmarks
demonstrate that our method significantly improves the alignment between
verbalized confidence and response correctness while maintaining or enhancing
overall task performance. These results highlight the potential of semantic
perturbation as a practical tool for improving the reliability and
interpretability of VLMs.

</details>


### [167] [Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer](https://arxiv.org/abs/2504.14860)
*Ziyi Liu,Yangcen Liu*

Main category: cs.CV

TL;DR: PseudoFormer提出了一种两分支框架，通过生成高质量伪标签和利用不同先验，缩小了弱监督与全监督时序动作定位的性能差距。


<details>
  <summary>Details</summary>
Motivation: 弱监督时序动作定位（WTAL）因缺乏时间标注导致性能不足，现有方法在伪标签质量、先验利用和噪声标签训练方面存在挑战。

Method: 提出RickerFusion映射动作提议到共享空间生成伪标签，结合片段级和提议级标签训练回归模型，并引入不确定性掩码和迭代优化机制。

Result: 在THUMOS14和ActivityNet1.3基准上达到最优性能，消融实验验证了各模块的有效性。

Conclusion: PseudoFormer通过改进伪标签生成和训练机制，显著提升了WTAL性能。

Abstract: Weakly-supervised Temporal Action Localization (WTAL) has achieved notable
success but still suffers from a lack of temporal annotations, leading to a
performance and framework gap compared with fully-supervised methods. While
recent approaches employ pseudo labels for training, three key challenges:
generating high-quality pseudo labels, making full use of different priors, and
optimizing training methods with noisy labels remain unresolved. Due to these
perspectives, we propose PseudoFormer, a novel two-branch framework that
bridges the gap between weakly and fully-supervised Temporal Action
Localization (TAL). We first introduce RickerFusion, which maps all predicted
action proposals to a global shared space to generate pseudo labels with better
quality. Subsequently, we leverage both snippet-level and proposal-level labels
with different priors from the weak branch to train the regression-based model
in the full branch. Finally, the uncertainty mask and iterative refinement
mechanism are applied for training with noisy pseudo labels. PseudoFormer
achieves state-of-the-art WTAL results on the two commonly used benchmarks,
THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate
the contribution of each component of our method.

</details>


### [168] [Twin Co-Adaptive Dialogue for Progressive Image Generation](https://arxiv.org/abs/2504.14868)
*Jianhui Wang,Yangfan He,Yan Zhong,Xinyuan Song,Jiayi Su,Yuheng Feng,Hongyang He,Wenyu Zhu,Xinhang Yuan,Kuan Lu,Menghao Huo,Miao Zhang,Keqin Li,Jiaqi Chen,Tianyu Shi,Xueqian Wang*

Main category: cs.CV

TL;DR: Twin-Co框架通过同步对话逐步优化图像生成，减少用户试错并提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像生成系统在处理用户提示模糊性时的不足。

Method: 采用动态迭代工作流，通过智能对话代理与用户交互，逐步优化生成图像。

Result: 实验表明Twin-Co提升了用户体验和生成图像质量。

Conclusion: Twin-Co通过同步对话有效优化图像生成，适用于多种应用场景。

Abstract: Modern text-to-image generation systems have enabled the creation of
remarkably realistic and high-quality visuals, yet they often falter when
handling the inherent ambiguities in user prompts. In this work, we present
Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to
progressively refine image generation. Instead of a static generation process,
Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue
agent continuously interacts with the user. Initially, a base image is
generated from the user's prompt. Then, through a series of synchronized
dialogue exchanges, the system adapts and optimizes the image according to
evolving user feedback. The co-adaptive process allows the system to
progressively narrow down ambiguities and better align with user intent.
Experiments demonstrate that Twin-Co not only enhances user experience by
reducing trial-and-error iterations but also improves the quality of the
generated images, streamlining the creative process across various
applications.

</details>


### [169] [ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams](https://arxiv.org/abs/2504.14875)
*Chris Dongjoo Kim,Jihwan Moon,Sangwoo Moon,Heeseung Yun,Sihaeng Lee,Aniruddha Kembhavi,Soonyoung Lee,Gunhee Kim,Sangho Lee,Christopher Clark*

Main category: cs.CV

TL;DR: ReSpec是一个基于相关性和特异性的在线过滤框架，用于高效处理视频-文本数据，减少存储和计算需求，同时提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 视频-文本数据的快速增长带来了存储和计算的挑战，需要一种实时处理流数据的方法，同时优化数据选择以提升任务性能。

Method: ReSpec通过四个标准选择数据：模态对齐、任务相关性、特异性和效率，利用概率对齐和根嵌入距离作为代理指标。

Result: 在WebVid2M和VideoCC3M数据集上，ReSpec仅用5%的数据就在五个零样本视频检索任务中达到最先进性能。

Conclusion: ReSpec通过实时数据过滤，显著减少了存储和计算需求，同时保持了高性能，适用于实时响应场景。

Abstract: The rapid growth of video-text data presents challenges in storage and
computation during training. Online learning, which processes streaming data in
real-time, offers a promising solution to these issues while also allowing
swift adaptations in scenarios demanding real-time responsiveness. One strategy
to enhance the efficiency and effectiveness of learning involves identifying
and prioritizing data that enhances performance on target downstream tasks. We
propose Relevance and Specificity-based online filtering framework (ReSpec)
that selects data based on four criteria: (i) modality alignment for clean
data, (ii) task relevance for target focused data, (iii) specificity for
informative and detailed data, and (iv) efficiency for low-latency processing.
Relevance is determined by the probabilistic alignment of incoming data with
downstream tasks, while specificity employs the distance to a root embedding
representing the least specific data as an efficient proxy for informativeness.
By establishing reference points from target task data, ReSpec filters incoming
data in real-time, eliminating the need for extensive storage and compute.
Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains
state-of-the-art performance on five zeroshot video retrieval tasks, using as
little as 5% of the data while incurring minimal compute. The source code is
available at https://github.com/cdjkim/ReSpec.

</details>


### [170] [Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle Re-identification](https://arxiv.org/abs/2504.14877)
*Aihua Zheng,Yongqi Sun,Zi Wang,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出了一种协作增强网络（CoEN），通过生成高质量代理并动态选择主光谱，提升多光谱车辆重识别性能。


<details>
  <summary>Details</summary>
Motivation: 多光谱车辆重识别中，主光谱选择困难及低质量主光谱导致性能下降。

Method: 设计了代理生成器（PG）、动态质量排序模块（DQSM）和协作增强模块（CEM），协同提升光谱特征。

Result: 在三个基准数据集上验证了方法的有效性。

Conclusion: CoEN通过代理生成和协作增强，显著提升了多光谱车辆重识别的鲁棒性。

Abstract: The performance of multi-spectral vehicle Re-identification (ReID) is
significantly degraded when some important discriminative cues in visible, near
infrared and thermal infrared spectra are lost. Existing methods generate or
enhance missing details in low-quality spectra data using the high-quality one,
generally called the primary spectrum, but how to justify the primary spectrum
is a challenging problem. In addition, when the quality of the primary spectrum
is low, the enhancement effect would be greatly degraded, thus limiting the
performance of multi-spectral vehicle ReID. To address these problems, we
propose the Collaborative Enhancement Network (CoEN), which generates a
high-quality proxy from all spectra data and leverages it to supervise the
selection of primary spectrum and enhance all spectra features in a
collaborative manner, for robust multi-spectral vehicle ReID. First, to
integrate the rich cues from all spectra data, we design the Proxy Generator
(PG) to progressively aggregate multi-spectral features. Second, we design the
Dynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring
their correlations with the proxy, to accurately select the primary spectra
with the highest correlation. Finally, we design the Collaborative Enhancement
Module (CEM) to effectively compensate for missing contents of all spectra by
collaborating the primary spectra and the proxy, thereby mitigating the impact
of low-quality primary spectra. Extensive experiments on three benchmark
datasets are conducted to validate the efficacy of the proposed approach
against other multi-spectral vehicle ReID methods. The codes will be released
at https://github.com/yongqisun/CoEN.

</details>


### [171] [Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2504.14884)
*Jingyu Xing,Chenwei Tang,Tao Wang,Rong Xiao,Wei Ju,Ji-Zhe Zhou,Liangli Zhen,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种记忆增强的双解码器网络（MDD-Net），用于解决多类无监督异常检测中的过泛化和正常特征重建不足问题。


<details>
  <summary>Details</summary>
Motivation: 多类场景中，重建方法面临过泛化和正常特征重建不足的挑战，现有方法通常只解决前者，反而加剧后者。

Method: MDD-Net包含双解码器反向蒸馏网络（DRD-Net）和类感知记忆模块（CMM），前者通过两个解码器生成特征差异优化异常评分，后者保存类特定正常原型以避免异常重建。

Result: 实验表明MDD-Net在多类无监督异常检测任务中优于现有方法，减少了假阳性并提高了定位准确性。

Conclusion: MDD-Net通过双解码器和记忆模块有效解决了多类异常检测中的两大挑战，性能优于当前最优方法。

Abstract: Recent advances in unsupervised anomaly detection (UAD) have shifted from
single-class to multi-class scenarios. In such complex contexts, the increasing
pattern diversity has brought two challenges to reconstruction-based
approaches: (1) over-generalization: anomalies that are subtle or share
compositional similarities with normal patterns may be reconstructed with high
fidelity, making them difficult to distinguish from normal instances; and (2)
insufficient normality reconstruction: complex normal features, such as
intricate textures or fine-grained structures, may not be faithfully
reconstructed due to the model's limited representational capacity, resulting
in false positives. Existing methods typically focus on addressing the former,
which unintentionally exacerbate the latter, resulting in inadequate
representation of intricate normal patterns. To concurrently address these two
challenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This
network includes two critical components: a Dual-Decoder Reverse Distillation
Network (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the
DRD-Net incorporates a restoration decoder designed to recover normal features
from synthetic abnormal inputs and an identity decoder to reconstruct features
that maintain the anomalous semantics. By exploiting the discrepancy between
features produced by two decoders, our approach refines anomaly scores beyond
the conventional encoder-decoder comparison paradigm, effectively reducing
false positives and enhancing localization accuracy. Furthermore, the CMM
explicitly encodes and preserves class-specific normal prototypes, actively
steering the network away from anomaly reconstruction. Comprehensive
experimental results across several benchmarks demonstrate the superior
performance of our MDD-Net framework over current SoTA approaches in
multi-class UAD tasks.

</details>


### [172] [WMKA-Net: A Weighted Multi-Kernel Attention NetworkMethod for Retinal Vessel Segmentation](https://arxiv.org/abs/2504.14888)
*Xinran Xu,Yuliang Ma,Sifu Cai*

Main category: cs.CV

TL;DR: WMKA-Net是一种新型视网膜血管分割网络，通过多尺度特征融合和注意力机制提升小血管和低对比度区域的分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决视网膜血管分割中多尺度特征捕获不足、上下文信息丢失和噪声敏感性问题。

Method: 结合MKDC模块（多尺度并行卷积核）、UDFF策略（加权融合高低层特征）和AttentionBlock（注意力机制）。

Result: 在多个公开数据集中表现出色，尤其在小血管和病理区域分割上效果显著。

Conclusion: WMKA-Net为视网膜血管分割提供了一种高效且鲁棒的新方法。

Abstract: We propose a novel retinal vessel segmentation network, the Weighted
Multi-Kernel Attention Network (WMKA-Net), which aims to address the issues of
insufficient multiscale feature capture, loss of contextual information, and
noise sensitivity in retinal vessel segmentation. WMKA-Net significantly
improves the segmentation performance of small vessels and low-contrast regions
by integrating several innovative components, including the MultiKernelFeature
Fusion Module (MKDC), the Progressive Feature Weighting Fusion Strategy (UDFF),
and the Attention Mechanism Module (AttentionBlock). The MKDC module employs
multiscale parallel convolutional kernels to extract vessel characteristics,
thereby enhancing the ability to capture complex vascular structures. The UDFF
strategy optimizes the transmission of feature information by weighted fusion
of high- and low-level features. The AttentionBlock highlights key regions and
suppresses noise interference through the attention mechanism. Experimental
results demonstrate that WMKA-Net achieves excellent segmentation performance
in multiple public datasets, particularly in segmentation of small vessels and
processing of pathological regions. This work provides a robust and efficient
new method for segmentation of the retinal vessel.

</details>


### [173] [Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation](https://arxiv.org/abs/2504.14899)
*Chenjie Cao,Jingkai Zhou,Shikai Li,Jingyun Liang,Chaohui Yu,Fan Wang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: Uni3C是一个统一框架，通过点云和3D世界引导，实现了视频生成中相机和人体运动的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常分开处理相机和人体运动控制，且依赖高质量标注数据，限制了效果。

Method: 提出PCDController模块和联合对齐的3D世界引导，分别控制相机和人体运动。

Result: Uni3C在相机控制和人体运动质量上优于现有方法，且具有强鲁棒性。

Conclusion: Uni3C通过模块化设计和3D引导，减少了对联合标注数据的依赖，提升了控制效果。

Abstract: Camera and human motion controls have been extensively studied for video
generation, but existing approaches typically address them separately,
suffering from limited data with high-quality annotations for both aspects. To
overcome this, we present Uni3C, a unified 3D-enhanced framework for precise
control of both camera and human motion in video generation. Uni3C includes two
key contributions. First, we propose a plug-and-play control module trained
with a frozen video generative backbone, PCDController, which utilizes
unprojected point clouds from monocular depth to achieve accurate camera
control. By leveraging the strong 3D priors of point clouds and the powerful
capacities of video foundational models, PCDController shows impressive
generalization, performing well regardless of whether the inference backbone is
frozen or fine-tuned. This flexibility enables different modules of Uni3C to be
trained in specific domains, i.e., either camera control or human motion
control, reducing the dependency on jointly annotated data. Second, we propose
a jointly aligned 3D world guidance for the inference phase that seamlessly
integrates both scenic point clouds and SMPL-X characters to unify the control
signals for camera and human motion, respectively. Extensive experiments
confirm that PCDController enjoys strong robustness in driving camera motion
for fine-tuned backbones of video generation. Uni3C substantially outperforms
competitors in both camera controllability and human motion quality.
Additionally, we collect tailored validation sets featuring challenging camera
movements and human actions to validate the effectiveness of our method.

</details>


### [174] [Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments](https://arxiv.org/abs/2504.14913)
*Kenji Iwata,Eiki Ishidera,Toshifumi Yamaai,Yutaka Satoh,Hiroshi Tanaka,Katsuhiko Takahashi,Akio Furuhata,Yoshihisa Tanabe,Hiroshi Matsumura*

Main category: cs.CV

TL;DR: 论文总结了OCR性能下降的外部干扰因素，并整理为指南。


<details>
  <summary>Details</summary>
Motivation: 随着OCR应用范围扩大，外部环境干扰导致性能下降，影响识别精度和设备质量控制。

Method: 整理实际外部干扰因素及图像退化现象，形成干扰因素表，并制定使用指南。

Result: 提供了外部干扰因素表和指南，帮助用户正确使用OCR。

Conclusion: 通过指南确保用户能有效应对OCR性能下降问题。

Abstract: The performance of OCR has improved with the evolution of AI technology. As
OCR continues to broaden its range of applications, the increased likelihood of
interference introduced by various usage environments can prevent it from
achieving its inherent performance. This results in reduced recognition
accuracy under certain conditions, and makes the quality control of recognition
devices more challenging. Therefore, to ensure that users can properly utilize
OCR, we compiled the real-world external disturbance factors that cause
performance degradation, along with the resulting image degradation phenomena,
into an external disturbance factor table and, by also indicating how to make
use of it, organized them into guidelines.

</details>


### [175] [GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection](https://arxiv.org/abs/2504.14919)
*Donghyeong Kim,Chaewon Park,Suhwan Cho,Hyeonjeong Lim,Minseok Kang,Jungho Lee,Sangyoun Lee*

Main category: cs.CV

TL;DR: GenCLIP通过多层提示和双分支推理，提升零样本异常检测的通用性和类别特异性。


<details>
  <summary>Details</summary>
Motivation: 解决零样本异常检测中通用提示的稳定优化和有效部署问题。

Method: 采用多层提示整合不同CLIP层的视觉线索，结合双分支推理平衡通用性与特异性，并引入自适应文本提示过滤机制。

Result: 提升了在未见类别上的异常检测稳定性和可靠性。

Conclusion: GenCLIP框架有效解决了零样本异常检测中的关键挑战，具有广泛的应用潜力。

Abstract: Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen
categories by leveraging CLIP's zero-shot capabilities to match text prompts
with visual features. A key challenge in ZSAD is learning general prompts
stably and utilizing them effectively, while maintaining both generalizability
and category specificity. Although general prompts have been explored in prior
works, achieving their stable optimization and effective deployment remains a
significant challenge. In this work, we propose GenCLIP, a novel framework that
learns and leverages general prompts more effectively through multi-layer
prompting and dual-branch inference. Multi-layer prompting integrates
category-specific visual cues from different CLIP layers, enriching general
prompts with more comprehensive and robust feature representations. By
combining general prompts with multi-layer visual features, our method further
enhances its generalization capability. To balance specificity and
generalization, we introduce a dual-branch inference strategy, where a
vision-enhanced branch captures fine-grained category-specific features, while
a query-only branch prioritizes generalization. The complementary outputs from
both branches improve the stability and reliability of anomaly detection across
unseen categories. Additionally, we propose an adaptive text prompt filtering
mechanism, which removes irrelevant or atypical class names not encountered
during CLIP's training, ensuring that only meaningful textual inputs contribute
to the final vision-language alignment.

</details>


### [176] [DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding](https://arxiv.org/abs/2504.14920)
*Geng Li,Jinglin Xu,Yunzhen Zhao,Yuxin Peng*

Main category: cs.CV

TL;DR: Dyfo是一种无需训练的视觉搜索方法，通过双向交互和MCTS算法模拟人类视觉聚焦，提升多模态模型的细粒度视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 受人类视觉搜索机制启发，旨在解决现有方法需要额外模块或数据收集的问题。

Method: 利用双向交互和MCTS算法动态调整视觉焦点，无需额外训练或模块。

Result: 显著提升细粒度视觉理解，减少幻觉问题，在固定和动态分辨率模型中表现优异。

Conclusion: Dyfo为多模态模型提供了一种高效、无需训练的视觉搜索解决方案。

Abstract: Humans can effortlessly locate desired objects in cluttered environments,
relying on a cognitive mechanism known as visual search to efficiently filter
out irrelevant information and focus on task-related regions. Inspired by this
process, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing
visual search method that enhances fine-grained visual understanding in large
multimodal models (LMMs). Unlike existing approaches which require additional
modules or data collection, Dyfo leverages a bidirectional interaction between
LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to
simulate human-like focus adjustments. This enables LMMs to focus on key visual
regions while filtering out irrelevant content, without introducing additional
training caused by vocabulary expansion or the integration of specialized
localization modules. Experimental results demonstrate that Dyfo significantly
improves fine-grained visual understanding and reduces hallucination issues in
LMMs, achieving superior performance across both fixed and dynamic resolution
models. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025

</details>


### [177] [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](https://arxiv.org/abs/2504.14921)
*Songping Wang,Hanqing Liu,Yueming Lyu,Xiantao Hu,Ziwen He,Wei Wang,Caifeng Shan,Liang Wang*

Main category: cs.CV

TL;DR: VFAT-WS是一种针对视频数据的快速对抗训练方法，通过时间频率增强和弱到强的一致性正则化，显著提高了训练效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视频对抗训练面临计算成本高和清洁准确性与鲁棒性之间的权衡问题，VFAT-WS旨在解决这些问题。

Method: 结合时间频率增强（TF-AUG）及其时空增强版本（STF-AUG），以及单步PGD攻击，同时采用弱到强的一致性正则化。

Result: 在UCF-101和HMDB-51数据集上，VFAT-WS显著提升了对抗鲁棒性和训练速度（加速490%）。

Conclusion: VFAT-WS通过高效设计和一致性正则化，成功平衡了清洁准确性和鲁棒性，为视频对抗训练提供了实用解决方案。

Abstract: Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.

</details>


### [178] [TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models](https://arxiv.org/abs/2504.14933)
*Mazharul Islam Rakib,Showrin Rahman,Joyanta Jyoti Mondal,Xi Xiao,David Lewis,Alessandra Mileo,Meem Arafat Manab*

Main category: cs.CV

TL;DR: 提出一种基于条件扩散模型的两步图像生成方法，避免版权侵权和源复制问题。


<details>
  <summary>Details</summary>
Motivation: 解决生成AI模型在图像生成中可能侵犯版权的问题，尤其是源复制现象。

Method: 采用两步法：首先生成图像分割掩码，然后扩散模型重新生成图像并避免掩码形状。

Result: 降低了与训练图像的结构相似性，有效避免源复制，无需昂贵重训练。

Conclusion: 该方法计算成本低，是解决扩散模型版权问题的有效途径。

Abstract: In today's age of social media and marketing, copyright issues can be a major
roadblock to the free sharing of images. Generative AI models have made it
possible to create high-quality images, but concerns about copyright
infringement are a hindrance to their abundant use. As these models use data
from training images to generate new ones, it is often a daunting task to
ensure they do not violate intellectual property rights. Some AI models have
even been noted to directly copy copyrighted images, a problem often referred
to as source copying. Traditional copyright protection measures such as
watermarks and metadata have also proven to be futile in this regard. To
address this issue, we propose a novel two-step image generation model inspired
by the conditional diffusion model. The first step involves creating an image
segmentation mask for some prompt-based generated images. This mask embodies
the shape of the image. Thereafter, the diffusion model is asked to generate
the image anew while avoiding the shape in question. This approach shows a
decrease in structural similarity from the training image, i.e. we are able to
avoid the source copying problem using this approach without expensive
retraining of the model or user-centered prompt generation techniques. This
makes our approach the most computationally inexpensive approach to avoiding
both copyright infringement and source copying for diffusion model-based image
generation.

</details>


### [179] [PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV](https://arxiv.org/abs/2504.14952)
*Qianyu Zhu,Junjie Wang,Jeremiah Hu,Jia Ai,Yong Lee*

Main category: cs.CV

TL;DR: 论文提出了一种基于迁移学习的去噪扩散模型（PIV-FlowDiffuser），用于减少粒子图像测速（PIV）中的特殊噪声，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在合成数据集上训练后，在实际粒子图像中可能因领域差异表现不佳，导致向量场中出现特殊噪声。

Method: 采用去噪扩散模型（FlowDiffuser），通过迁移学习策略训练：先在计算机视觉数据集（如Sintel、KITTI）上预训练，再在合成PIV数据集上微调。

Result: PIV-FlowDiffuser将平均端点误差（AEE）降低了59.4%，并在未见过的粒子图像上表现出更好的泛化性能。

Conclusion: 研究强调了基于迁移学习的去噪扩散模型在PIV中的潜力，并提供了详细实现供参考。

Abstract: Deep learning algorithms have significantly reduced the computational time
and improved the spatial resolution of particle image velocimetry~(PIV).
However, the models trained on synthetic datasets might have a degraded
performance on practical particle images due to domain gaps. As a result,
special residual patterns are often observed for the vector fields of deep
learning-based estimators. To reduce the special noise step-by-step, we employ
a denoising diffusion model~(FlowDiffuser) for PIV analysis. And the
data-hungry iterative denoising diffusion model is trained via a transfer
learning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)
pre-training a FlowDiffuser model with multiple optical flow datasets of the
computer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the
pre-trained model on synthetic PIV datasets. Note that the PIV images are
upsampled by a factor of two to resolve the small-scale turbulent flow
structures. The visualized results indicate that our PIV-FlowDiffuser
effectively suppresses the noise patterns. Therefore, the denoising diffusion
model reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV
baseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits
enhanced generalization performance on unseen particle images due to transfer
learning. Overall, this study highlights the transfer-learning-based denoising
diffusion models for PIV. And a detailed implementation is recommended for
interested readers in the repository
https://github.com/Zhu-Qianyu/PIV-FlowDiffuser.

</details>


### [180] [3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations](https://arxiv.org/abs/2504.14967)
*Yating Wang,Xuan Wang,Ran Yi,Yanbo Fan,Jichen Hu,Jingcheng Zhu,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种结合3D高斯和3DMM的高质量3D头像生成方法，解决了动态纹理捕获和存储开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效捕获动态纹理或存在运行时速度和存储空间的高开销问题。

Method: 引入紧凑的张量表示，静态纹理存储在tri-planes中，动态纹理细节用轻量级1D特征线表示，并提出自适应截断不透明度惩罚和类平衡采样。

Result: 实验表明该方法能准确捕获面部动态细节，保持实时渲染并显著降低存储成本。

Conclusion: 该方法扩展了3D头像在更多场景中的适用性。

Abstract: Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to
construct high-quality 3D head avatars. In this line of research, existing
methods either fail to capture the dynamic textures or incur significant
overhead in terms of runtime speed or storage space. To this end, we propose a
novel method that addresses all the aforementioned demands. In specific, we
introduce an expressive and compact representation that encodes texture-related
attributes of the 3D Gaussians in the tensorial format. We store appearance of
neutral expression in static tri-planes, and represents dynamic texture details
for different expressions using lightweight 1D feature lines, which are then
decoded into opacity offset relative to the neutral face. We further propose
adaptive truncated opacity penalty and class-balanced sampling to improve
generalization across different expressions. Experiments show this design
enables accurate face dynamic details capturing while maintains real-time
rendering and significantly reduces storage costs, thus broadening the
applicability to more scenarios.

</details>


### [181] [Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization](https://arxiv.org/abs/2504.14975)
*Hongbin Xu,Chaohui Yu,Feng Xiao,Jiazheng Xing,Hai Ci,Weitao Chen,Ming Li*

Main category: cs.CV

TL;DR: 论文提出了一个名为\name{}的新框架，通过循环一致性增强可控3D生成，解决了现有方法在输入条件（如边缘和深度）与生成内容之间对齐不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法在保持输入条件与生成内容的一致性方面存在显著挑战，导致明显的不匹配。

Method: 采用高效的feed-forward主干网络，通过循环过程（包括视图一致性和条件一致性约束）生成和重新生成3D内容，以确保对齐。

Result: 在多个基准测试中，\name{}显著提升了可控性，特别是在细节方面（如边缘PSNR提升14.17%，草图PSNR提升6.26%）。

Conclusion: \name{}通过循环一致性约束有效提升了3D生成的可控性，尤其在细节对齐方面优于现有方法。

Abstract: Despite the remarkable progress of 3D generation, achieving controllability,
i.e., ensuring consistency between generated 3D content and input conditions
like edge and depth, remains a significant challenge. Existing methods often
struggle to maintain accurate alignment, leading to noticeable discrepancies.
To address this issue, we propose \name{}, a new framework that enhances
controllable 3D generation by explicitly encouraging cyclic consistency between
the second-order 3D content, generated based on extracted signals from the
first-order generation, and its original input controls. Specifically, we
employ an efficient feed-forward backbone that can generate a 3D object from an
input condition and a text prompt. Given an initial viewpoint and a control
signal, a novel view is rendered from the generated 3D content, from which the
extracted condition is used to regenerate the 3D content. This re-generated
output is then rendered back to the initial viewpoint, followed by another
round of control signal extraction, forming a cyclic process with two
consistency constraints. \emph{View consistency} ensures coherence between the
two generated 3D objects, measured by semantic similarity to accommodate
generative diversity. \emph{Condition consistency} aligns the final extracted
signal with the original input control, preserving structural or geometric
details throughout the process. Extensive experiments on popular benchmarks
demonstrate that \name{} significantly improves controllability, especially for
fine-grained details, outperforming existing methods across various conditions
(e.g., +14.17\% PSNR for edge, +6.26\% PSNR for sketch).

</details>


### [182] [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
*Ji Qi,Yuan Yao,Yushi Bai,Bin Xu,Juanzi Li,Zhiyuan Liu,Tat-Seng Chua*

Main category: cs.CV

TL;DR: Quicksviewer是一种新型大型多模态模型（LMM），通过动态分区和统一重采样视频帧，显著提升视频理解的效率，减少时空冗余，并在性能上优于固定分区策略的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有LMM对视频帧的均匀感知导致计算效率低下，尤其是对信息密度不均的视频。

Method: 使用Gumbel Softmax将视频分区为非均匀密度的立方体，并对每个立方体进行统一重采样，动态压缩视频。

Result: 实现了45倍的压缩率，训练效率高，性能优于基线模型（最高提升8.72准确率），在Video-MME上达到SOTA。

Conclusion: Quicksviewer的动态分区策略显著提升了视频理解效率，且模型能力随输入帧数增加呈幂律增长。

Abstract: Large Multimodal Models (LMMs) uniformly perceive video frames, creating
computational inefficiency for videos with inherently varying temporal
information density. This paper present \textbf{Quicksviewer}, an LMM with new
perceiving paradigm that partitions a video of nonuniform density into varying
cubes using Gumbel Softmax, followed by a unified resampling for each cube to
achieve efficient video understanding. This simple and intuitive approach
dynamically compress video online based on its temporal density, significantly
reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while
enabling efficient training with large receptive field. We train the model from
a language backbone through three progressive stages, each incorporating
lengthy videos on average of 420s/1fps thanks to the perceiving efficiency.
With only 0.8M total video-text samples for training, our model outperforms the
direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in
accuracy, demonstrating the effectiveness in performance. On Video-MME,
Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\%
of tokens per frame required by baselines. With this paradigm, scaling up the
number of input frames reveals a clear power law of the model capabilities. It
is also empirically verified that the segments generated by the cubing network
can help for analyzing continuous events in videos.

</details>


### [183] [RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild](https://arxiv.org/abs/2504.14977)
*Jingkai Zhou,Yifan Wu,Shikai Li,Min Wei,Chao Fan,Weihua Chen,Wei Jiang,Fan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于强大基础模型的简单修改方法RealisDance-DiT，通过灵活微调策略解决可控角色动画中的挑战，并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决可控角色动画中罕见姿势、风格化角色、角色-物体交互、复杂光照和动态场景等挑战，现有方法泛化能力不足。

Method: 基于Wan-2.1视频基础模型，提出RealisDance-DiT，通过最小化架构修改和低噪声预热、大批次小迭代策略优化微调。

Result: 实验表明RealisDance-DiT在多样真实场景中表现优异，大幅超越现有方法。

Conclusion: 强大的基础模型结合简单修改和灵活微调策略，可有效解决可控角色动画的开放世界挑战。

Abstract: Controllable character animation remains a challenging problem, particularly
in handling rare poses, stylized characters, character-object interactions,
complex illumination, and dynamic scenes. To tackle these issues, prior work
has largely focused on injecting pose and appearance guidance via elaborate
bypass networks, but often struggles to generalize to open-world scenarios. In
this paper, we propose a new perspective that, as long as the foundation model
is powerful enough, straightforward model modifications with flexible
fine-tuning strategies can largely address the above challenges, taking a step
towards controllable character animation in the wild. Specifically, we
introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our
sufficient analysis reveals that the widely adopted Reference Net design is
suboptimal for large-scale DiT models. Instead, we demonstrate that minimal
modifications to the foundation model architecture yield a surprisingly strong
baseline. We further propose the low-noise warmup and "large batches and small
iterations" strategies to accelerate model convergence during fine-tuning while
maximally preserving the priors of the foundation model. In addition, we
introduce a new test dataset that captures diverse real-world challenges,
complementing existing benchmarks such as TikTok dataset and UBC fashion video
dataset, to comprehensively evaluate the proposed method. Extensive experiments
show that RealisDance-DiT outperforms existing methods by a large margin.

</details>


### [184] [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
*Chun-Hsiao Yeh,Chenyu Wang,Shengbang Tong,Ta-Ying Cheng,Rouyu Wang,Tianzhe Chu,Yuexiang Zhai,Yubei Chen,Shenghua Gao,Yi Ma*

Main category: cs.CV

TL;DR: 论文提出了All-Angles Bench基准，用于评估多模态大语言模型（MLLMs）在多视角场景理解中的表现，发现其在跨视角一致性和几何对应方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 多视角理解是MLLMs作为具身智能体的关键能力，但现有模型在高层次推理中表现优异，却在多视角几何一致性和跨视角对应上表现不佳。

Method: 通过构建包含2,100个人工标注的多视角问答对的基准All-Angles Bench，设计了六项任务测试模型的几何对应和跨视角信息对齐能力。

Result: 在27个代表性MLLMs上的实验显示，当前模型与人类水平存在显著差距，尤其在部分遮挡视角的跨视角对应和粗略相机姿态估计上表现较差。

Conclusion: 研究强调了在多视角理解中嵌入更强多视角感知的必要性，All-Angles Bench为缩小MLLMs与人类水平的差距提供了宝贵见解。

Abstract: Multi-view understanding, the ability to reconcile visual information across
diverse viewpoints for effective navigation, manipulation, and 3D scene
comprehension, is a fundamental challenge in Multi-Modal Large Language Models
(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive
advances in high-level reasoning and planning, they frequently fall short when
confronted with multi-view geometric consistency and cross-view correspondence.
To comprehensively evaluate the challenges of MLLMs in multi-view scene
reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human
carefully annotated multi-view question-answer pairs across 90 diverse
real-world scenes. Our six tasks (counting, attribute identification, relative
distance, relative direction, object manipulation, and camera pose estimation)
specifically test model's geometric correspondence and the capacity to align
information consistently across views. Our extensive experiments, benchmark on
27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and
GPT-4o against human evaluators reveals a substantial performance gap,
indicating that current MLLMs remain far from human-level proficiency. Through
in-depth analysis, we show that MLLMs are particularly underperforming under
two aspects: (1) cross-view correspondence for partially occluded views and (2)
establishing the coarse camera poses. These findings highlight the necessity of
domain-specific refinements or modules that embed stronger multi-view
awareness. We believe that our All-Angles Bench offers valuable insights and
contribute to bridging the gap between MLLMs and human-level multi-view
understanding. The project and benchmark are publicly available at
https://danielchyeh.github.io/All-Angles-Bench/.

</details>


### [185] [Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation](https://arxiv.org/abs/2504.14988)
*Hong-Tao Yu,Xiu-Shen Wei,Yuxin Peng,Serge Belongie*

Main category: cs.CV

TL;DR: 该论文提出了一个名为FG-BMK的细粒度评估基准，用于评估大型视觉语言模型（LVLMs）在细粒度图像任务中的表现，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态感知方面表现出色，但细粒度图像任务的评估仍未被充分研究。

Method: 通过构建包含349万问题和332万图像的FG-BMK基准，从人类和机器视角系统评估LVLMs的语义识别和细粒度特征表示能力。

Result: 实验揭示了训练范式、模态对齐、扰动敏感性和细粒度类别推理对任务性能的影响。

Conclusion: 研究为当前LVLMs的局限性提供了关键见解，并为未来数据构建和模型设计提供了指导。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
remarkable multimodal perception capabilities, garnering significant attention.
While numerous evaluation studies have emerged, assessing LVLMs both
holistically and on specialized tasks, fine-grained image tasks-fundamental to
computer vision-remain largely unexplored. To fill this gap, we introduce a
comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49
million questions and 3.32 million images. Our evaluation systematically
examines LVLMs from both human-oriented and machine-oriented perspectives,
focusing on their semantic recognition and fine-grained feature representation
capabilities. Through extensive experiments on eight representative LVLMs/VLMs,
we uncover key findings regarding the influence of training paradigms, modality
alignment, perturbation susceptibility, and fine-grained category reasoning on
task performance. This work provides critical insights into the limitations of
current LVLMs and offers guidance for future data construction and model design
in the development of more advanced LVLMs. Our code is open-source and
available at https://github.com/SEU-VIPGroup/FG-BMK.

</details>


### [186] [NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study](https://arxiv.org/abs/2504.15003)
*Xin Li,Xijun Wang,Bingchen Li,Kun Yuan,Yizhen Shao,Suhang Yao,Ming Sun,Chao Zhou,Radu Timofte,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出了首个用于短用户生成内容（UGC）图像超分辨率的基准数据集KwaiSR，包含合成和野生两部分，用于推动相关算法研究。


<details>
  <summary>Details</summary>
Motivation: 为短UGC平台的图像超分辨率研究提供基准数据集，填补现有研究的空白。

Method: 从Kwai平台收集数据，分为合成（模拟退化）和野生（直接采集）两部分，并通过KVQ质量评估方法筛选。

Result: KwaiSR数据集包含1800对合成图像和1900张野生图像，挑战赛结果显示现有超分辨率方法难以应对。

Conclusion: KwaiSR数据集具有挑战性，有望引领图像超分辨率领域的新方向。

Abstract: In this work, we build the first benchmark dataset for short-form UGC Image
Super-resolution in the wild, termed KwaiSR, intending to advance the research
on developing image super-resolution algorithms for short-form UGC platforms.
This dataset is collected from the Kwai Platform, which is composed of two
parts, i.e., synthetic and wild parts. Among them, the synthetic dataset,
including 1,900 image pairs, is produced by simulating the degradation
following the distribution of real-world low-quality short-form UGC images,
aiming to provide the ground truth for training and objective comparison in the
validation/testing. The wild dataset contains low-quality images collected
directly from the Kwai Platform, which are filtered using the quality
assessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset
contains 1800 synthetic image pairs and 1900 wild images, which are divided
into training, validation, and testing parts with a ratio of 8:1:1. Based on
the KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form
UGC Video quality assessment and enhancement, which attracts lots of
researchers to develop the algorithm for it. The results of this competition
have revealed that our KwaiSR dataset is pretty challenging for existing Image
SR methods, which is expected to lead to a new direction in the image
super-resolution field. The dataset can be found from
https://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.

</details>


### [187] [Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images](https://arxiv.org/abs/2504.15007)
*David C Wong,Bin Wang,Gorkem Durak,Marouane Tliba,Mohamed Amine Kerkouri,Aladine Chetouani,Ahmet Enis Cetin,Cagdas Topel,Nicolo Gennaro,Camila Vendrami,Tugce Agirlar Trabzonlu,Amir Ali Rahsepar,Laetitia Perronne,Matthew Antalek,Onural Ozturk,Gokcan Okur,Andrew C. Gordon,Ayis Pyrros,Frank H Miller,Amir A Borhani,Hatice Savas,Eric M. Hart*

Main category: cs.CV

TL;DR: 该论文通过分析放射科医生的眼动模式，研究了他们在真实与深度学习生成图像上的注意力分配差异。


<details>
  <summary>Details</summary>
Motivation: 探索放射科医生在诊断过程中如何分配注意力，以及真实与合成图像对其视觉行为的影响。

Method: 测量眼动模式（如扫视方向、幅度及其联合分布）和注视偏差图，比较真实与合成图像的视觉显著性差异。

Result: 揭示了放射科医生在真实与合成图像上的注意力分配和视觉行为差异。

Conclusion: 眼动分析为理解放射科医生的诊断策略及合成图像的影响提供了重要工具。

Abstract: Eye-tracking analysis plays a vital role in medical imaging, providing key
insights into how radiologists visually interpret and diagnose clinical cases.
In this work, we first analyze radiologists' attention and agreement by
measuring the distribution of various eye-movement patterns, including saccades
direction, amplitude, and their joint distribution. These metrics help uncover
patterns in attention allocation and diagnostic strategies. Furthermore, we
investigate whether and how doctors' gaze behavior shifts when viewing
authentic (Real) versus deep-learning-generated (Fake) images. To achieve this,
we examine fixation bias maps, focusing on first, last, short, and longest
fixations independently, along with detailed saccades patterns, to quantify
differences in gaze distribution and visual saliency between authentic and
synthetic images.

</details>


### [188] [Insert Anything: Image Insertion via In-Context Editing in DiT](https://arxiv.org/abs/2504.15009)
*Wensong Song,Hong Jiang,Zongxing Yang,Ruijie Quan,Yi Yang*

Main category: cs.CV

TL;DR: Insert Anything是一个统一的框架，用于基于参考的图像插入，支持灵活的文本和掩码引导编辑，并在多样化任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要为不同任务训练单独模型，缺乏灵活性和泛化能力。本文旨在提出一个统一的框架，支持多种插入场景。

Method: 利用Diffusion Transformer的多模态注意力，结合掩码和文本引导编辑，并引入上下文编辑机制，将参考图像作为上下文信息。

Result: 在AnyInsertion、DreamBooth和VTON-HD基准测试中表现优于现有方法，展示了在创意内容生成等实际应用中的潜力。

Conclusion: Insert Anything框架在统一性和性能上具有优势，适用于多种实际应用场景。

Abstract: This work presents Insert Anything, a unified framework for reference-based
image insertion that seamlessly integrates objects from reference images into
target scenes under flexible, user-specified control guidance. Instead of
training separate models for individual tasks, our approach is trained once on
our new AnyInsertion dataset--comprising 120K prompt-image pairs covering
diverse tasks such as person, object, and garment insertion--and effortlessly
generalizes to a wide range of insertion scenarios. Such a challenging setting
requires capturing both identity features and fine-grained details, while
allowing versatile local adaptations in style, color, and texture. To this end,
we propose to leverage the multimodal attention of the Diffusion Transformer
(DiT) to support both mask- and text-guided editing. Furthermore, we introduce
an in-context editing mechanism that treats the reference image as contextual
information, employing two prompting strategies to harmonize the inserted
elements with the target scene while faithfully preserving their distinctive
features. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD
benchmarks demonstrate that our method consistently outperforms existing
alternatives, underscoring its great potential in real-world applications such
as creative content generation, virtual try-on, and scene composition.

</details>


### [189] [Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models](https://arxiv.org/abs/2504.15026)
*Zijin Yang,Xin Zhang,Kejiang Chen,Kai Zeng,Qiyi Yao,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TL;DR: Gaussian Shading++ 是一种针对扩散模型的实用水印方法，解决了现有方法在密钥管理、用户定义参数和第三方验证方面的不足。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际应用中面临版权保护和不当内容生成的伦理问题，现有水印方法忽视了密钥管理和第三方验证等现实挑战。

Method: 提出双通道设计，利用伪随机纠错码编码水印种子，并采用软决策解码策略增强鲁棒性，同时引入公钥签名支持第三方验证。

Result: 实验表明，Gaussian Shading++ 在保持性能无损的同时，鲁棒性优于现有方法。

Conclusion: Gaussian Shading++ 是一种更实用的水印解决方案，适用于现实部署场景。

Abstract: Ethical concerns surrounding copyright protection and inappropriate content
generation pose challenges for the practical implementation of diffusion
models. One effective solution involves watermarking the generated images.
Existing methods primarily focus on ensuring that watermark embedding does not
degrade the model performance. However, they often overlook critical challenges
in real-world deployment scenarios, such as the complexity of watermark key
management, user-defined generation parameters, and the difficulty of
verification by arbitrary third parties. To address this issue, we propose
Gaussian Shading++, a diffusion model watermarking method tailored for
real-world deployment. We propose a double-channel design that leverages
pseudorandom error-correcting codes to encode the random seed required for
watermark pseudorandomization, achieving performance-lossless watermarking
under a fixed watermark key and overcoming key management challenges.
Additionally, we model the distortions introduced during generation and
inversion as an additive white Gaussian noise channel and employ a novel soft
decision decoding strategy during extraction, ensuring strong robustness even
when generation parameters vary. To enable third-party verification, we
incorporate public key signatures, which provide a certain level of resistance
against forgery attacks even when model inversion capabilities are fully
disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only
maintains performance losslessness but also outperforms existing methods in
terms of robustness, making it a more practical solution for real-world
deployment.

</details>


### [190] [DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation](https://arxiv.org/abs/2504.15032)
*Weijie He,Mushui Liu,Yunlong Yu,Zhao Wang,Chao Wu*

Main category: cs.CV

TL;DR: DyST-XL是一个无需训练的框架，通过动态布局规划、双提示控制注意力机制和实体一致性约束，显著提升了文本到视频生成的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在合成动态场景时面临的布局不连续、实体身份漂移和交互动态不合理等问题。

Method: 1. 动态布局规划器利用LLM解析输入提示并生成物理感知的关键帧布局；2. 双提示控制注意力机制通过帧感知注意力掩码实现局部对齐；3. 实体一致性约束策略传播首帧特征以保持对象身份。

Result: DyST-XL在复杂提示下表现优异，显著提升了训练无关的视频合成能力。

Conclusion: DyST-XL填补了训练无关视频合成的关键空白，为复杂场景生成提供了有效解决方案。

Abstract: Compositional text-to-video generation, which requires synthesizing dynamic
scenes with multiple interacting entities and precise spatial-temporal
relationships, remains a critical challenge for diffusion-based models.
Existing methods struggle with layout discontinuity, entity identity drift, and
implausible interaction dynamics due to unconstrained cross-attention
mechanisms and inadequate physics-aware reasoning. To address these
limitations, we propose DyST-XL, a \textbf{training-free} framework that
enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through
frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic
Layout Planner that leverages large language models (LLMs) to parse input
prompts into entity-attribute graphs and generates physics-aware keyframe
layouts, with intermediate frames interpolated via trajectory optimization; (2)
A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video
alignment through frame-aware attention masking, achieving the precise control
over individual entities; and (3) An Entity-Consistency Constraint strategy
that propagates first-frame feature embeddings to subsequent frames during
denoising, preserving object identity without manual annotation. Experiments
demonstrate that DyST-XL excels in compositional text-to-video generation,
significantly improving performance on complex prompts and bridging a crucial
gap in training-free video synthesis.

</details>


### [191] [Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification](https://arxiv.org/abs/2504.15041)
*Shiben Liu,Huijie Fan,Qiang Wang,Baojie Fan,Yandong Tang,Liangqiong Qu*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAFC的新模型，通过跨域共享表示学习和域特定分布集成，解决了LReID中的遗忘问题，无需使用旧样本或知识蒸馏。实验结果表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LReID中在适应新信息时保留旧知识的关键挑战，现有方法（基于排练和无排练）存在遗忘问题。

Method: 提出DAFC模型，包括文本驱动的提示聚合（TPA）、分布感知与集成（DAI）和知识巩固机制（KCM）。

Result: DAFC在两种训练顺序下的平均mAP/R@1分别优于现有方法至少9.8%/6.6%和6.4%/6.2%。

Conclusion: DAFC通过跨域共享表示和域特定分布集成，有效缓解了遗忘问题，提升了性能。

Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in
preserving old knowledge while adapting to new information. The existing
solutions include rehearsal-based and rehearsal-free methods to address this
challenge. Rehearsal-based approaches rely on knowledge distillation,
continuously accumulating forgetting during the distillation process.
Rehearsal-free methods insufficiently learn the distribution of each domain,
leading to forgetfulness over time. To solve these issues, we propose a novel
Distribution-aware Forgetting Compensation (DAFC) model that explores
cross-domain shared representation learning and domain-specific distribution
integration without using old exemplars or knowledge distillation. We propose a
Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich
prompt elements and guide the prompt model to learn fine-grained
representations for each instance. This can enhance the differentiation of
identity information and establish the foundation for domain distribution
awareness. Then, Distribution-based Awareness and Integration (DAI) is designed
to capture each domain-specific distribution by a dedicated expert network and
adaptively consolidate them into a shared region in high-dimensional space. In
this manner, DAI can consolidate and enhance cross-domain shared representation
learning while alleviating catastrophic forgetting. Furthermore, we develop a
Knowledge Consolidation Mechanism (KCM) that comprises instance-level
discrimination and cross-domain consistency alignment strategies to facilitate
model adaptive learning of new knowledge from the current domain and promote
knowledge consolidation learning between acquired domain-specific
distributions, respectively. Experimental results show that our DAFC outperform
state-of-the-art methods by at least 9.8\%/6.6\% and 6.4\%/6.2\% of average
mAP/R@1 on two training orders.

</details>


### [192] [ScanEdit: Hierarchically-Guided Functional 3D Scan Editing](https://arxiv.org/abs/2504.15049)
*Mohamed el amine Boudjoghra,Ivan Laptev,Angela Dai*

Main category: cs.CV

TL;DR: ScanEdit是一种基于指令驱动的3D扫描编辑方法，通过层次化场景图和大型语言模型（LLM）实现高效编辑，并结合物理约束生成逼真场景。


<details>
  <summary>Details</summary>
Motivation: 随着3D捕获技术的快速发展，3D数据激增，高效编辑3D场景成为图形应用的关键需求。

Method: 提出层次化场景图表示，利用LLM将语言指令转化为可操作的命令，并结合物理约束生成场景。

Result: ScanEdit在实验中表现优异，优于现有技术，适用于多种真实场景和指令。

Conclusion: ScanEdit通过层次化编辑和LLM推理，实现了高效且逼真的3D场景编辑。

Abstract: With the fast pace of 3D capture technology and resulting abundance of 3D
data, effective 3D scene editing becomes essential for a variety of graphics
applications. In this work we present ScanEdit, an instruction-driven method
for functional editing of complex, real-world 3D scans. To model large and
interdependent sets of ob- jectswe propose a hierarchically-guided approach.
Given a 3D scan decomposed into its object instances, we first construct a
hierarchical scene graph representation to enable effective, tractable editing.
We then leverage reason- ing capabilities of Large Language Models (LLMs) and
translate high-level language instructions into actionable commands applied
hierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based
guidance with ex- plicit physical constraints and generates realistic scenes
where object arrangements obey both physics and common sense. In our extensive
experimental evaluation ScanEdit outperforms state of the art and demonstrates
excellent re- sults for a variety of real-world scenes and input instruc-
tions.

</details>


### [193] [Structure-guided Diffusion Transformer for Low-Light Image Enhancement](https://arxiv.org/abs/2504.15054)
*Xiangchen Yin,Zhenda Yu,Longtao Jiang,Xin Gao,Xiao Sun,Zhi Liu,Xun Yang*

Main category: cs.CV

TL;DR: 本文首次将扩散变换器（DiT）引入低光图像增强任务，提出了一种结构引导的扩散变换器框架（SDTL），通过小波变换和结构增强模块（SEM）提升模型效率与纹理增强效果，同时设计了结构引导注意力块（SAB）以减少噪声干扰。实验证明该方法在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前低光图像增强方法在恢复细节的同时会放大噪声，导致视觉质量不佳。本文旨在探索DiT在低光增强中的应用，并提出更高效的解决方案。

Method: 1. 使用小波变换压缩特征以提高推理效率；2. 提出结构增强模块（SEM）利用结构先验增强纹理；3. 设计结构引导注意力块（SAB）专注于纹理丰富的区域并减少噪声干扰。

Result: 在多个流行数据集上的定性和定量实验表明，SDTL方法达到了SOTA性能，验证了其有效性及DiT在低光增强中的潜力。

Conclusion: SDTL框架通过结合DiT与结构引导策略，显著提升了低光图像增强的质量，为DiT在该领域的应用开辟了新方向。

Abstract: While the diffusion transformer (DiT) has become a focal point of interest in
recent years, its application in low-light image enhancement remains a blank
area for exploration. Current methods recover the details from low-light images
while inevitably amplifying the noise in images, resulting in poor visual
quality. In this paper, we firstly introduce DiT into the low-light enhancement
task and design a novel Structure-guided Diffusion Transformer based Low-light
image enhancement (SDTL) framework. We compress the feature through wavelet
transform to improve the inference efficiency of the model and capture the
multi-directional frequency band. Then we propose a Structure Enhancement
Module (SEM) that uses structural prior to enhance the texture and leverages an
adaptive fusion strategy to achieve more accurate enhancement effect. In
Addition, we propose a Structure-guided Attention Block (SAB) to pay more
attention to texture-riched tokens and avoid interference from noisy areas in
noise prediction. Extensive qualitative and quantitative experiments
demonstrate that our method achieves SOTA performance on several popular
datasets, validating the effectiveness of SDTL in improving image quality and
the potential of DiT in low-light enhancement tasks.

</details>


### [194] [Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.15085)
*Wangyu Wu,Zhenhong Chen,Siqi Song,Xianglin Qiua,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: HAF-VT是一种跨域序列推荐方法，通过结合视觉和文本数据增强认知建模，利用分层注意力机制学习单域和跨域偏好，在四个电商数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 受人类认知过程启发，研究旨在通过多模态数据（视觉和文本）提升跨域用户兴趣建模能力。

Method: 使用冻结的CLIP模型生成图像和文本嵌入，结合分层注意力机制学习单域和跨域偏好。

Result: 在四个电商数据集上，HAF-VT优于现有方法，验证了多模态数据在序列决策中的重要性。

Conclusion: HAF-VT成功将认知原理与计算模型结合，突显了多模态数据在跨域序列推荐中的价值。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences through intra- and inter-sequence item
relationships. Inspired by human cognitive processes, we propose Hierarchical
Attention Fusion of Visual and Textual Representations (HAF-VT), a novel
approach integrating visual and textual data to enhance cognitive modeling.
Using the frozen CLIP model, we generate image and text embeddings, enriching
item representations with multimodal data. A hierarchical attention mechanism
jointly learns single-domain and cross-domain preferences, mimicking human
information integration. Evaluated on four e-commerce datasets, HAF-VT
outperforms existing methods in capturing cross-domain user interests, bridging
cognitive principles with computational models and highlighting the role of
multimodal data in sequential decision-making.

</details>


### [195] [VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation](https://arxiv.org/abs/2504.15095)
*Mingxia Zhan,Li Zhang,XiaoMeng Chu,Beibei Wang*

Main category: cs.CV

TL;DR: VistaDepth是一种新的单目深度估计框架，通过结合频域特征增强和自适应权重平衡机制，改进了扩散模型在远距离深度重建中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的单目深度估计方法在远距离深度重建上表现不佳，主要由于深度值分布不平衡和过度依赖空间域特征。

Method: 提出VistaDepth框架，包括Latent Frequency Modulation（LFM）模块动态优化频域特征，以及自适应权重策略实时调整扩散损失。

Result: 实验表明VistaDepth在扩散基MDE技术中达到最优性能，尤其在远距离区域重建上表现突出。

Conclusion: VistaDepth通过频域特征增强和自适应权重机制，显著提升了深度感知性能，特别是在远距离细节重建方面。

Abstract: Monocular depth estimation (MDE) aims to predict per-pixel depth values from
a single RGB image. Recent advancements have positioned diffusion models as
effective MDE tools by framing the challenge as a conditional image generation
task. Despite their progress, these methods often struggle with accurately
reconstructing distant depths, due largely to the imbalanced distribution of
depth values and an over-reliance on spatial-domain features. To overcome these
limitations, we introduce VistaDepth, a novel framework that integrates
adaptive frequency-domain feature enhancements with an adaptive
weight-balancing mechanism into the diffusion process. Central to our approach
is the Latent Frequency Modulation (LFM) module, which dynamically refines
spectral responses in the latent feature space, thereby improving the
preservation of structural details and reducing noisy artifacts. Furthermore,
we implement an adaptive weighting strategy that modulates the diffusion loss
in real-time, enhancing the model's sensitivity towards distant depth
reconstruction. These innovations collectively result in superior depth
perception performance across both distance and detail. Experimental
evaluations confirm that VistaDepth achieves state-of-the-art performance among
diffusion-based MDE techniques, particularly excelling in the accurate
reconstruction of distant regions.

</details>


### [196] [A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae](https://arxiv.org/abs/2504.15105)
*Yurun Wang,Zerong Qi,Shujun Fu,Mingzheng Hu*

Main category: cs.CV

TL;DR: 提出了一种名为TBSFNet的三分支空间融合网络，结合MLFGNet提升潜指纹增强效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在潜指纹增强中效果不足，尤其是低质量区域的恢复。

Method: 提出TBSFNet，针对不同区域采用定制策略增强，并结合方向场和细节模块，引入MLFGNet提升泛化能力。

Result: 在MOLF和MUST数据集上，MLFGNet表现优于现有增强算法。

Conclusion: TBSFNet和MLFGNet有效提升了潜指纹增强效果，尤其适用于低质量区域。

Abstract: Latent fingerprint enhancement is a critical step in the process of latent
fingerprint identification. Existing deep learning-based enhancement methods
still fall short of practical application requirements, particularly in
restoring low-quality fingerprint regions. Recognizing that different regions
of latent fingerprints require distinct enhancement strategies, we propose a
Triple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances
different regions of the image using tailored strategies. Furthermore, to
improve the generalization capability of the network, we integrate orientation
field and minutiae-related modules into TBSFNet and introduce a Multi-Level
Feature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST
datasets demonstrate that MLFGNet outperforms existing enhancement algorithms.

</details>


### [197] [Unwarping Screen Content Images via Structure-texture Enhancement Network and Transformation Self-estimation](https://arxiv.org/abs/2504.15108)
*Zhenzhen Xiao,Heng Liu,Bingwen Hu*

Main category: cs.CV

TL;DR: 提出了一种结构-纹理增强网络（STEN），用于处理屏幕内容图像（SCI）的扭曲问题，结合B样条隐式神经表示和变换自估计算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经网络方法在自然图像上表现良好，但在处理包含大几何扭曲、文本、符号和锐利边缘的SCI时效果不佳。

Method: STEN包含结构估计分支（SEB）和纹理估计分支（TEB），分别增强局部聚合和全局依赖建模，以及纹理细节合成。变换自估计模块自主估计误差并修正坐标变换矩阵。

Result: 在公开SCI数据集上的实验表明，该方法显著优于现有技术，且在自然图像数据集上也显示出潜力。

Conclusion: STEN有效解决了SCI的扭曲问题，并在自然图像上展示了扩展应用的潜力。

Abstract: While existing implicit neural network-based image unwarping methods perform
well on natural images, they struggle to handle screen content images (SCIs),
which often contain large geometric distortions, text, symbols, and sharp
edges. To address this, we propose a structure-texture enhancement network
(STEN) with transformation self-estimation for SCI warping. STEN integrates a
B-spline implicit neural representation module and a transformation error
estimation and self-correction algorithm. It comprises two branches: the
structure estimation branch (SEB), which enhances local aggregation and global
dependency modeling, and the texture estimation branch (TEB), which improves
texture detail synthesis using B-spline implicit neural representation.
Additionally, the transformation self-estimation module autonomously estimates
the transformation error and corrects the coordinate transformation matrix,
effectively handling real-world image distortions. Extensive experiments on
public SCI datasets demonstrate that our approach significantly outperforms
state-of-the-art methods. Comparisons on well-known natural image datasets also
show the potential of our approach for natural image distortion.

</details>


### [198] [Improving Sound Source Localization with Joint Slot Attention on Image and Audio](https://arxiv.org/abs/2504.15118)
*Inho Kim,Youngkil Song,Jicheol Park,Won Hwa Kim,Suha Kwak*

Main category: cs.CV

TL;DR: 论文提出了一种基于联合槽注意力（joint slot attention）的新方法，用于解决声音源定位（SSL）任务中图像和音频嵌入的噪声问题，并通过跨模态注意力匹配进一步对齐特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有SSL方法通过对比学习将图像和音频表示为单一嵌入向量，但由于噪声和背景干扰，这种方法效果不佳。

Method: 使用联合槽注意力将图像和音频特征分解为目标和非目标表示，仅用目标表示进行对比学习，并引入跨模态注意力匹配对齐局部特征。

Result: 在三个公开SSL基准测试中几乎全面领先，跨模态检索性能大幅超越之前的工作。

Conclusion: 提出的方法通过联合槽注意力和跨模态匹配，显著提升了声音源定位的性能。

Abstract: Sound source localization (SSL) is the task of locating the source of sound
within an image. Due to the lack of localization labels, the de facto standard
in SSL has been to represent an image and audio as a single embedding vector
each, and use them to learn SSL via contrastive learning. To this end, previous
work samples one of local image features as the image embedding and aggregates
all local audio features to obtain the audio embedding, which is far from
optimal due to the presence of noise and background irrelevant to the actual
target in the input. We present a novel SSL method that addresses this chronic
issue by joint slot attention on image and audio. To be specific, two slots
competitively attend image and audio features to decompose them into target and
off-target representations, and only target representations of image and audio
are used for contrastive learning. Also, we introduce cross-modal attention
matching to further align local features of image and audio. Our method
achieved the best in almost all settings on three public benchmarks for SSL,
and substantially outperformed all the prior work in cross-modal retrieval.

</details>


### [199] [Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations](https://arxiv.org/abs/2504.15121)
*Csongor Csanad Kariko,Muhammad Rafi Faisal,Levente Hajder*

Main category: cs.CV

TL;DR: 提出了一种基于校正立体图像对的表面法线估计新方法，通过从视差值中提取仿射变换实现快速准确的结果。


<details>
  <summary>Details</summary>
Motivation: 校正立体图像对可以简化表面法线估计过程，降低计算复杂度。

Method: 结合仿射变换、自定义卷积操作算法和自适应启发式技术，构建快速准确的表面法线估计器。

Result: 在Middlebury和Cityscapes数据集上验证了方法的实时性和准确性，GPU实现表现优异。

Conclusion: 方法高效且准确，代码将公开以促进研究复现。

Abstract: This work introduces a novel method for surface normal estimation from
rectified stereo image pairs, leveraging affine transformations derived from
disparity values to achieve fast and accurate results. We demonstrate how the
rectification of stereo image pairs simplifies the process of surface normal
estimation by reducing computational complexity. To address noise reduction, we
develop a custom algorithm inspired by convolutional operations, tailored to
process disparity data efficiently. We also introduce adaptive heuristic
techniques for efficiently detecting connected surface components within the
images, further improving the robustness of the method. By integrating these
methods, we construct a surface normal estimator that is both fast and
accurate, producing a dense, oriented point cloud as the final output. Our
method is validated using both simulated environments and real-world stereo
images from the Middlebury and Cityscapes datasets, demonstrating significant
improvements in real-time performance and accuracy when implemented on a GPU.
Upon acceptance, the shader source code will be made publicly available to
facilitate further research and reproducibility.

</details>


### [200] [MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video](https://arxiv.org/abs/2504.15122)
*Minh-Quan Viet Bui,Jongmin Park,Juan Luis Gonzalez Bello,Jaeho Moon,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: MoBGS是一种新的去模糊动态3D高斯泼溅框架，能从模糊单目视频中重建清晰的高质量时空视图。


<details>
  <summary>Details</summary>
Motivation: 现有动态新视角合成方法对运动模糊敏感，导致渲染质量下降。MoBGS旨在解决这一问题，专注于动态对象的运动建模。

Method: 提出BLCE方法用于潜在相机轨迹估计，以及LCEE方法确保全局相机和局部对象运动的一致性去模糊。

Result: 在Stereo Blur数据集和真实模糊视频上，MoBGS显著优于现有方法（DyBluRF和Deblur4DGS），达到动态NVS在运动模糊下的最佳性能。

Conclusion: MoBGS通过创新的去模糊方法和运动建模，实现了动态场景的高质量重建，具有显著优势。

Abstract: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)
framework capable of reconstructing sharp and high-quality novel
spatio-temporal views from blurry monocular videos in an end-to-end manner.
Existing dynamic novel view synthesis (NVS) methods are highly sensitive to
motion blur in casually captured videos, resulting in significant degradation
of rendering quality. While recent approaches address motion-blurred inputs for
NVS, they primarily focus on static scene reconstruction and lack dedicated
motion modeling for dynamic objects. To overcome these limitations, our MoBGS
introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for
effective latent camera trajectory estimation, improving global camera motion
deblurring. In addition, we propose a physically-inspired Latent Camera-induced
Exposure Estimation (LCEE) method to ensure consistent deblurring of both
global camera and local object motion. Our MoBGS framework ensures the temporal
consistency of unseen latent timestamps and robust motion decomposition of
static and dynamic regions. Extensive experiments on the Stereo Blur dataset
and real-world blurry videos show that our MoBGS significantly outperforms the
very recent advanced methods (DyBluRF and Deblur4DGS), achieving
state-of-the-art performance for dynamic NVS under motion blur.

</details>


### [201] [Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation](https://arxiv.org/abs/2504.15134)
*Xiao Zhang,Lu Zou,Tao Lu,Yuan Yao,Zhangjin Huang,Guoping Wang*

Main category: cs.CV

TL;DR: INKL-Pose是一个新颖的类别级物体姿态估计框架，通过实例自适应关键点学习和局部到全局几何聚合，解决了复杂几何和形状偏差的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂几何或显著偏离标准形状的物体实例时表现不佳，需要更强的泛化能力。

Method: INKL-Pose通过实例自适应关键点生成器预测关键点，并结合局部和全局特征聚合器进行优化，同时引入特征序列翻转策略和两种损失函数。

Result: 在CAMERA25、REAL275和HouseCat6D数据集上，INKL-Pose实现了最先进的性能，显著优于现有方法。

Conclusion: INKL-Pose通过创新的关键点学习和几何聚合方法，有效提升了类别级物体姿态估计的精度和泛化能力。

Abstract: Category-level object pose estimation aims to predict the 6D pose and size of
previously unseen instances from predefined categories, requiring strong
generalization across diverse object instances. Although many previous methods
attempt to mitigate intra-class variations, they often struggle with instances
exhibiting complex geometries or significant deviations from canonical shapes.
To address this challenge, we propose INKL-Pose, a novel category-level object
pose estimation framework that enables INstance-adaptive Keypoint Learning with
local-to-global geometric aggregation. Specifically, our approach first
predicts semantically consistent and geometric informative keypoints through an
Instance-Adaptive Keypoint Generator, then refines them with: (1) a Local
Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global
Keypoint Feature Aggregator using bidirectional Mamba for structural
consistency. To enable bidirectional modeling in Mamba, we introduce a Feature
Sequence Flipping strategy that preserves spatial coherence while constructing
backward feature sequences. Additionally, we design a surface loss and a
separation loss to enforce uniform coverage and spatial diversity in keypoint
distribution. The generated keypoints are finally mapped to a canonical space
for regressing the object's 6D pose and size. Extensive experiments on
CAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves
state-of-the-art performance and significantly outperforms existing methods.

</details>


### [202] ["I Know It When I See It": Mood Spaces for Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2504.15145)
*Huzheng Yang,Katherine Xu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TL;DR: 论文提出了一种Mood Board方法，通过示例表达抽象概念，并构建Mood Space来压缩特征，实现高效的图像操作。


<details>
  <summary>Details</summary>
Motivation: 许多抽象概念难以量化但易于识别，传统方法难以处理。

Method: 使用Mood Board和Mood Space，通过纤维化计算压缩特征，学习图像间的亲和关系，并在特征空间中进行操作。

Result: Mood Space紧凑且线性，支持图像级操作（如平均、类比、姿态转移），计算高效，仅需少量示例。

Conclusion: 该方法高效且灵活，适用于抽象概念的视觉表达和操作。

Abstract: Expressing complex concepts is easy when they can be labeled or quantified,
but many ideas are hard to define yet instantly recognizable. We propose a Mood
Board, where users convey abstract concepts with examples that hint at the
intended direction of attribute changes. We compute an underlying Mood Space
that 1) factors out irrelevant features and 2) finds the connections between
images, thus bringing relevant concepts closer. We invent a fibration
computation to compress/decompress pre-trained features into/from a compact
space, 50-100x smaller. The main innovation is learning to mimic the pairwise
affinity relationship of the image tokens across exemplars. To focus on the
coarse-to-fine hierarchical structures in the Mood Space, we compute the top
eigenvector structure from the affinity matrix and define a loss in the
eigenvector space. The resulting Mood Space is locally linear and compact,
allowing image-level operations, such as object averaging, visual analogy, and
pose transfer, to be performed as a simple vector operation in Mood Space. Our
learning is efficient in computation without any fine-tuning, needs only a few
(2-20) exemplars, and takes less than a minute to learn.

</details>


### [203] [Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection](https://arxiv.org/abs/2504.15152)
*Jun Zhou,Bingchen Gao,Kai Wang,Jialun Pei,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种基于自监督学习的无标记术前-术中肝脏配准框架，解决了传统方法依赖解剖标记和形状变形建模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有配准方法依赖解剖标记，存在标记定义模糊和术中视觉信息整合不足的局限性。

Method: 将传统3D-2D配准流程转为3D-3D，分为刚性和非刚性配准子任务，使用特征解耦变换器和结构正则化变形网络。

Result: 在合成和真实数据集上的实验及用户研究验证了方法的优越性和临床潜力。

Conclusion: 提出的框架显著提升了肝脏配准的准确性和效率，具有临床应用前景。

Abstract: Liver registration by overlaying preoperative 3D models onto intraoperative
2D frames can assist surgeons in perceiving the spatial anatomy of the liver
clearly for a higher surgical success rate. Existing registration methods rely
heavily on anatomical landmark-based workflows, which encounter two major
limitations: 1) ambiguous landmark definitions fail to provide efficient
markers for registration; 2) insufficient integration of intraoperative liver
visual information in shape deformation modeling. To address these challenges,
in this paper, we propose a landmark-free preoperative-to-intraoperative
registration framework utilizing effective self-supervised learning, termed
\ourmodel. This framework transforms the conventional 3D-2D workflow into a
3D-3D registration pipeline, which is then decoupled into rigid and non-rigid
registration subtasks. \ourmodel~first introduces a feature-disentangled
transformer to learn robust correspondences for recovering rigid
transformations. Further, a structure-regularized deformation network is
designed to adjust the preoperative model to align with the intraoperative
liver surface. This network captures structural correlations through geometry
similarity modeling in a low-rank transformer network. To facilitate the
validation of the registration performance, we also construct an in-vivo
registration dataset containing liver resection videos of 21 patients, called
\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the
liver together with liver mask annotations and calibrated camera intrinsic
parameters. Extensive experiments and user studies on both synthetic and
in-vivo datasets demonstrate the superiority and potential clinical
applicability of our method.

</details>


### [204] [Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15155)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 本文提出KANet，基于改进的3D-DenseNet模型，通过3D KAN Conv和自适应网格更新机制，解决高光谱图像分类中的高维数据、稀疏分布和光谱冗余问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类中，深度神经网络面临高维数据、稀疏分布和光谱冗余等挑战，导致过拟合和泛化能力受限。

Method: 引入可学习的单变量B样条函数，将三维邻域展平为向量，并用B样条参数化的非线性激活函数替代传统3D卷积核的固定线性权重；同时通过动态网格调整机制优化B样条网格点位置。

Result: KANet在IN、UP和KSC数据集上表现优异，优于主流高光谱图像分类方法，显著提高了高维数据建模的精度和参数效率。

Conclusion: KANet通过3D动态专家卷积系统提升模型表示能力，无需增加网络深度或宽度，有效缓解维度灾难和过拟合风险。

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more efficiently adapt to
ground object distributions while extracting image features without introducing
excessive parameters and skipping redundant information, this paper proposes
KANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an
adaptive grid update mechanism. By introducing learnable univariate B-spline
functions on network edges, specifically by flattening three-dimensional
neighborhoods into vectors and applying B-spline-parameterized nonlinear
activation functions to replace the fixed linear weights of traditional 3D
convolutional kernels, we precisely capture complex spectral-spatial nonlinear
relationships in hyperspectral data. Simultaneously, through a dynamic grid
adjustment mechanism, we adaptively update the grid point positions of
B-splines based on the statistical characteristics of input data, optimizing
the resolution of spline functions to match the non-uniform distribution of
spectral features, significantly improving the model's accuracy in
high-dimensional data modeling and parameter efficiency, effectively
alleviating the curse of dimensionality. This characteristic demonstrates
superior neural scaling laws compared to traditional convolutional neural
networks and reduces overfitting risks in small-sample and high-noise
scenarios. KANet enhances model representation capability through a 3D dynamic
expert convolution system without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [205] [Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration](https://arxiv.org/abs/2504.15159)
*Junyuan Deng,Xinyi Wu,Yongxing Yang,Congchao Zhu,Song Wang,Zhenyao Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FluxGen的数据生成流程和轻量级适配器FluxIR，利用预训练的T2I模型（如Flux）生成高质量训练样本，以低成本解决图像修复任务。


<details>
  <summary>Details</summary>
Motivation: 预训练的T2I模型虽然强大，但直接用于图像修复需要大量高质量数据和计算资源，成本高且隐私不友好。论文旨在通过生成无限训练样本来解决这一问题。

Method: 设计了FluxGen流程，包括无条件图像生成、图像选择和退化图像模拟，并开发了轻量级适配器FluxIR来控制基于DiT的T2I模型。

Result: 实验表明，该方法能以约8.5%的训练成本，在合成和真实退化数据集上实现优异的修复效果和视觉质量。

Conclusion: FluxGen和FluxIR的组合为图像修复提供了一种高效、低成本的解决方案，显著降低了资源需求。

Abstract: Recently, pre-trained text-to-image (T2I) models have been extensively
adopted for real-world image restoration because of their powerful generative
prior. However, controlling these large models for image restoration usually
requires a large number of high-quality images and immense computational
resources for training, which is costly and not privacy-friendly. In this
paper, we find that the well-trained large T2I model (i.e., Flux) is able to
produce a variety of high-quality images aligned with real-world distributions,
offering an unlimited supply of training samples to mitigate the above issue.
Specifically, we proposed a training data construction pipeline for image
restoration, namely FluxGen, which includes unconditional image generation,
image selection, and degraded image simulation. A novel light-weighted adapter
(FluxIR) with squeeze-and-excitation layers is also carefully designed to
control the large Diffusion Transformer (DiT)-based T2I model so that
reasonable details can be restored. Experiments demonstrate that our proposed
method enables the Flux model to adapt effectively to real-world image
restoration tasks, achieving superior scores and visual quality on both
synthetic and real-world degradation datasets - at only about 8.5\% of the
training cost compared to current approaches.

</details>


### [206] [An Efficient Aerial Image Detection with Variable Receptive Fields](https://arxiv.org/abs/2504.15165)
*Liu Wenbin*

Main category: cs.CV

TL;DR: VRF-DETR是一种基于Transformer的检测器，通过动态调整感受野和高效架构设计，解决了无人机目标检测中的小目标、遮挡和计算限制问题。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临小目标（<10px）、密集遮挡和计算资源限制的挑战，现有检测器难以平衡精度与效率。

Method: 提出VRF-DETR，包含多尺度上下文融合模块（MSCF）、门控卷积层（GConv）和门控多尺度融合瓶颈（GMCF），通过动态调整感受野和高效架构设计提升性能。

Result: 在VisDrone2019数据集上达到51.4% mAP50和31.8% mAP50:95，仅需13.5M参数。

Conclusion: VRF-DETR为无人机目标检测任务建立了新的效率-精度平衡基准。

Abstract: Aerial object detection using unmanned aerial vehicles (UAVs) faces critical
challenges including sub-10px targets, dense occlusions, and stringent
computational constraints. Existing detectors struggle to balance accuracy and
efficiency due to rigid receptive fields and redundant architectures. To
address these limitations, we propose Variable Receptive Field DETR (VRF-DETR),
a transformer-based detector incorporating three key components: 1) Multi-Scale
Context Fusion (MSCF) module that dynamically recalibrates features through
adaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution
(GConv) layer enabling parameter-efficient local-context modeling via depthwise
separable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)
Bottleneck that hierarchically disentangles occluded objects through cascaded
global-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR
achieves 51.4\% mAP\textsubscript{50} and 31.8\% mAP\textsubscript{50:95} with
only 13.5M parameters. This work establishes a new efficiency-accuracy Pareto
frontier for UAV-based detection tasks.

</details>


### [207] [HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change Detection](https://arxiv.org/abs/2504.15170)
*Chengxi Han,Xiaoyu Su,Zhiqiang Wei,Meiqi Hu,Yichu Xu*

Main category: cs.CV

TL;DR: HSANet提出了一种用于遥感图像变化检测的网络，通过分层卷积和多尺度特征提取，结合自注意力与跨注意力机制，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像变化检测是大规模监测的重要方法，但现有方法在全局上下文和多尺度特征融合方面存在不足。

Method: HSANet采用分层卷积提取多尺度特征，结合混合自注意力和跨注意力机制，学习并融合全局及跨尺度信息。

Result: HSANet能够捕捉不同尺度的全局上下文，整合跨尺度特征，优化边缘细节，显著提升检测性能。

Conclusion: HSANet通过多尺度特征和注意力机制的结合，有效提升了遥感图像变化检测的精度和鲁棒性，代码已开源。

Abstract: The remote sensing image change detection task is an essential method for
large-scale monitoring. We propose HSANet, a network that uses hierarchical
convolution to extract multi-scale features. It incorporates hybrid
self-attention and cross-attention mechanisms to learn and fuse global and
cross-scale information. This enables HSANet to capture global context at
different scales and integrate cross-scale features, refining edge details and
improving detection performance. We will also open-source our model code:
https://github.com/ChengxiHAN/HSANet.

</details>


### [208] [DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution](https://arxiv.org/abs/2504.15176)
*Miaomiao Cai,Simiao Li,Wei Li,Xudong Huang,Hanting Chen,Jie Hu,Yunhe Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为DSPO的方法，将人类偏好对齐引入Real-ISR任务，解决了现有方法缺乏人类反馈的问题，并通过语义指导优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有Real-ISR方法缺乏人类反馈集成，可能导致生成结果与人类偏好不一致，甚至产生伪影和有害内容。

Method: 提出Direct Semantic Preference Optimization (DSPO)，结合语义实例对齐策略和用户描述反馈策略，实现实例级人类偏好对齐。

Result: DSPO在一步和多步超分辨率框架中均表现出高效性，显著提升了生成质量。

Conclusion: DSPO作为一种即插即用的解决方案，有效解决了Real-ISR中人类偏好对齐的难题，并提升了生成结果的感知一致性。

Abstract: Recent advances in diffusion models have improved Real-World Image
Super-Resolution (Real-ISR), but existing methods lack human feedback
integration, risking misalignment with human preference and may leading to
artifacts, hallucinations and harmful content generation. To this end, we are
the first to introduce human preference alignment into Real-ISR, a technique
that has been successfully applied in Large Language Models and Text-to-Image
tasks to effectively enhance the alignment of generated outputs with human
preferences. Specifically, we introduce Direct Preference Optimization (DPO)
into Real-ISR to achieve alignment, where DPO serves as a general alignment
technique that directly learns from the human preference dataset. Nevertheless,
unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR
are difficult to reconcile with the image-level preferences of DPO, which can
lead to the DPO being overly sensitive to local anomalies, leading to reduced
generation quality. To resolve this dichotomy, we propose Direct Semantic
Preference Optimization (DSPO) to align instance-level human preferences by
incorporating semantic guidance, which is through two strategies: (a) semantic
instance alignment strategy, implementing instance-level alignment to ensure
fine-grained perceptual consistency, and (b) user description feedback
strategy, mitigating hallucinations through semantic textual feedback on
instance-level images. As a plug-and-play solution, DSPO proves highly
effective in both one-step and multi-step SR frameworks.

</details>


### [209] [FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image](https://arxiv.org/abs/2504.15179)
*Fei Yin,Mallikarjun B R,Chun-Han Yao,Rafał Mantiuk,Varun Jampani*

Main category: cs.CV

TL;DR: 提出了一种从单张图像生成高质量、可动画4D头像的新框架，解决了现有方法需要多视图数据或形状和身份一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在4D头像生成中要么需要大量多视图数据，要么在形状准确性和身份一致性上表现不佳。本文旨在解决这些限制。

Method: 结合形状、图像和视频先验，通过3D-GAN反演获取初始形状，利用深度引导的变形信号增强多视图纹理，并引入视频先验处理表情动画。采用一致性-非一致性训练处理4D重建中的数据不一致问题。

Result: 实验表明，该方法在质量上优于现有技术，且在不同视角和表情下保持一致性。

Conclusion: 提出的框架在生成高质量、可动画4D头像方面表现出色，解决了现有方法的局限性。

Abstract: We present a novel framework for generating high-quality, animatable 4D
avatar from a single image. While recent advances have shown promising results
in 4D avatar creation, existing methods either require extensive multiview data
or struggle with shape accuracy and identity consistency. To address these
limitations, we propose a comprehensive system that leverages shape, image, and
video priors to create full-view, animatable avatars. Our approach first
obtains initial coarse shape through 3D-GAN inversion. Then, it enhances
multiview textures using depth-guided warping signals for cross-view
consistency with the help of the image diffusion model. To handle expression
animation, we incorporate a video prior with synchronized driving signals
across viewpoints. We further introduce a Consistent-Inconsistent training to
effectively handle data inconsistencies during 4D reconstruction. Experimental
results demonstrate that our method achieves superior quality compared to the
prior art, while maintaining consistency across different viewpoints and
expressions.

</details>


### [210] [Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform](https://arxiv.org/abs/2504.15182)
*Xianpan Zhou*

Main category: cs.CV

TL;DR: Tiger200K是一个高质量的视频数据集，通过人工筛选UGC平台内容，解决了现有开源数据集质量不足的问题，支持视频生成模型的优化。


<details>
  <summary>Details</summary>
Motivation: 现有开源视频数据集质量不足，无法满足高级视频生成模型的微调需求。

Method: 通过人工筛选UGC平台视频，结合边界检测、OCR、运动过滤等技术，构建高质量视频-文本对数据集。

Result: Tiger200K提供了高质量、时间一致的视频-文本对，支持视频生成模型的优化。

Conclusion: Tiger200K强调了人工数据筛选的重要性，并将作为开源项目持续扩展，推动视频生成模型的研究和应用。

Abstract: The recent surge in open-source text-to-video generation models has
significantly energized the research community, yet their dependence on
proprietary training datasets remains a key constraint. While existing open
datasets like Koala-36M employ algorithmic filtering of web-scraped videos from
early platforms, they still lack the quality required for fine-tuning advanced
video generation models. We present Tiger200K, a manually curated high visual
quality video dataset sourced from User-Generated Content (UGC) platforms. By
prioritizing visual fidelity and aesthetic quality, Tiger200K underscores the
critical role of human expertise in data curation, and providing high-quality,
temporally consistent video-text pairs for fine-tuning and optimizing video
generation architectures through a simple but effective pipeline including shot
boundary detection, OCR, border detecting, motion filter and fine bilingual
caption. The dataset will undergo ongoing expansion and be released as an
open-source initiative to advance research and applications in video generative
models. Project page: https://tinytigerpan.github.io/tiger200k/

</details>


### [211] [Breast density in MRI: an AI-based quantification and relationship to assessment in mammography](https://arxiv.org/abs/2504.15192)
*Yaqian Chen,Lin Li,Hanxue Gu,Haoyu Dong,Derek L. Nguyen,Allan D. Kirk,Maciej A. Mazurowski,E. Shelley Hwang*

Main category: cs.CV

TL;DR: 该论文研究了通过MRI评估乳腺密度的机器学习方法，发现其与乳腺X线密度相关，但MRI能捕捉到一些独特成分。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度是乳腺癌的重要风险因素，MRI作为一种补充工具，能提供更全面的乳腺组织评估，但其3D特性带来了分析挑战。

Method: 使用内部开发的机器学习算法，在三个MRI数据集中评估正常乳腺的密度。

Result: 乳腺密度在不同数据集中表现一致（0.104-0.114），且随年龄下降的趋势与先前研究一致。MRI密度与乳腺X线密度相关，但存在差异。

Conclusion: 未来研究将探索如何整合MRI乳腺密度与现有工具，以改进乳腺癌风险预测。

Abstract: Mammographic breast density is a well-established risk factor for breast
cancer. Recently there has been interest in breast MRI as an adjunct to
mammography, as this modality provides an orthogonal and highly quantitative
assessment of breast tissue. However, its 3D nature poses analytic challenges
related to delineating and aggregating complex structures across slices. Here,
we applied an in-house machine-learning algorithm to assess breast density on
normal breasts in three MRI datasets. Breast density was consistent across
different datasets (0.104 - 0.114). Analysis across different age groups also
demonstrated strong consistency across datasets and confirmed a trend of
decreasing density with age as reported in previous studies. MR breast density
was correlated with mammographic breast density, although some notable
differences suggest that certain breast density components are captured only on
MRI. Future work will determine how to integrate MR breast density with current
tools to improve future breast cancer risk prediction.

</details>


### [212] [Automated Measurement of Eczema Severity with Self-Supervised Learning](https://arxiv.org/abs/2504.15193)
*Neelesh Kumar,Oya Aran*

Main category: cs.CV

TL;DR: 提出一种自监督学习框架，用于在有限训练数据下自动诊断湿疹，通过SegGPT分割和DINO特征提取，性能优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要大量标注数据，但在湿疹诊断中标注数据难以获取，因此提出自监督学习框架以解决数据稀缺问题。

Method: 框架分为两阶段：1) 使用SegGPT进行少样本分割；2) 提取DINO特征并通过MLP进行湿疹严重程度的4分类。

Result: 在真实湿疹图像数据集上，方法表现优于微调Resnet-18和Vision Transformer（加权F1：0.67 vs. 0.44和0.40）。

Conclusion: 自监督学习是解决皮肤诊断中数据稀缺问题的可行方案。

Abstract: Automated diagnosis of eczema using images acquired from digital camera can
enable individuals to self-monitor their recovery. The process entails first
segmenting out the eczema region from the image and then measuring the severity
of eczema in the segmented region. The state-of-the-art methods for automated
eczema diagnosis rely on deep neural networks such as convolutional neural
network (CNN) and have shown impressive performance in accurately measuring the
severity of eczema. However, these methods require massive volume of annotated
data to train which can be hard to obtain. In this paper, we propose a
self-supervised learning framework for automated eczema diagnosis under limited
training data regime. Our framework consists of two stages: i) Segmentation,
where we use an in-context learning based algorithm called SegGPT for few-shot
segmentation of eczema region from the image; ii) Feature extraction and
classification, where we extract DINO features from the segmented regions and
feed it to a multi-layered perceptron (MLP) for 4-class classification of
eczema severity. When evaluated on a dataset of annotated "in-the-wild" eczema
images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the
state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted
F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our
results show that self-supervised learning can be a viable solution for
automated skin diagnosis where labeled data is scarce.

</details>


### [213] [Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning](https://arxiv.org/abs/2504.15199)
*Yassir Benhammou,Alessandro Tiberio,Gabriel Trautmann,Suman Kalyan*

Main category: cs.CV

TL;DR: MILS框架声称无需训练即可实现多模态任务，但其迭代优化过程带来高昂计算成本，而BLIP-2和GPT-4V等单次推理模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 揭示MILS框架在零样本任务中的隐藏计算成本，挑战其无需训练即可高效完成任务的宣称。

Method: 通过对比MILS与BLIP-2、GPT-4V等模型的性能与计算成本，量化其迭代过程的资源消耗。

Result: MILS的迭代优化虽提升性能，但计算成本显著高于单次推理模型。

Conclusion: MILS的高计算成本可能削弱其实际优势，提示多模态模型设计需平衡性能与效率。

Abstract: MILS (Multimodal Iterative LLM Solver) is a recently published framework that
claims "LLMs can see and hear without any training" by leveraging an iterative,
LLM-CLIP based approach for zero-shot image captioning. While this MILS
approach demonstrates good performance, our investigation reveals that this
success comes at a hidden, substantial computational cost due to its expensive
multi-step refinement process. In contrast, alternative models such as BLIP-2
and GPT-4V achieve competitive results through a streamlined, single-pass
approach. We hypothesize that the significant overhead inherent in MILS's
iterative process may undermine its practical benefits, thereby challenging the
narrative that zero-shot performance can be attained without incurring heavy
resource demands. This work is the first to expose and quantify the trade-offs
between output quality and computational cost in MILS, providing critical
insights for the design of more efficient multimodal models.

</details>


### [214] [Shape-Guided Clothing Warping for Virtual Try-On](https://arxiv.org/abs/2504.15232)
*Xiaoyu Han,Shunyuan Zheng,Zonglin Li,Chenyang Wang,Xin Sun,Quanling Meng*

Main category: cs.CV

TL;DR: 提出了一种名为SCW-VTON的形状引导服装变形方法，通过全局形状约束和肢体纹理增强虚拟试穿的逼真度和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在服装变形时缺乏对细节的精确控制，导致服装与身体形状不一致以及肢体区域变形。

Method: 设计了双路径服装变形模块（形状路径和流路径）和基于掩膜图像建模的肢体重建网络。

Result: 实验表明，SCW-VTON在服装形状一致性和细节控制上优于现有方法。

Conclusion: SCW-VTON能够生成更逼真且一致的虚拟试穿结果。

Abstract: Image-based virtual try-on aims to seamlessly fit in-shop clothing to a
person image while maintaining pose consistency. Existing methods commonly
employ the thin plate spline (TPS) transformation or appearance flow to deform
in-shop clothing for aligning with the person's body. Despite their promising
performance, these methods often lack precise control over fine details,
leading to inconsistencies in shape between clothing and the person's body as
well as distortions in exposed limb regions. To tackle these challenges, we
propose a novel shape-guided clothing warping method for virtual try-on, dubbed
SCW-VTON, which incorporates global shape constraints and additional limb
textures to enhance the realism and consistency of the warped clothing and
try-on results. To integrate global shape constraints for clothing warping, we
devise a dual-path clothing warping module comprising a shape path and a flow
path. The former path captures the clothing shape aligned with the person's
body, while the latter path leverages the mapping between the pre- and
post-deformation of the clothing shape to guide the estimation of appearance
flow. Furthermore, to alleviate distortions in limb regions of try-on results,
we integrate detailed limb guidance by developing a limb reconstruction network
based on masked image modeling. Through the utilization of SCW-VTON, we are
able to generate try-on results with enhanced clothing shape consistency and
precise control over details. Extensive experiments demonstrate the superiority
of our approach over state-of-the-art methods both qualitatively and
quantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON.

</details>


### [215] [Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation](https://arxiv.org/abs/2504.15259)
*Yunxuan Cai,Sitao Xiang,Zongjian Li,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种基于生成网络的语义可控人脸建模方法，通过扩散模型生成高质量3D人脸数据，并开发了GAN生成器和交互工具，增强了多样性和控制能力。


<details>
  <summary>Details</summary>
Motivation: 传统人脸建模受限于设备、人工和演员条件，导致多样性和控制能力不足。

Method: 使用预训练扩散模型生成3D人脸数据，开发GAN生成器支持语义输入和编辑，并构建交互工具。

Result: 生成了44,000个高质量人脸模型，开发了高效生成器和交互工具。

Conclusion: 提出的系统显著提升了人脸建模的多样性和控制能力，并计划公开交互工具。

Abstract: Digital modeling and reconstruction of human faces serve various
applications. However, its availability is often hindered by the requirements
of data capturing devices, manual labor, and suitable actors. This situation
restricts the diversity, expressiveness, and control over the resulting models.
This work aims to demonstrate that a semantically controllable generative
network can provide enhanced control over the digital face modeling process. To
enhance diversity beyond the limited human faces scanned in a controlled
setting, we introduce a novel data generation pipeline that creates a
high-quality 3D face database using a pre-trained diffusion model. Our proposed
normalization module converts synthesized data from the diffusion model into
high-quality scanned data. Using the 44,000 face models we obtained, we further
developed an efficient GAN-based generator. This generator accepts semantic
attributes as input, and generates geometry and albedo. It also allows
continuous post-editing of attributes in the latent space. Our asset refinement
component subsequently creates physically-based facial assets. We introduce a
comprehensive system designed for creating and editing high-quality face
assets. Our proposed model has undergone extensive experiment, comparison and
evaluation. We also integrate everything into a web-based interactive tool. We
aim to make this tool publicly available with the release of the paper.

</details>


### [216] [Diffusion Bridge Models for 3D Medical Image Translation](https://arxiv.org/abs/2504.15267)
*Shaorong Zhang,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Jose-Luis Ambite,Paul M. Thompson,Greg Ver Steeg*

Main category: cs.CV

TL;DR: 提出了一种扩散桥模型，用于在T1w MRI和DTI模态之间进行3D脑图像翻译，以减少DTI采集时间并支持跨模态数据增强。


<details>
  <summary>Details</summary>
Motivation: DTI成像耗时且不如T1w MRI易获取，需要一种方法生成高质量的DTI图像以减少实际采集需求。

Method: 使用扩散桥模型学习从T1w图像生成DTI FA图像，反之亦然，并通过多种指标评估生成质量。

Result: 模型在捕捉解剖结构和保留白质完整性信息方面表现优异，生成图像在分类任务中与真实数据性能相当。

Conclusion: 扩散桥模型为神经影像数据集改进和临床决策提供了有前景的解决方案，可能对研究和临床实践产生重要影响。

Abstract: Diffusion tensor imaging (DTI) provides crucial insights into the
microstructure of the human brain, but it can be time-consuming to acquire
compared to more readily available T1-weighted (T1w) magnetic resonance imaging
(MRI). To address this challenge, we propose a diffusion bridge model for 3D
brain image translation between T1w MRI and DTI modalities. Our model learns to
generate high-quality DTI fractional anisotropy (FA) images from T1w images and
vice versa, enabling cross-modality data augmentation and reducing the need for
extensive DTI acquisition. We evaluate our approach using perceptual
similarity, pixel-level agreement, and distributional consistency metrics,
demonstrating strong performance in capturing anatomical structures and
preserving information on white matter integrity. The practical utility of the
synthetic data is validated through sex classification and Alzheimer's disease
classification tasks, where the generated images achieve comparable performance
to real data. Our diffusion bridge model offers a promising solution for
improving neuroimaging datasets and supporting clinical decision-making, with
the potential to significantly impact neuroimaging research and clinical
practice.

</details>


### [217] [Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](https://arxiv.org/abs/2504.15271)
*Guo Chen,Zhiqi Li,Shihao Wang,Jindong Jiang,Yicheng Liu,Lidong Lu,De-An Huang,Wonmin Byeon,Matthieu Le,Tuomas Rintamaki,Tyler Poon,Max Ehrlich,Tuomas Rintamaki,Tyler Poon,Tong Lu,Limin Wang,Bryan Catanzaro,Jan Kautz,Andrew Tao,Zhiding Yu,Guilin Liu*

Main category: cs.CV

TL;DR: Eagle 2.5是一个前沿的视觉语言模型家族，专注于长上下文多模态学习，解决了长视频理解和高分辨率图像识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在长上下文多模态任务中存在局限性，如长视频理解和高质量图像处理。

Method: 提出了自动降级采样和图像区域保留技术，优化了长上下文数据训练的效率。

Result: Eagle 2.5-8B在Video-MME基准测试中达到72.4%的准确率，媲美顶级商业和开源模型。

Conclusion: Eagle 2.5为长上下文多模态任务提供了高效且强大的解决方案。

Abstract: We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)
for long-context multimodal learning. Our work addresses the challenges in long
video comprehension and high-resolution image understanding, introducing a
generalist framework for both tasks. The proposed training framework
incorporates Automatic Degrade Sampling and Image Area Preservation, two
techniques that preserve contextual integrity and visual details. The framework
also includes numerous efficiency optimizations in the pipeline for
long-context data training. Finally, we propose Eagle-Video-110K, a novel
dataset that integrates both story-level and clip-level annotations,
facilitating long-video understanding. Eagle 2.5 demonstrates substantial
improvements on long-context multimodal benchmarks, providing a robust solution
to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B
achieves 72.4% on Video-MME with 512 input frames, matching the results of
top-tier commercial model such as GPT-4o and large-scale open-source models
like Qwen2.5-VL-72B and InternVL2.5-78B.

</details>


### [218] [DRAWER: Digital Reconstruction and Articulation With Environment Realism](https://arxiv.org/abs/2504.15278)
*Hongchi Xia,Entong Su,Marius Memmel,Arhan Jain,Raymond Yu,Numfor Mbiziwo-Tiapo,Ali Farhadi,Abhishek Gupta,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.CV

TL;DR: DRAWER框架将静态室内场景视频转换为逼真、交互式的数字环境，适用于游戏和机器人仿真。


<details>
  <summary>Details</summary>
Motivation: 通过虚拟数字复制品释放游戏和机器人等领域的潜力。

Method: 基于双场景表示的重建模块和关节模块，实现几何细节重建与交互式场景生成。

Result: 生成的虚拟环境逼真、交互性强，实时运行，兼容游戏引擎和机器人仿真平台。

Conclusion: DRAWER成功应用于自动创建游戏和机器人仿真，展示了其广泛潜力。

Abstract: Creating virtual digital replicas from real-world data unlocks significant
potential across domains like gaming and robotics. In this paper, we present
DRAWER, a novel framework that converts a video of a static indoor scene into a
photorealistic and interactive digital environment. Our approach centers on two
main contributions: (i) a reconstruction module based on a dual scene
representation that reconstructs the scene with fine-grained geometric details,
and (ii) an articulation module that identifies articulation types and hinge
positions, reconstructs simulatable shapes and appearances and integrates them
into the scene. The resulting virtual environment is photorealistic,
interactive, and runs in real time, with compatibility for game engines and
robotic simulation platforms. We demonstrate the potential of DRAWER by using
it to automatically create an interactive game in Unreal Engine and to enable
real-to-sim-to-real transfer for robotics applications.

</details>


### [219] [VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2504.15279)
*Weiye Xu,Jiahao Wang,Weiyun Wang,Zhe Chen,Wengang Zhou,Aijun Yang,Lewei Lu,Houqiang Li,Xiaohua Wang,Xizhou Zhu,Wenhai Wang,Jifeng Dai,Jinguo Zhu*

Main category: cs.CV

TL;DR: VisuLogic是一个包含1000个人工验证问题的视觉推理基准，用于评估多模态大语言模型（MLLMs）的视觉推理能力。测试结果显示，大多数模型表现不佳，准确率低于30%，远低于人类的51.4%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的推理评估依赖文本描述，存在语言推理捷径，无法真正衡量视觉中心推理能力。

Method: 提出VisuLogic基准，包含六类问题，评估MLLMs的视觉推理能力，并提供训练数据集和强化学习基线。

Result: 大多数模型准确率低于30%，仅略高于25%的随机基线，远低于人类的51.4%。

Conclusion: MLLMs在视觉推理方面存在显著差距，需进一步研究提升。

Abstract: Visual reasoning is a core component of human intelligence and a critical
capability for advanced multimodal models. Yet current reasoning evaluations of
multimodal large language models (MLLMs) often rely on text descriptions and
allow language-based reasoning shortcuts, failing to measure genuine
vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark
of 1,000 human-verified problems across six categories (e.g., quantitative
shifts, spatial relations, attribute comparisons). These various types of
questions can be evaluated to assess the visual reasoning capabilities of MLLMs
from multiple perspectives. We evaluate leading MLLMs on this benchmark and
analyze their results to identify common failure modes. Most models score below
30% accuracy-only slightly above the 25% random baseline and far below the
51.4% achieved by humans-revealing significant gaps in visual reasoning.
Furthermore, we provide a supplementary training dataset and a
reinforcement-learning baseline to support further progress.

</details>


### [220] [StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians](https://arxiv.org/abs/2504.15281)
*Cailin Zhuang,Yaoqi Hu,Xuanyang Zhang,Wei Cheng,Jiacheng Bao,Shengqi Liu,Yiying Yang,Xianfang Zeng,Gang Yu,Ming Li*

Main category: cs.CV

TL;DR: StyleMe3D是一个用于3D高斯溅射（3DGS）风格迁移的整体框架，解决了传统方法在风格化场景中的不足，通过多模态风格调节、多级语义对齐和感知质量增强实现高质量风格迁移。


<details>
  <summary>Details</summary>
Motivation: 3DGS在真实场景重建中表现出色，但在风格化场景（如卡通、游戏）中存在纹理碎片化、语义不对齐和抽象美学适应性差的问题。

Method: 提出StyleMe3D框架，包含动态风格分数蒸馏（DSSD）、对比风格描述符（CSD）、同时优化尺度（SOS）和3D高斯质量评估（3DG-QA）四个新组件。

Result: 在NeRF合成数据集和tandt db数据集上，StyleMe3D在保留几何细节和风格一致性方面优于现有方法，同时保持实时渲染。

Conclusion: StyleMe3D填补了真实3DGS与艺术风格化之间的空白，为游戏、虚拟世界和数字艺术提供了新应用可能。

Abstract: 3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction
but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented
textures, semantic misalignment, and limited adaptability to abstract
aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer
that integrates multi-modal style conditioning, multi-level semantic alignment,
and perceptual quality enhancement. Our key insights include: (1) optimizing
only RGB attributes preserves geometric integrity during stylization; (2)
disentangling low-, medium-, and high-level semantics is critical for coherent
style transfer; (3) scalability across isolated objects and complex scenes is
essential for practical deployment. StyleMe3D introduces four novel components:
Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent
space for semantic alignment; Contrastive Style Descriptor (CSD) for localized,
content-aware texture transfer; Simultaneously Optimized Scale (SOS) to
decouple style details and structural coherence; and 3D Gaussian Quality
Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated
data to suppress artifacts and enhance visual harmony. Evaluated on NeRF
synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D
outperforms state-of-the-art methods in preserving geometric details (e.g.,
carvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,
coherent lighting in landscapes), while maintaining real-time rendering. This
work bridges photorealistic 3D GS and artistic stylization, unlocking
applications in gaming, virtual worlds, and digital art.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [221] [PyFRep: Shape Modeling with Differentiable Function Representation](https://arxiv.org/abs/2504.14216)
*Pierre-Alain Fayolle,Evgenii Maltsev*

Main category: cs.GR

TL;DR: 提出了一种基于函数表示（FRep）的可微分几何建模框架，支持自动微分，应用于曲率估计、符号距离函数计算及形状参数拟合。


<details>
  <summary>Details</summary>
Motivation: 为几何建模提供一种可微分的方法，便于计算空间或形状参数的导数。

Method: 基于FRep构建框架，利用现代自动微分库实现导数计算。

Result: 展示了框架在曲率估计、符号距离函数计算和形状参数拟合中的应用。

Conclusion: 框架开源发布，为几何建模提供了灵活的工具。

Abstract: We propose a framework for performing differentiable geometric modeling based
on the Function Representation (FRep). The framework is built on top of modern
libraries for performing automatic differentiation allowing us to obtain
derivatives w.r.t. space or shape parameters. We demonstrate possible
applications of this framework: Curvature estimation for shape interrogation,
signed distance function computation and approximation and fitting shape
parameters of a parametric model to data. Our framework is released as
open-source.

</details>


### [222] [PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling](https://arxiv.org/abs/2504.14219)
*Alara Dirik,Tuanfeng Wang,Duygu Ceylan,Stefanos Zafeiriou,Anna Frühstück*

Main category: cs.GR

TL;DR: PRISM是一个统一的框架，通过微调预训练的文本到图像扩散模型，同时生成RGB图像和内在映射（X层），支持多种图像生成和编辑任务。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要单独推断内在属性或使用不同模型的问题，PRISM通过联合生成所有内在层保持模态一致性。

Method: 基于预训练的文本到图像扩散模型，提出有效的微调策略，同时生成RGB图像和X层。

Result: 实验表明PRISM在内在图像分解和条件图像生成中表现优异，同时保留了基础模型的文本到图像生成能力。

Conclusion: PRISM是一个多功能框架，支持多种图像任务，并在性能和一致性上具有竞争力。

Abstract: We present PRISM, a unified framework that enables multiple image generation
and editing tasks in a single foundational model. Starting from a pre-trained
text-to-image diffusion model, PRISM proposes an effective fine-tuning strategy
to produce RGB images along with intrinsic maps (referred to as X layers)
simultaneously. Unlike previous approaches, which infer intrinsic properties
individually or require separate models for decomposition and conditional
generation, PRISM maintains consistency across modalities by generating all
intrinsic layers jointly. It supports diverse tasks, including text-to-RGBX
generation, RGB-to-X decomposition, and X-to-RGBX conditional generation.
Additionally, PRISM enables both global and local image editing through
conditioning on selected intrinsic layers and text prompts. Extensive
experiments demonstrate the competitive performance of PRISM both for intrinsic
image decomposition and conditional image generation while preserving the base
model's text-to-image generation capability.

</details>


### [223] [HoLa: B-Rep Generation using a Holistic Latent Representation](https://arxiv.org/abs/2504.14257)
*Yilin Liu,Duoteng Xu,Xingyao Yu,Xiang Xu,Daniel Cohen-Or,Hao Zhang,Hui Huang*

Main category: cs.GR

TL;DR: 提出了一种基于边界表示（B-Reps）的新型CAD模型表示方法，通过统一的Holistic Latent（HoLa）空间整合几何与拓扑关系，显著提升了生成模型的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CAD模型表示方法在几何与拓扑关系处理上存在冗余和不一致性，需要更高效且统一的表示方法。

Method: 通过神经交叉网络学习曲面间的几何关系，消除曲线和拓扑连接的显式表示，构建仅基于曲面的Holistic Latent空间。

Result: 生成模型的准确性显著提升（82% vs. 50%），同时减少了训练复杂性和生成冗余。

Conclusion: HoLa空间为CAD模型的学习与生成提供了更高效、统一的解决方案，显著优于现有方法。

Abstract: We introduce a novel representation for learning and generating
Computer-Aided Design (CAD) models in the form of $\textit{boundary
representations}$ (B-Reps). Our representation unifies the continuous geometric
properties of B-Rep primitives in different orders (e.g., surfaces and curves)
and their discrete topological relations in a $\textit{holistic latent}$ (HoLa)
space. This is based on the simple observation that the topological connection
between two surfaces is intrinsically tied to the geometry of their
intersecting curve. Such a prior allows us to reformulate topology learning in
B-Reps as a geometric reconstruction problem in Euclidean space. Specifically,
we eliminate the presence of curves, vertices, and all the topological
connections in the latent space by learning to distinguish and derive curve
geometries from a pair of surface primitives via a neural intersection network.
To this end, our holistic latent space is only defined on surfaces but encodes
a full B-Rep model, including the geometry of surfaces, curves, vertices, and
their topological relations. Our compact and holistic latent space facilitates
the design of a first diffusion-based generator to take on a large variety of
inputs including point clouds, single/multi-view images, 2D sketches, and text
prompts. Our method significantly reduces ambiguities, redundancies, and
incoherences among the generated B-Rep primitives, as well as training
complexities inherent in prior multi-step B-Rep learning pipelines, while
achieving greatly improved validity rate over current state of the art: 82% vs.
$\approx$50%.

</details>


### [224] [Can AI Recognize the Style of Art? Analyzing Aesthetics through the Lens of Style Transfer](https://arxiv.org/abs/2504.14272)
*Yunha Yeo,Daeho Um*

Main category: cs.GR

TL;DR: 论文探讨了AI如何通过风格迁移识别风格，从美学角度分析两种算法（CNN和Transformer），并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管风格迁移研究多关注图像质量提升，但本文从美学角度出发，试图连接AI技术与美学。

Method: 分析基于CNN和Transformer的风格迁移算法，通过美学比较探索风格构成元素。

Result: 揭示了当前风格迁移技术的局限性。

Conclusion: 基于局限性，提出了未来风格迁移技术的潜在研究方向。

Abstract: This study investigates how artificial intelligence (AI) recognizes style
through style transfer-an AI technique that generates a new image by applying
the style of one image to another. Despite the considerable interest that style
transfer has garnered among researchers, most efforts have focused on enhancing
the quality of output images through advanced AI algorithms. In this paper, we
approach style transfer from an aesthetic perspective, thereby bridging AI
techniques and aesthetics. We analyze two style transfer algorithms: one based
on convolutional neural networks (CNNs) and the other utilizing recent
Transformer models. By comparing the images produced by each, we explore the
elements that constitute the style of artworks through an aesthetic analysis of
the style transfer results. We then elucidate the limitations of current style
transfer techniques. Based on these limitations, we propose potential
directions for future research on style transfer techniques.

</details>


### [225] [SEGA: Drivable 3D Gaussian Head Avatar from a Single Image](https://arxiv.org/abs/2504.14373)
*Chen Guo,Zhuo Su,Jian Wang,Shuang Li,Xu Chang,Zhaohu Li,Yang Zhao,Guidong Wang,Ruqi Huang*

Main category: cs.GR

TL;DR: SEGA提出了一种基于单图像的3D可驱动高斯头像创建方法，结合了先验模型和分层UV空间高斯溅射框架，实现了高保真度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖多图像或多视角输入的限制，提升单图像创建3D头像的实用性和质量。

Method: 结合广义先验模型和分层UV空间高斯溅射框架，利用双分支架构分离动态和静态面部组件。

Result: 在泛化能力、身份保持和表情真实感方面优于现有方法。

Conclusion: SEGA为单图像3D头像创建提供了高效且高质量的解决方案，适用于实际应用。

Abstract: Creating photorealistic 3D head avatars from limited input has become
increasingly important for applications in virtual reality, telepresence, and
digital entertainment. While recent advances like neural rendering and 3D
Gaussian splatting have enabled high-quality digital human avatar creation and
animation, most methods rely on multiple images or multi-view inputs, limiting
their practicality for real-world use. In this paper, we propose SEGA, a novel
approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that
combines generalized prior models with a new hierarchical UV-space Gaussian
Splatting framework. SEGA seamlessly combines priors derived from large-scale
2D datasets with 3D priors learned from multi-view, multi-expression, and
multi-ID data, achieving robust generalization to unseen identities while
ensuring 3D consistency across novel viewpoints and expressions. We further
present a hierarchical UV-space Gaussian Splatting framework that leverages
FLAME-based structural priors and employs a dual-branch architecture to
disentangle dynamic and static facial components effectively. The dynamic
branch encodes expression-driven fine details, while the static branch focuses
on expression-invariant regions, enabling efficient parameter inference and
precomputation. This design maximizes the utility of limited 3D data and
achieves real-time performance for animation and rendering. Additionally, SEGA
performs person-specific fine-tuning to further enhance the fidelity and
realism of the generated avatars. Experiments show our method outperforms
state-of-the-art approaches in generalization ability, identity preservation,
and expression realism, advancing one-shot avatar creation for practical
applications.

</details>


### [226] [Interdisciplinary Integration of Remote Sensing -- A Review with Four Examples](https://arxiv.org/abs/2504.14590)
*Zichen Jin*

Main category: cs.GR

TL;DR: 本文简要回顾了遥感与生态学、数学形态学、机器学习和电子学四个领域的跨学科整合。


<details>
  <summary>Details</summary>
Motivation: 探讨遥感作为一门高级学科如何依赖并整合其他基础和应用学科与技术。

Method: 通过四个具体例子（生态学、数学形态学、机器学习和电子学）说明遥感的跨学科整合。

Result: 展示了遥感如何通过整合多学科知识和技术实现其发展。

Conclusion: 遥感的发展离不开跨学科整合，未来将继续依赖多学科的贡献。

Abstract: As a high-level discipline, the development of remote sensing depends on the
contribution of many other basic and applied disciplines and technologies. For
example, due to the close relationship between remote sensing and
photogrammetry, remote sensing would inevitably integrate disciplines such as
optics and color science. Also, remote sensing integrates the knowledge of
electronics in the conversion from optical signals to electrical signals via
CCD (Charge-Coupled Device) or other image sensors. Moreover, when conducting
object identification and classification with remote sensing data, mathematical
morphology and other digital image processing technologies are used. These
examples are only the tip of the iceberg of interdisciplinary integration of
remote sensing. This work briefly reviews the interdisciplinary integration of
remote sensing with four examples - ecology, mathematical morphology, machine
learning, and electronics.

</details>


### [227] [A Controllable Appearance Representation for Flexible Transfer and Editing](https://arxiv.org/abs/2504.15028)
*Santiago Jimenez-Navarro,Julia Guerrero-Viu,Belen Masia*

Main category: cs.GR

TL;DR: 提出一种自监督学习方法，通过FactorVAE生成紧凑、解耦的潜在空间表示，用于材料外观的直观编辑。


<details>
  <summary>Details</summary>
Motivation: 避免人工标注带来的偏差，实现材料外观和光照的无监督解耦表示。

Method: 使用FactorVAE自监督学习，设计无标签数据集训练模型，并结合轻量级IP-Adapter指导扩散模型。

Result: 模型能有效解耦材料外观和光照，支持用户直观编辑图像属性（如色调、光泽度）。

Conclusion: 该方法提供了细粒度的外观控制，适用于材料外观的迁移和编辑。

Abstract: We present a method that computes an interpretable representation of material
appearance within a highly compact, disentangled latent space. This
representation is learned in a self-supervised fashion using an adapted
FactorVAE. We train our model with a carefully designed unlabeled dataset,
avoiding possible biases induced by human-generated labels. Our model
demonstrates strong disentanglement and interpretability by effectively
encoding material appearance and illumination, despite the absence of explicit
supervision. Then, we use our representation as guidance for training a
lightweight IP-Adapter to condition a diffusion pipeline that transfers the
appearance of one or more images onto a target geometry, and allows the user to
further edit the resulting appearance. Our approach offers fine-grained control
over the generated results: thanks to the well-structured compact latent space,
users can intuitively manipulate attributes such as hue or glossiness in image
space to achieve the desired final appearance.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [228] [VoxLogicA UI: Supporting Declarative Medical Image Analysis](https://arxiv.org/abs/2504.13846)
*Antonio Strippoli*

Main category: cs.HC

TL;DR: 该论文设计了一个用户友好的VoxLogicA界面，旨在简化复杂的图像分析工具，使其更适合医学专业人士使用。


<details>
  <summary>Details</summary>
Motivation: 现有工具过于复杂，限制了医学专业人士和研究人员的实用性，因此需要更易用的界面。

Method: 采用现代Web技术（如Svelte和Niivue）设计并构建界面，并通过用户研究和实际案例分析测试其效果。

Result: 目标是开发出实用且易用的界面，提升工具在临床环境中的可访问性。

Conclusion: 通过简化界面设计，可以增强VoxLogicA在医疗领域的实用性。

Abstract: This Master's Thesis in Computer Science dives into the design and creation
of a user-friendly interface for VoxLogicA, an image analysis tool using
spatial model checking with a focus on neuroimaging. The research tackles the
problem of existing tools being too complex, which makes them hard for medical
professionals and researchers to use. By using spatial logic, the goal is to
make these powerful analytical tools more practical and accessible in
real-world clinical settings. The main objectives are to design a modern web
interface that's easy to use, build it with the latest web technologies (e.g.
Svelte and Niivue), and test its effectiveness through user studies and
real-world case analyses.

</details>


### [229] [Semantic Direct Modeling](https://arxiv.org/abs/2504.13893)
*Qiang Zou,Shuo Liu*

Main category: cs.HC

TL;DR: 论文提出了一种语义直接建模（SDM）方法，通过结合大语言模型（LLM）和生成式AI，实现从高级设计意图到低级几何修改的无缝转换。


<details>
  <summary>Details</summary>
Motivation: 现有直接建模系统限制用户只能进行低级几何操作，无法专注于高级设计意图。

Method: 利用经过CAD特定提示微调的LLM解析设计意图，并通过条件上下文敏感的特征识别方法将意图映射到几何特征。

Result: SDM在真实机械设计案例中验证了其有效性。

Conclusion: SDM通过自然语言交互和动态特征识别，显著提升了设计效率。

Abstract: Current direct modeling systems limit users to low-level interactions with
vertices, edges, and faces, forcing designers to manage detailed geometric
elements rather than focusing on high-level design intent. This paper
introduces semantic direct modeling (SDM), a novel approach that lifts direct
modeling from low-level geometric modifications to high-level semantic
interactions. This is achieved by utilizing a large language model (LLM)
fine-tuned with CAD-specific prompts, which can guide the LLM to reason through
design intent and accurately interpret CAD commands, thereby allowing designers
to express their intent using natural language. Additionally, SDM maps design
intent to the corresponding geometric features in the CAD model through a new
conditional, context-sensitive feature recognition method, which uses
generative AI to dynamically assign feature labels based on design intent.
Together, they enable a seamless flow from high-level design intent to
low-level geometric modifications, bypassing tedious software interactions. The
effectiveness of SDM has been validated through real mechanical design cases.

</details>


### [230] [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
*Zhe Liu*

Main category: cs.HC

TL;DR: 该研究提出了一种名为Interview AI-ssistant的系统，旨在通过大型语言模型（LLMs）增强人类与AI在访谈中的协作，解决访谈者在实时信息处理、问题调整和关系维护中的认知挑战。


<details>
  <summary>Details</summary>
Motivation: 访谈在获取深度、情境化见解方面具有重要价值，但访谈者面临实时信息处理和问题适应等认知挑战，因此需要AI辅助。

Method: 通过四项相互关联的研究，包括需求分析、原型开发、实时AI辅助实验和实地部署，设计并评估了Interview AI-ssistant系统。

Result: 研究为智能访谈支持系统的实际应用提供了设计指南，并推动了人类-AI协作界面在复杂社交任务中的理解。

Conclusion: 该研究不仅为定性研究工具提供了AI增强的设计方案，还为智能用户界面（IUI）社区贡献了新的见解。

Abstract: Recent advances in large language models (LLMs) offer unprecedented
opportunities to enhance human-AI collaboration in qualitative research
methods, including interviews. While interviews are highly valued for gathering
deep, contextualized insights, interviewers often face significant cognitive
challenges, such as real-time information processing, question adaptation, and
rapport maintenance. My doctoral research introduces Interview AI-ssistant, a
system designed for real-time interviewer-AI collaboration during both the
preparation and execution phases. Through four interconnected studies, this
research investigates the design of effective human-AI collaboration in
interviewing contexts, beginning with a formative study of interviewers' needs,
followed by a prototype development study focused on AI-assisted interview
preparation, an experimental evaluation of real-time AI assistance during
interviews, and a field study deploying the system in a real-world research
setting. Beyond informing practical implementations of intelligent interview
support systems, this work contributes to the Intelligent User Interfaces (IUI)
community by advancing the understanding of human-AI collaborative interfaces
in complex social tasks and establishing design guidelines for AI-enhanced
qualitative research tools.

</details>


### [231] [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
*Ivan Sviridov,Amina Miftakhova,Artemiy Tereshchenko,Galina Zubkova,Pavel Blinov,Andrey Savchenko*

Main category: cs.HC

TL;DR: 3MDBench是一个开源评估框架，用于评估大型视觉语言模型（LVLM）在医疗咨询中的表现，通过模拟多样化患者行为和结合多模态数据提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 探索LVLM在远程医疗中的应用潜力，尤其是其处理多样化患者行为的能力。

Method: 开发3MDBench框架，模拟真实患者行为，结合文本和图像数据，评估LVLM在不同诊断策略下的表现。

Result: 对话和多模态输入显著提升诊断准确性（F1分数从50.4提高到54.2），结合CNN模型后F1分数进一步提升至70.3。

Conclusion: 3MDBench为AI驱动的医疗助手提供了可扩展的评估框架，强调了患者行为、对话策略和多模态推理对诊断质量的影响。

Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for
applications in telemedicine, yet their ability to engage with diverse patient
behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal
Multi-agent Dialogue Benchmark), an open-source evaluation framework designed
to assess LLM-driven medical consultations. Unlike existing benchmarks,
3MDBench simulates real-world patient variability by incorporating four
temperament-driven Patient Agents and an Assessor Agent that evaluates
diagnostic accuracy and dialogue quality. The benchmark integrates textual and
image-based patient data across 34 common diagnoses, mirroring real-world
telemedicine interactions. Under different diagnostic strategies, we evaluate
state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue
improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,
underscoring the value of context-driven, information-seeking questioning.
Additionally, we demonstrate that multimodal inputs enhance diagnostic
efficiency. Image-supported models outperform text-only counterparts by raising
the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.
Finally, we suggest an approach that improves the diagnostic F1-score to 70.3
by training the CNN model on the diagnosis prediction task and incorporating
its top-3 predictions into the LVLM context. 3MDBench provides a reproducible
and extendable evaluation framework for AI-driven medical assistants. It offers
insights into how patient temperament, dialogue strategies, and multimodal
reasoning influence diagnosis quality. By addressing real-world complexities in
telemedicine, our benchmark paves the way for more empathetic, reliable, and
context-aware AI-driven healthcare solutions. The source code of our benchmark
is publicly available: https://github.com/univanxx/3mdbench

</details>


### [232] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
*Fei Tang,Haolei Xu,Hang Zhang,Siqi Chen,Xingyu Wu,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Zeqi Tan,Yuchen Yan,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.HC

TL;DR: 综述探讨了基于LLM的GUI代理的架构、技术组件和评估方法，分析了感知、探索、规划和交互四大组件，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究GUI代理的快速发展及其在自动化界面操作中的应用，揭示LLM和多模态学习如何推动该领域进步。

Method: 系统分析GUI代理的四大组件：感知、探索、规划和交互，并评估现有框架的局限性。

Result: 揭示了LLM和多模态学习对GUI自动化的革命性影响，提出了标准化方向和技术挑战。

Conclusion: 为研究者和从业者提供了全面理解，并展望了智能界面自动化的未来发展方向。

Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative
paradigm in human-computer interaction, evolving from rule-based automation
scripts to sophisticated AI-driven systems capable of understanding and
executing complex interface operations. This survey provides a comprehensive
examination of the rapidly advancing field of LLM-based GUI Agents,
systematically analyzing their architectural foundations, technical components,
and evaluation methodologies. We identify and analyze four fundamental
components that constitute modern GUI Agents: (1) perception systems that
integrate text-based parsing with multimodal understanding for comprehensive
interface comprehension; (2) exploration mechanisms that construct and maintain
knowledge bases through internal modeling, historical experience, and external
information retrieval; (3) planning frameworks that leverage advanced reasoning
methodologies for task decomposition and execution; and (4) interaction systems
that manage action generation with robust safety controls. Through rigorous
analysis of these components, we reveal how recent advances in large language
models and multimodal learning have revolutionized GUI automation across
desktop, mobile, and web platforms. We critically examine current evaluation
frameworks, highlighting methodological limitations in existing benchmarks
while proposing directions for standardization. This survey also identifies key
technical challenges, including accurate element localization, effective
knowledge retrieval, long-horizon planning, and safety-aware execution control,
while outlining promising research directions for enhancing GUI Agents'
capabilities. Our systematic review provides researchers and practitioners with
a thorough understanding of the field's current state and offers insights into
future developments in intelligent interface automation.

</details>


### [233] [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
*Megan Gu,Chloe Qianhui Zhao,Claire Liu,Nikhil Patel,Jahnvi Shah,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.HC

TL;DR: 研究利用GPT-3.5自动评估五种辅导策略的有效性，结果显示模型在排除错误分类上表现良好，但在识别正确策略上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在辅导策略分析中的应用潜力。

Method: 使用GPT-3.5和少量示例提示，分析公共数据集中的辅导对话。

Result: True Negative Rates（TNR）为0.655至0.738，Recall为0.327至0.432，表明模型在排除错误分类上表现较好，但识别正确策略的能力有限。

Conclusion: LLMs在辅导策略分析中具有潜力，未来可通过更先进的模型提升性能。

Abstract: Our study introduces an automated system leveraging large language models
(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving
effective praise, 2. reacting to errors, 3. determining what students know, 4.
helping students manage inequity, and 5. responding to negative self-talk.
Using a public dataset from the Teacher-Student Chatroom Corpus, our system
classifies each tutoring strategy as either being employed as desired or
undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use
of these strategies and analyze tutoring dialogues. The results show that for
the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to
0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is
effective at excluding incorrect classifications but struggles to consistently
identify the correct strategy. The strategy \textit{helping students manage
inequity} showed the highest performance with a TNR of 0.738 and Recall of
0.432. The study highlights the potential of LLMs in tutoring strategy analysis
and outlines directions for future improvements, including incorporating more
advanced models for more nuanced feedback.

</details>


### [234] [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
*Isabel Villanueva,Tara Bobinac,Binwei Yao,Junjie Hu,Kaiping Chen*

Main category: cs.HC

TL;DR: 研究发现，AI聊天机器人在促进跨文化共情方面存在局限性，特别是在文化对齐和对话类型上的差异影响了效果。


<details>
  <summary>Details</summary>
Motivation: 探讨AI聊天机器人作为对话代理在跨文化共情中的作用，填补实证研究的空白。

Method: 采用随机对话实验，比较了不同类型（审议与非审议、文化对齐与非对齐）的AI聊天机器人互动对跨文化共情的影响。

Result: 审议对话增加了美国参与者的跨文化共情，但对拉丁美洲参与者无效，因AI未能准确代表其文化背景。实时分析揭示了文化知识差距。

Conclusion: 研究强调了设计具备文化真实性AI系统的重要性，为审议理论和AI对齐研究提供了新见解。

Abstract: Despite the growing integration of AI chatbots as conversational agents in
public discourse, empirical evidence regarding their capacity to foster
intercultural empathy remains limited. Using a randomized dialogue experiment,
we examined how different types of AI chatbot interaction, i.e., deliberative
versus non-deliberative and culturally aligned versus non-aligned, affect
intercultural empathy across cultural groups. Results show that deliberative
conversations increased intercultural empathy among American participants but
not Latin American participants, who perceived AI responses as culturally
inaccurate and failing to represent their cultural contexts and perspectives
authentically. Real-time interaction analyses reveal that these differences
stem from cultural knowledge gaps inherent in Large Language Models. Despite
explicit prompting and instruction to represent cultural perspectives in
participants' native languages, AI systems still exhibit significant
disparities in cultural representation. This highlights the importance of
designing AI systems capable of culturally authentic engagement in deliberative
conversations. Our study contributes to deliberation theory and AI alignment
research by underscoring AI's role in intercultural dialogue and the persistent
challenge of representational asymmetry in democratic discourse.

</details>


### [235] [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
*Paul Taele,Jung In Koh,Tracy Hammond*

Main category: cs.HC

TL;DR: Kanji Workbook是一个智能辅导系统，通过模拟教师反馈帮助学生学习日语汉字书写，提升课程成绩。


<details>
  <summary>Details</summary>
Motivation: 英语母语学生在学习日语汉字时面临困难，现有教学方法和应用缺乏直接反馈。

Method: 开发Kanji Workbook系统，结合智能评分和视觉动画提供多样化书写评估。

Result: 用户课程成绩显著高于同龄人，并对系统功能反应积极。

Conclusion: Kanji Workbook有效辅助汉字学习，提升学生表现。

Abstract: Kanji script writing is a skill that is often introduced to novice Japanese
foreign language students for achieving Japanese writing mastery, but often
poses difficulties to students with primarily English fluency due to their its
vast differences with written English. Instructors often introduce various
pedagogical methods -- such as visual structure and written techniques -- to
assist students in kanji study, but may lack availability providing direct
feedback on students' writing outside of class. Current educational
applications are also limited due to lacking richer instructor-emulated
feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring
system for students to receive intelligent assessment that emulates human
instructor feedback. Our interface not only leverages students' computing
devices for allowing them to learn, practice, and review the writing of
prompted characters from their course's kanji script lessons, but also provides
a diverse set of writing assessment metrics -- derived from instructor
interviews and classroom observation insights -- through intelligent scoring
and visual animations. We deployed our interface onto novice- and
intermediate-level university courses over an entire academic year, and
observed that interface users on average achieved higher course grades than
their peers and also reacted positively to our interface's various features.

</details>


### [236] [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
*Chen Shani,Elizabeth C. Stade*

Main category: cs.HC

TL;DR: 论文指出计算心理健康研究中使用的心理病理学测量方法存在问题，并提出使用已验证、维度和跨诊断的测量方法的建议。


<details>
  <summary>Details</summary>
Motivation: 解决计算心理健康研究中因使用不适当的心理病理学测量方法而导致的效度问题。

Method: 识别三个关键问题并提出改进建议：使用已验证的测量方法、维度化处理心理结构、关注跨诊断指标。

Result: 强调使用有效测量方法的重要性，以反映心理病理学的本质和结构。

Conclusion: 计算心理健康研究应采用已验证、维度和跨诊断的测量方法，以提高研究的有效性。

Abstract: Computational mental health research develops models to predict and
understand psychological phenomena, but often relies on inappropriate measures
of psychopathology constructs, undermining validity. We identify three key
issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)
over validated ones (e.g., diagnosis by clinician); (2) treating mental health
constructs as categorical rather than dimensional; and (3) focusing on
disorder-specific constructs instead of transdiagnostic ones. We outline the
benefits of using validated, dimensional, and transdiagnostic measures and
offer practical recommendations for practitioners. Using valid measures that
reflect the nature and structure of psychopathology is essential for
computational mental health research.

</details>


### [237] [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
*Stefano De Paoli,Alex Fawzi*

Main category: cs.HC

TL;DR: 本文介绍了一种基于大型语言模型（LLM）的图形用户界面（GUI）应用，用于辅助研究人员进行主题分析（TA），无需编程技能即可生成和优化代码与主题。


<details>
  <summary>Details</summary>
Motivation: 为缺乏编程技能的研究人员（如社会科学或人文学科）提供一种简单易用的工具，以利用LLM进行主题分析。

Method: 开发了一个基于streamlit框架的GUI应用，结合Python脚本和LLM的API，支持用户上传文本数据并生成初始代码和主题。

Result: 应用成功实现了无需编程的主题分析流程，支持用户通过人机交互迭代优化结果。

Conclusion: 该应用为定性研究提供了便捷的工具，同时保持了方法论的严谨性，未来将进一步优化功能。

Abstract: Thematic analysis (TA) is a widely used qualitative research method for
identifying and interpreting patterns within textual data, such as qualitative
interviews. Recent research has shown that it is possible to satisfactorily
perform TA using Large Language Models (LLMs). This paper presents a novel
application using LLMs to assist researchers in conducting TA. The application
enables users to upload textual data, generate initial codes and themes. All of
this is possible through a simple Graphical User Interface, (GUI) based on the
streamlit framework, working with python scripts for the analysis, and using
Application Program Interfaces of LLMs. Having a GUI is particularly important
for researchers in fields where coding skills may not be prevalent, such as
social sciences or humanities. With the app, users can iteratively refine codes
and themes adopting a human-in-the-loop process, without the need to work with
programming and scripting. The paper describes the application key features,
highlighting its potential for qualitative research while preserving
methodological rigor. The paper discusses the design and interface of the app
and outlines future directions for this work.

</details>


### [238] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
*Donghuo Zeng,Roberto Legaspi,Yuewen Sun,Xinshuai Dong,Kazushi Ikeda,Peter Spirtes,Kun Zhang*

Main category: cs.HC

TL;DR: 论文提出了一种基于因果和反事实知识的自适应策略，通过反事实推理和因果发现优化系统响应，显著提升了说服性对话系统的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过因果和反事实知识优化系统响应，以提升用户与系统交互的效果。

Method: 方法包括利用反事实推理生成假设场景，通过因果发现识别策略，并将心理构造和不可观测噪声作为潜在因素进行估计。

Result: 实验结果显示，该方法在真实数据集上显著提升了说服性对话系统的累积奖励。

Conclusion: 结论表明，因果发现能有效指导个性化反事实推理，并优化对话策略。

Abstract: We hypothesize that optimal system responses emerge from adaptive strategies
grounded in causal and counterfactual knowledge. Counterfactual inference
allows us to create hypothetical scenarios to examine the effects of
alternative system responses. We enhance this process through causal discovery,
which identifies the strategies informed by the underlying causal structure
that govern system behaviors. Moreover, we consider the psychological
constructs and unobservable noises that might be influencing user-system
interactions as latent factors. We show that these factors can be effectively
estimated. We employ causal discovery to identify strategy-level causal
relationships among user and system utterances, guiding the generation of
personalized counterfactual dialogues. We model the user utterance strategies
as causal factors, enabling system strategies to be treated as counterfactual
actions. Furthermore, we optimize policies for selecting system responses based
on counterfactual data. Our results using a real-world dataset on social good
demonstrate significant improvements in persuasive system outcomes, with
increased cumulative rewards validating the efficacy of causal discovery in
guiding personalized counterfactual inference and optimizing dialogue policies
for a persuasive dialogue system.

</details>


### [239] [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
*Fan Gao,Xinjie Zhao,Ding Xia,Zhongyi Zhou,Rui Yang,Jinghui Lu,Hang Jiang,Chanjun Park,Irene Li*

Main category: cs.HC

TL;DR: HealthGenie结合知识图谱（KG）和大型语言模型（LLM），提供个性化饮食建议和可视化信息，降低用户认知负担。


<details>
  <summary>Details</summary>
Motivation: 解决用户在获取饮食建议时面临的专业知识复杂性和个性化需求问题。

Method: 通过KG和LLM结合，进行查询优化、信息检索和可视化，支持交互式个性化调整。

Result: 实验表明HealthGenie能有效提供个性化饮食建议，减少用户交互负担。

Conclusion: LLM与KG结合在决策支持中具有潜力，未来系统设计可参考此方法。

Abstract: Seeking dietary guidance often requires navigating complex professional
knowledge while accommodating individual health conditions. Knowledge Graphs
(KGs) offer structured and interpretable nutritional information, whereas Large
Language Models (LLMs) naturally facilitate conversational recommendation
delivery. In this paper, we present HealthGenie, an interactive system that
combines the strengths of LLMs and KGs to provide personalized dietary
recommendations along with hierarchical information visualization for a quick
and intuitive overview. Upon receiving a user query, HealthGenie performs query
refinement and retrieves relevant information from a pre-built KG. The system
then visualizes and highlights pertinent information, organized by defined
categories, while offering detailed, explainable recommendation rationales.
Users can further tailor these recommendations by adjusting preferences
interactively. Our evaluation, comprising a within-subject comparative
experiment and an open-ended discussion, demonstrates that HealthGenie
effectively supports users in obtaining personalized dietary guidance based on
their health conditions while reducing interaction effort and cognitive load.
These findings highlight the potential of LLM-KG integration in supporting
decision-making through explainable and visualized information. We examine the
system's usefulness and effectiveness with an N=12 within-subject study and
provide design considerations for future systems that integrate conversational
LLM and KG.

</details>


### [240] [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
*Rui Qiu,Shijie Chen,Yu Su,Po-Yin Yen,Han-Wei Shen*

Main category: cs.HC

TL;DR: InsightAgent是一个基于大型语言模型的交互式AI代理，通过语义分割和多代理设计改进系统综述（SRs）的质量和效率，显著减少完成时间。


<details>
  <summary>Details</summary>
Motivation: 系统综述在医疗等高要求领域至关重要，但传统方法耗时且依赖专家知识，现有自动摘要方法效果不佳。

Method: InsightAgent采用语义分割和多代理设计处理文献，并提供可视化界面供用户实时监控和反馈。

Result: 实验显示，InsightAgent将SRs质量提升27.2%，接近人工水平（79.7%），用户满意度提高34.4%，完成时间从数月缩短至1.5小时。

Conclusion: InsightAgent通过交互式AI和可视化工具，显著提升了系统综述的效率和质量，具有实际应用价值。

Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes
disciplines, such as healthcare, but are often impeded by intensive labors and
lengthy processes that can take months to complete. Due to the high demand for
domain expertise, existing automatic summarization methods fail to accurately
identify relevant studies and generate high-quality summaries. To that end, we
introduce InsightAgent, a human-centered interactive AI agent powered by large
language models that revolutionize this workflow. InsightAgent partitions a
large literature corpus based on semantics and employs a multi-agent design for
more focused processing of literature, leading to significant improvement in
the quality of generated SRs. InsightAgent also provides intuitive
visualizations of the corpus and agent trajectories, allowing users to
effortlessly monitor the actions of the agent and provide real-time feedback
based on their expertise. Our user studies with 9 medical professionals
demonstrate that the visualization and interaction mechanisms can effectively
improve the quality of synthesized SRs by 27.2%, reaching 79.7% of
human-written quality. At the same time, user satisfaction is improved by
34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather
than months, to complete a high-quality systematic review.

</details>


### [241] [Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises](https://arxiv.org/abs/2504.13866)
*Aleksa Marusic,Sao Mai Nguyen,Adriana Tapus*

Main category: cs.HC

TL;DR: 提出了一种基于Transformer的算法，用于康复训练中的错误分类，并通过关节重要性计算提供更详细的反馈。


<details>
  <summary>Details</summary>
Motivation: 患者在无监督情况下康复训练的参与度下降，现有评估系统仅提供二元分类或连续评分，不足以帮助患者改进。

Method: 采用基于骨架的运动评估方法，提出一种受HyperFormer启发的Transformer模型，用于康复训练的错误分类。

Result: 在KERAAL数据集上显著超越现有方法，并提出关节重要性计算方法。

Conclusion: 该研究为患者提供了更详细的反馈，是康复训练评估的重要进展。

Abstract: Physical rehabilitation exercises suggested by healthcare professionals can
help recovery from various musculoskeletal disorders and prevent re-injury.
However, patients' engagement tends to decrease over time without direct
supervision, which is why there is a need for an automated monitoring system.
In recent years, there has been great progress in quality assessment of
physical rehabilitation exercises. Most of them only provide a binary
classification if the performance is correct or incorrect, and a few provide a
continuous score. This information is not sufficient for patients to improve
their performance. In this work, we propose an algorithm for error
classification of rehabilitation exercises, thus making the first step toward
more detailed feedback to patients. We focus on skeleton-based exercise
assessment, which utilizes human pose estimation to evaluate motion. Inspired
by recent algorithms for quality assessment during rehabilitation exercises, we
propose a Transformer-based model for the described classification. Our model
is inspired by the HyperFormer method for human action recognition, and adapted
to our problem and dataset. The evaluation is done on the KERAAL dataset, as it
is the only medical dataset with clear error labels for the exercises, and our
model significantly surpasses state-of-the-art methods. Furthermore, we bridge
the gap towards better feedback to the patients by presenting a way to
calculate the importance of joints for each exercise.

</details>


### [242] [Towards a Multimodal Document-grounded Conversational AI System for Education](https://arxiv.org/abs/2504.13884)
*Karan Taneja,Anjali Singh,Ashok K. Goel*

Main category: cs.HC

TL;DR: MuDoC是一个基于GPT-4o的多模态对话AI系统，结合文本和图像提升学习效果，验证内容可增强学习者的信任和参与度，但对任务表现无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态对话AI在教育中的应用，解决现有文本对话系统在多媒体学习和内容验证方面的不足。

Method: 基于GPT-4o开发MuDoC系统，结合文档中的文本和视觉内容生成多模态响应，并提供内容验证功能。

Result: 多模态内容和验证功能提升了学习者的信任和参与度，但对任务表现无显著影响。

Conclusion: 多模态对话AI在教育中具有潜力，未来需进一步优化以提升学习效果。

Abstract: Multimedia learning using text and images has been shown to improve learning
outcomes compared to text-only instruction. But conversational AI systems in
education predominantly rely on text-based interactions while multimodal
conversations for multimedia learning remain unexplored. Moreover, deploying
conversational AI in learning contexts requires grounding in reliable sources
and verifiability to create trust. We present MuDoC, a Multimodal
Document-grounded Conversational AI system based on GPT-4o, that leverages both
text and visuals from documents to generate responses interleaved with text and
images. Its interface allows verification of AI generated content through
seamless navigation to the source. We compare MuDoC to a text-only system to
explore differences in learner engagement, trust in AI system, and their
performance on problem-solving tasks. Our findings indicate that both visuals
and verifiability of content enhance learner engagement and foster trust;
however, no significant impact in performance was observed. We draw upon
theories from cognitive and learning sciences to interpret the findings and
derive implications, and outline future directions for the development of
multimodal conversational AI systems in education.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [243] [Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields](https://arxiv.org/abs/2504.15262)
*Brandon Zhao,Aviad Levis,Liam Connor,Pratul P. Srinivasan,Katherine L. Bouman*

Main category: astro-ph.CO

TL;DR: 论文提出了一种利用引力约束神经场从2D望远镜图像重建3D暗物质分布的方法，克服了单视角和噪声干扰的挑战，并在模拟实验中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 暗物质的3D分布对宇宙结构定位和理论验证至关重要，但传统方法因单视角观测和噪声干扰难以实现高精度重建。

Method: 采用引力约束神经场和可微分物理前向模型，通过分析-合成方法优化神经网络权重，以模拟透镜信号。

Result: 在模拟实验中，该方法不仅优于传统方法，还能恢复潜在的意外暗物质结构。

Conclusion: 该方法为暗物质3D重建提供了灵活且高效的解决方案，有望应用于未来望远镜观测数据。

Abstract: Weak gravitational lensing is the slight distortion of galaxy shapes caused
primarily by the gravitational effects of dark matter in the universe. In our
work, we seek to invert the weak lensing signal from 2D telescope images to
reconstruct a 3D map of the universe's dark matter field. While inversion
typically yields a 2D projection of the dark matter field, accurate 3D maps of
the dark matter distribution are essential for localizing structures of
interest and testing theories of our universe. However, 3D inversion poses
significant challenges. First, unlike standard 3D reconstruction that relies on
multiple viewpoints, in this case, images are only observed from a single
viewpoint. This challenge can be partially addressed by observing how galaxy
emitters throughout the volume are lensed. However, this leads to the second
challenge: the shapes and exact locations of unlensed galaxies are unknown, and
can only be estimated with a very large degree of uncertainty. This introduces
an overwhelming amount of noise which nearly drowns out the lensing signal
completely. Previous approaches tackle this by imposing strong assumptions
about the structures in the volume. We instead propose a methodology using a
gravitationally-constrained neural field to flexibly model the continuous
matter distribution. We take an analysis-by-synthesis approach, optimizing the
weights of the neural network through a fully differentiable physical forward
model to reproduce the lensing signal present in image measurements. We
showcase our method on simulations, including realistic simulated measurements
of dark matter distributions that mimic data from upcoming telescope surveys.
Our results show that our method can not only outperform previous methods, but
importantly is also able to recover potentially surprising dark matter
structures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [244] [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
*Deyu Cao,Samin Aref*

Main category: cs.LG

TL;DR: 论文提出了一种基于ApiQ的改进方法，通过结合显著性感知正则化，在超低位量化中提升模型精度，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算需求和规模带来实际挑战，现有量化方法在精度和资源消耗间存在权衡。

Method: 结合ApiQ的部分训练与显著性感知正则化，优先保留关键参数。

Result: 在LLaMA系列模型上验证，新方法提升了精度，缩小了量化与全精度模型的差距。

Conclusion: 提出的方法在超低位量化中表现优异，未来将公开以促进相关研究。

Abstract: Large language models offer remarkable capabilities, but their size and
computational demands pose practical challenges. Quantization methods compress
their size through replacing their high-precision parameters by quantized
values of lower precision. Post-training quantization reduces model size
efficiently at the cost of decreased accuracy, while quantization-aware
training better preserves accuracy but is resource-intensive. Among existing
post-training quantization algorithms, the ApiQ method achieves superior
accuracy preservation at minimal memory and time overhead. We investigate two
ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.
First, we look into combining existing quantization-aware training techniques
with ApiQ's partial training. We show that this does not outperform the
baseline ApiQ method with limited training data and frozen weights. This leads
to two key insights: (1) The substantial representational capacity that is
gained through full retraining may not be feasible through partial training.
(2) This gain seems to depend on using a large and diverse dataset in
quantization-aware training. Second, through a novel approach informed by the
two insights, we propose an ultra-low-bit quantization method that builds upon
ApiQ and extends its performance without the need for full retraining. It
relies on a saliency-aware regularization term that prioritizes preserving the
most impactful parameters during quantization. Our experiments on benchmark
language models from the LLaMA family show that our proposed approach boosts
accuracy and tightens the gap between the quantized model and the
full-precision model, with minimal overhead. Our method will be made publicly
available to facilitate future developments in ultra-low-bit quantization of
large language models.

</details>


### [245] [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
*Cheng Qian,Emre Can Acikgoz,Qi He,Hongru Wang,Xiusi Chen,Dilek Hakkani-Tür,Gokhan Tur,Heng Ji*

Main category: cs.LG

TL;DR: 论文探讨了强化学习中奖励设计对提升大语言模型工具使用能力的作用，提出了一种定制化奖励策略，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在复杂工具使用场景中泛化能力不足，而强化学习（RL）的奖励设计面临多样性和细粒度反馈的挑战。

Method: 系统研究了奖励策略的类型、规模、粒度和时间动态，提出了一种定制化奖励设计，并采用GRPO方法训练模型。

Result: 实验表明，该方法在多个基准测试中表现优异，比基础模型提升17%，比SFT模型提升15%。

Conclusion: 合理的奖励设计对提升LLMs的工具使用能力和泛化性能至关重要，代码已开源以促进未来研究。

Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.

</details>


### [246] [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
*Amrit Diggavi Seshadri*

Main category: cs.LG

TL;DR: 论文提出了一种名为OJFA的低秩捷径方法，显著减少了推理时的参数成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型推理的时间和计算成本，同时保持参数效率。

Method: 提出了一种One-Jump-Fits-All（OJFA）低秩捷径方法，替代了传统的多捷径跳跃方式。

Result: OJFA方法在GPT2-XL、Phi3-Mini和Llama2-7B模型上实现了30倍的参数成本降低，同时性能接近多捷径跳跃方法。

Conclusion: OJFA方法在显著降低参数成本的同时，保持了模型的性能和稳定性。

Abstract: To reduce the time and computational costs of inference of large language
models, there has been interest in parameter-efficient low-rank early-exit
casting of transformer hidden-representations to final-representations. Such
low-rank short-cutting has been shown to outperform identity shortcuts at early
model stages while offering parameter-efficiency in shortcut jumps. However,
current low-rank methods maintain a separate early-exit shortcut jump to
final-representations for each transformer intermediate block-level during
inference. In this work, we propose selection of a single One-Jump-Fits-All
(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter
costs during inference. We show that despite this extreme reduction, our OJFA
choice largely matches the performance of maintaining multiple shortcut jumps
during inference and offers stable precision from all transformer block-levels
for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.

</details>


### [247] [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Hadamard矩阵的量化方法，有效减少了大型语言模型中的异常值，实现了3位量化，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在边缘设备上的部署受到其庞大参数规模的限制，量化是减少内存和推理时间的常用方法，但异常值的存在是主要挑战。

Method: 利用Hadamard矩阵的理论优势，通过渐进式二分搜索实现3位量化，适用于权重、激活和KV缓存，并扩展支持非2的幂次嵌入维度。

Result: 在Mistral、LLaMA和Qwen等模型上验证了方法的有效性，相比现有方法准确率提升40%，实现了实用的3位量化。

Conclusion: Hadamard矩阵在减少异常值方面优于随机旋转矩阵，为LLMs的低位量化提供了有效解决方案。

Abstract: Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of quantization
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving low-bit quantization.
Our method based on a gradual binary search enables 3-bit quantization for
weights, activations, and key-value (KV) caches, resulting in a 40\% increase
in accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit quantization for weights, activations, and KV cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
quantization.

</details>


### [248] [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
*Till Rossner,Ziteng Li,Jonas Balke,Nikoo Salehfard,Tom Seifert,Ming Tang*

Main category: cs.LG

TL;DR: 提出了一种基于scGPT和DeepCDR的创新方法，用于预测癌症药物反应（CDR），实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高癌症药物反应预测的准确性，探索scGPT在基因表达数据中的应用潜力。

Method: 将scGPT生成的基因表达嵌入数据作为DeepCDR的输入，结合两者优势进行预测。

Result: 实验证明该方法优于原DeepCDR和scFoundation模型，提升了预测精度。

Conclusion: scGPT嵌入数据能显著提升CDR预测效果，为现有方法提供了有前景的替代方案。

Abstract: In this study, we propose an innovative methodology for predicting Cancer
Drug Response (CDR) through the integration of the scGPT foundation model
within the DeepCDR model. Our approach utilizes scGPT to generate embeddings
from gene expression data, which are then used as gene expression input data
for DeepCDR. The experimental findings demonstrate the efficacy of this
scGPT-based method in outperforming previous related works, including the
original DeepCDR model and the scFoundation-based model. This study highlights
the potential of scGPT embeddings to enhance the accuracy of CDR predictions
and offers a promising alternative to existing approaches.

</details>


### [249] [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
*Shihan Dou,Muling Wu,Jingwen Xu,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: 提出了一种名为RRL的新算法，通过动态回放机制解决强化学习中探索效率低的问题，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习训练早期，模型虽有探索能力但无法解决问题，导致有价值的解决方案被抑制，后期能力提升后仍难以有效解决复杂问题。

Method: 提出Retrospective Replay-based Reinforcement Learning (RRL)，引入动态回放机制，让模型能重新访问早期发现的潜在解决方案。

Result: 实验表明RRL在复杂推理任务（如数学推理和代码生成）和对话任务中显著提升了探索效率和模型性能。

Conclusion: RRL不仅优化了LLM在复杂任务中的表现，还提升了RLHF的安全性和实用性。

Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in
the post-training of large language models (LLMs). The effective exploration of
the output space is essential for the success of RL. We observe that for
complex problems, during the early stages of training, the model exhibits
strong exploratory capabilities and can identify promising solution ideas.
However, its limited capability at this stage prevents it from successfully
solving these problems. The early suppression of these potentially valuable
solution ideas by the policy gradient hinders the model's ability to revisit
and re-explore these ideas later. Consequently, although the LLM's capabilities
improve in the later stages of training, it still struggles to effectively
address these complex problems. To address this exploration issue, we propose a
novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),
which introduces a dynamic replay mechanism throughout the training process.
RRL enables the model to revisit promising states identified in the early
stages, thereby improving its efficiency and effectiveness in exploration. To
evaluate the effectiveness of RRL, we conduct extensive experiments on complex
reasoning tasks, including mathematical reasoning and code generation, and
general dialogue tasks. The results indicate that RRL maintains high
exploration efficiency throughout the training period, significantly enhancing
the effectiveness of RL in optimizing LLMs for complicated reasoning tasks.
Moreover, it also improves the performance of RLHF, making the model both safer
and more helpful.

</details>


### [250] [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
*Avinandan Bose,Zhihan Xiong,Yuejie Chi,Simon Shaolei Du,Lin Xiao,Maryam Fazel*

Main category: cs.LG

TL;DR: 提出了一种基于低秩偏好建模的新框架，用于个性化大型语言模型，以适应用户多样性，优于传统RLHF方法。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF方法依赖单一价值表示，难以适应个体偏好，需更灵活的方法。

Method: 利用低维子空间表示奖励函数，通过共享基函数的加权组合建模个体偏好。

Result: 在多个偏好数据集上验证，表现出对未见用户的更好泛化能力和偏好预测准确性。

Conclusion: 该框架避免了僵化的用户分类，同时支持可扩展性和少样本适应。

Abstract: Personalizing large language models (LLMs) to accommodate diverse user
preferences is essential for enhancing alignment and user satisfaction.
Traditional reinforcement learning from human feedback (RLHF) approaches often
rely on monolithic value representations, limiting their ability to adapt to
individual preferences. We introduce a novel framework that leverages low-rank
preference modeling to efficiently learn and generalize user-specific reward
functions. By representing reward functions in a low-dimensional subspace and
modeling individual preferences as weighted combinations of shared basis
functions, our approach avoids rigid user categorization while enabling
scalability and few-shot adaptation. We validate our method on multiple
preference datasets, demonstrating superior generalization to unseen users and
improved accuracy in preference prediction tasks.

</details>


### [251] [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
*Yunhui Xia,Wei Shen,Yan Wang,Jason Klein Liu,Huifeng Sun,Siyue Wu,Jian Hu,Xiaolong Xu*

Main category: cs.LG

TL;DR: LeetCodeDataset是一个高质量的代码生成模型评估和训练基准，解决了LLM研究中缺乏推理导向的编码基准和自包含训练测试平台的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM研究中缺乏推理导向的编码基准和自包含训练测试平台的挑战。

Method: 通过整理LeetCode Python问题，提供丰富的元数据、广泛覆盖、每个问题100+测试用例和时间分割（2024年7月前后），支持无污染评估和高效监督微调（SFT）。

Result: 实验显示推理模型显著优于非推理模型，仅用2.6K模型生成的解决方案进行SFT即可达到与110K样本相当的性能。

Conclusion: LeetCodeDataset及其评估框架已在Hugging Face和Github上发布，为代码生成研究提供了高效工具。

Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
training code-generation models, addressing two key challenges in LLM research:
the lack of reasoning-focused coding benchmarks and self-contained training
testbeds. By curating LeetCode Python problems with rich metadata, broad
coverage, 100+ test cases per problem, and temporal splits (pre/post July
2024), our dataset enables contamination-free evaluation and efficient
supervised fine-tuning (SFT). Experiments show reasoning models significantly
outperform non-reasoning counterparts, while SFT with only 2.6K model-generated
solutions achieves performance comparable to 110K-sample counterparts. The
dataset and evaluation framework are available on Hugging Face and Github.

</details>


### [252] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
*Jianhao Yan,Yafu Li,Zican Hu,Zhi Wang,Ganqu Cui,Xiaoye Qu,Yu Cheng,Yue Zhang*

Main category: cs.LG

TL;DR: LUFFY框架通过结合离策略推理轨迹和策略内训练，提升了推理模型的泛化能力，在数学基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有零强化学习方法局限于策略内训练，无法超越初始能力，需要一种新方法来结合离策略推理。

Method: LUFFY框架动态平衡模仿与探索，使用正则化重要性采样避免浅层模仿，结合离策略演示和策略内训练。

Result: LUFFY在六个数学基准测试中平均提升7.0分，在分布外任务中优势超过6.2分，显著优于模仿监督微调。

Conclusion: LUFFY不仅有效模仿，还能超越演示数据探索，为训练泛化推理模型提供了可扩展路径。

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning (RL) with simple rule-based rewards. However,
existing zero-RL approaches are inherently ``on-policy'', limiting learning to
a model's own outputs and failing to acquire reasoning abilities beyond its
initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY
guidance), a framework that augments zero-RL with off-policy reasoning traces.
LUFFY dynamically balances imitation and exploration by combining off-policy
demonstrations with on-policy rollouts during training. Notably, we propose
policy shaping via regularized importance sampling to avoid superficial and
rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an
over +7.0 average gain across six math benchmarks and an advantage of over +6.2
points in out-of-distribution tasks. It also substantially surpasses
imitation-based supervised fine-tuning (SFT), particularly in generalization.
Analysis shows LUFFY not only imitates effectively but also explores beyond
demonstrations, offering a scalable path to train generalizable reasoning
models with off-policy guidance.

</details>


### [253] [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
*Vaishnavh Nagarajan,Chen Henry Wu,Charles Ding,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 设计了一套最小算法任务，用于量化语言模型的创造性极限，发现多标记方法优于单标记学习，并提出输入层噪声注入优于输出层温度采样。


<details>
  <summary>Details</summary>
Motivation: 量化语言模型在开放任务中的创造性表现，探索其局限性。

Method: 设计抽象任务，比较单标记与多标记学习方法，提出输入层噪声注入技术。

Result: 多标记方法（如无教师训练和扩散模型）表现更优，输入层噪声注入能提升随机性而不损害连贯性。

Conclusion: 为分析开放创造性技能提供了测试平台，支持超越单标记学习和基于softmax的采样方法。

Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction
of open-ended real-world tasks. This allows us to cleanly and controllably
quantify the creative limits of the present-day language model. Much like
real-world tasks that require a creative, far-sighted leap of thought, our
tasks require an implicit, open-ended stochastic planning step that either (a)
discovers new connections in an abstract knowledge graph (like in wordplay,
drawing analogies, or research) or (b) constructs new patterns (like in
designing math problems or new proteins). In these tasks, we empirically and
conceptually argue how next-token learning is myopic and memorizes excessively;
comparatively, multi-token approaches, namely teacherless training and
diffusion models, excel in producing diverse and original output. Secondly, in
our tasks, we find that to elicit randomness from the Transformer without
hurting coherence, it is better to inject noise right at the input layer (via a
method we dub hash-conditioning) rather than defer to temperature sampling from
the output layer. Thus, our work offers a principled, minimal test-bed for
analyzing open-ended creative skills, and offers new arguments for going beyond
next-token learning and softmax-based sampling. We make part of the code
available under https://github.com/chenwu98/algorithmic-creativity

</details>


### [254] [Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing](https://arxiv.org/abs/2504.13975)
*Mehmet Yamaç,Muhammad Numan Yousaf,Serkan Kiranyaz,Moncef Gabbouj*

Main category: cs.LG

TL;DR: 论文提出了一种新型神经网络算子MTS，通过多尺度张量求和提升效率，优于传统MLP和CNN。


<details>
  <summary>Details</summary>
Motivation: 传统MLP和CNN在高维输入输出或有限感受野下效率受限，需改进。

Method: 引入MTS算子，基于Tucker分解的多尺度张量求和，结合MHG非线性单元。

Result: MTSNet在分类、压缩等任务中表现优异，复杂度与性能优于现有Transformer。

Conclusion: MTS作为一种新骨干层，显著提升效率与性能，代码已开源。

Abstract: Multilayer perceptrons (MLP), or fully connected artificial neural networks,
are known for performing vector-matrix multiplications using learnable weight
matrices; however, their practical application in many machine learning tasks,
especially in computer vision, can be limited due to the high dimensionality of
input-output pairs at each layer. To improve efficiency, convolutional
operators have been utilized to facilitate weight sharing and local
connections, yet they are constrained by limited receptive fields. In this
paper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel
neural network operator that implements tensor summation at multiple scales,
where each tensor to be summed is obtained through Tucker-decomposition-like
mode products. Unlike other tensor decomposition methods in the literature, MTS
is not introduced as a network compression tool; instead, as a new backbone
neural layer. MTS not only reduces the number of parameters required while
enhancing the efficiency of weight optimization compared to traditional dense
layers (i.e., unfactorized weight matrices in MLP layers), but it also
demonstrates clear advantages over convolutional layers. The proof-of-concept
experimental comparison of the proposed MTS networks with MLPs and
Convolutional Neural Networks (CNNs) demonstrates their effectiveness across
various tasks, such as classification, compression, and signal restoration.
Additionally, when integrated with modern non-linear units such as the
multi-head gate (MHG), also introduced in this study, the corresponding neural
network, MTSNet, demonstrates a more favorable complexity-performance tradeoff
compared to state-of-the-art transformers in various computer vision
applications. The software implementation of the MTS layer and the
corresponding MTS-based networks, MTSNets, is shared at
https://github.com/mehmetyamac/MTSNet.

</details>


### [255] [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://arxiv.org/abs/2504.14662)
*Yeoreum Lee,Jinwook Jung,Sungyong Baik*

Main category: cs.LG

TL;DR: 论文提出了一种新的微调目标函数，旨在减少参数干扰并提升单任务性能，基于锐度感知最小化（SAM）方法。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型的预训练-微调范式导致了许多任务特定模型的出现，但合并这些模型时存在参数干扰问题。现有方法在减少干扰的同时牺牲了性能，因此需要一种新的微调方案。

Method: 设计了一种新的微调目标函数，类似于锐度感知最小化（SAM），通过寻找平坦最小值来减少干扰并提升性能。

Result: 实验和理论结果表明，该方法在合并和微调任务中表现优异，性能优于现有方法。

Conclusion: 通过锐度感知最小化微调预训练模型，有效减少了参数干扰并提升了合并模型的性能。

Abstract: Large-scale deep learning models with a pretraining-finetuning paradigm have
led to a surge of numerous task-specific models fine-tuned from a common
pre-trained model. Recently, several research efforts have been made on merging
these large models into a single multi-task model, particularly with simple
arithmetic on parameters. Such merging methodology faces a central challenge:
interference between model parameters fine-tuned on different tasks. Few recent
works have focused on designing a new fine-tuning scheme that can lead to small
parameter interference, however at the cost of the performance of each
task-specific fine-tuned model and thereby limiting that of a merged model. To
improve the performance of a merged model, we note that a fine-tuning scheme
should aim for (1) smaller parameter interference and (2) better performance of
each fine-tuned model on the corresponding task. In this work, we aim to design
a new fine-tuning objective function to work towards these two goals. In the
course of this process, we find such objective function to be strikingly
similar to sharpness-aware minimization (SAM) objective function, which aims to
achieve generalization by finding flat minima. Drawing upon our observation, we
propose to fine-tune pre-trained models via sharpness-aware minimization. The
experimental and theoretical results showcase the effectiveness and
orthogonality of our proposed approach, improving performance upon various
merging and fine-tuning methods. Our code is available at
https://github.com/baiklab/SAFT-Merge.

</details>


### [256] [Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning](https://arxiv.org/abs/2504.14727)
*Geng Liu,Fei Zhu,Rong Feng,Zhiqiang Yi,Shiqi Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种仿生持续学习框架，结合半参数记忆和醒睡巩固机制，解决了深度神经网络在持续学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在持续学习中容易忘记已学知识，而人类和动物却能持续学习新知识，因此需要模仿生物智能来解决这一问题。

Method: 提出了一种仿生持续学习框架，整合半参数记忆和醒睡巩固机制。

Result: 在真实世界的持续学习场景（如ImageNet的类增量学习）中，该方法能够保持新任务的高性能并保留先验知识。

Conclusion: 模仿生物智能是赋予深度神经网络持续学习能力的一条有前景的路径。

Abstract: Humans and most animals inherently possess a distinctive capacity to
continually acquire novel experiences and accumulate worldly knowledge over
time. This ability, termed continual learning, is also critical for deep neural
networks (DNNs) to adapt to the dynamically evolving world in open
environments. However, DNNs notoriously suffer from catastrophic forgetting of
previously learned knowledge when trained on sequential tasks. In this work,
inspired by the interactive human memory and learning system, we propose a
novel biomimetic continual learning framework that integrates semi-parametric
memory and the wake-sleep consolidation mechanism. For the first time, our
method enables deep neural networks to retain high performance on novel tasks
while maintaining prior knowledge in real-world challenging continual learning
scenarios, e.g., class-incremental learning on ImageNet. This study
demonstrates that emulating biological intelligence provides a promising path
to enable deep neural networks with continual learning capabilities.

</details>


### [257] [Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](https://arxiv.org/abs/2504.14798)
*Hao Xuan,Xingyu Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为“稳健遗忘”的概念，并通过“遗忘映射攻击”（UMA）验证现有遗忘技术是否满足安全标准。实验表明，现有技术仍存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘（MUL）对隐私保护和内容监管至关重要，但现有验证方法无法检测残留信息泄露。

Method: 提出“稳健遗忘”概念，并设计UMA框架，通过对抗性查询主动探测模型中的遗忘痕迹。

Result: 实验证明现有遗忘技术即使通过现有验证指标，仍存在漏洞。

Conclusion: UMA为评估和提升机器遗忘安全性设立了新标准。

Abstract: Machine Unlearning (MUL) is crucial for privacy protection and content
regulation, yet recent studies reveal that traces of forgotten information
persist in unlearned models, enabling adversaries to resurface removed
knowledge. Existing verification methods only confirm whether unlearning was
executed, failing to detect such residual information leaks. To address this,
we introduce the concept of Robust Unlearning, ensuring models are
indistinguishable from retraining and resistant to adversarial recovery. To
empirically evaluate whether unlearning techniques meet this security standard,
we propose the Unlearning Mapping Attack (UMA), a post-unlearning verification
framework that actively probes models for forgotten traces using adversarial
queries. Extensive experiments on discriminative and generative tasks show that
existing unlearning techniques remain vulnerable, even when passing existing
verification metrics. By establishing UMA as a practical verification tool,
this study sets a new standard for assessing and enhancing machine unlearning
security.

</details>


### [258] [A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions](https://arxiv.org/abs/2504.14800)
*Shuxian Zhao,Jie Gui,Minjing Dong,Baosheng Yu,Zhipeng Gui,Lu Dong,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.LG

TL;DR: 本文提出了一种系统性分析框架，用于解决小样本不平衡（S&I）问题，强调数据特征分析的重要性，并总结了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 小样本不平衡问题是机器学习和数据分析中的主要挑战，现有方法多依赖启发式算法，缺乏对数据特征的深入分析。

Method: 提出系统性分析框架，总结不平衡指标和复杂度分析方法，并回顾针对不同数据分布的解决方案。

Result: 实验表明，分类器性能差异远超重采样带来的改进。

Conclusion: 本文强调了数据视角分析的必要性，并讨论了未来研究方向。

Abstract: The small sample imbalance (S&I) problem is a major challenge in machine
learning and data analysis. It is characterized by a small number of samples
and an imbalanced class distribution, which leads to poor model performance. In
addition, indistinct inter-class feature distributions further complicate
classification tasks. Existing methods often rely on algorithmic heuristics
without sufficiently analyzing the underlying data characteristics. We argue
that a detailed analysis from the data perspective is essential before
developing an appropriate solution. Therefore, this paper proposes a systematic
analytical framework for the S\&I problem. We first summarize imbalance metrics
and complexity analysis methods, highlighting the need for interpretable
benchmarks to characterize S&I problems. Second, we review recent solutions for
conventional, complexity-based, and extreme S&I problems, revealing
methodological differences in handling various data distributions. Our summary
finds that resampling remains a widely adopted solution. However, we conduct
experiments on binary and multiclass datasets, revealing that classifier
performance differences significantly exceed the improvements achieved through
resampling. Finally, this paper highlights open questions and discusses future
trends.

</details>


### [259] [What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale](https://arxiv.org/abs/2504.14815)
*Xiaoyong Yuan,Xiaolong Ma,Linke Guo,Lan Zhang*

Main category: cs.LG

TL;DR: PAIA是一种新型的无提示、无图像的概念审计框架，用于检测微调扩散模型是否学习了特定目标概念，显著提高了审计效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着微调扩散模型的广泛共享，其可能生成敏感或未经授权的内容，引发伦理和法律问题，但目前缺乏有效的审计工具。

Method: 提出Prompt-Agnostic Image-Free Auditing (PAIA)，通过直接分析模型内部行为，避免依赖优化提示或生成图像。

Result: 在320个控制模型和690个真实社区模型上测试，PAIA检测准确率超过90%，审计时间减少18-40倍。

Conclusion: PAIA是首个可扩展且实用的扩散模型概念审计解决方案，为模型共享提供了更安全、透明的基础。

Abstract: Diffusion models (DMs) have revolutionized text-to-image generation, enabling
the creation of highly realistic and customized images from text prompts. With
the rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users
can now customize powerful pre-trained models using minimal computational
resources. However, the widespread sharing of fine-tuned DMs on open platforms
raises growing ethical and legal concerns, as these models may inadvertently or
deliberately generate sensitive or unauthorized content, such as copyrighted
material, private individuals, or harmful content. Despite the increasing
regulatory attention on generative AI, there are currently no practical tools
for systematically auditing these models before deployment. In this paper, we
address the problem of concept auditing: determining whether a fine-tuned DM
has learned to generate a specific target concept. Existing approaches
typically rely on prompt-based input crafting and output-based image
classification but suffer from critical limitations, including prompt
uncertainty, concept drift, and poor scalability. To overcome these challenges,
we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric
concept auditing framework. By treating the DM as the object of inspection,
PAIA enables direct analysis of internal model behavior, bypassing the need for
optimized prompts or generated images. We evaluate PAIA on 320 controlled model
and 690 real-world community models sourced from a public DM sharing platform.
PAIA achieves over 90% detection accuracy while reducing auditing time by
18-40x compared to existing baselines. To our knowledge, PAIA is the first
scalable and practical solution for pre-deployment concept auditing of
diffusion models, providing a practical foundation for safer and more
transparent diffusion model sharing.

</details>


### [260] [Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness](https://arxiv.org/abs/2504.14882)
*Mojtaba Kolahdouzi,Hatice Gunes,Ali Etemad*

Main category: cs.LG

TL;DR: 研究优化算法选择对深度神经网络群体公平性的影响，发现自适应优化器（如RMSProp）比随机优化器（如SGD）更易收敛到公平解，并通过理论和实验验证。


<details>
  <summary>Details</summary>
Motivation: 探讨优化算法选择是否及如何影响深度神经网络的群体公平性，特别是在数据严重不平衡的情况下。

Method: 通过随机微分方程分析优化动态，比较自适应优化器（如RMSProp）和随机优化器（如SGD）的公平性表现，并进行理论推导和实验验证。

Result: 自适应优化器在多个数据集和任务中表现出更高的群体公平性，同时保持预测准确性。

Conclusion: 自适应优化器的更新机制是促进公平结果的关键因素，这一发现为公平性研究提供了新视角。

Abstract: We study whether and how the choice of optimization algorithm can impact
group fairness in deep neural networks. Through stochastic differential
equation analysis of optimization dynamics in an analytically tractable setup,
we demonstrate that the choice of optimization algorithm indeed influences
fairness outcomes, particularly under severe imbalance. Furthermore, we show
that when comparing two categories of optimizers, adaptive methods and
stochastic methods, RMSProp (from the adaptive category) has a higher
likelihood of converging to fairer minima than SGD (from the stochastic
category). Building on this insight, we derive two new theoretical guarantees
showing that, under appropriate conditions, RMSProp exhibits fairer parameter
updates and improved fairness in a single optimization step compared to SGD. We
then validate these findings through extensive experiments on three publicly
available datasets, namely CelebA, FairFace, and MS-COCO, across different
tasks as facial expression recognition, gender classification, and multi-label
classification, using various backbones. Considering multiple fairness
definitions including equalized odds, equal opportunity, and demographic
parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD
in terms of group fairness, while maintaining comparable predictive accuracy.
Our results highlight the role of adaptive updates as a crucial yet overlooked
mechanism for promoting fair outcomes.

</details>


### [261] [VeLU: Variance-enhanced Learning Unit for Deep Neural Networks](https://arxiv.org/abs/2504.15051)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicolè,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: VeLU是一种基于输入方差动态调整的激活函数，通过ArcTan-Sin变换和Wasserstein-2正则化，解决了ReLU及其替代品的局限性。


<details>
  <summary>Details</summary>
Motivation: ReLU及其替代品（如Swish和GELU）在梯度消失和动态适应性方面存在不足，需要一种更灵活的激活函数。

Method: VeLU结合ArcTan-Sin变换和Wasserstein-2正则化，动态调整输入方差以优化梯度流和稳定性。

Result: 在多个视觉基准测试中，VeLU表现优于ReLU、ReLU6、Swish和GELU。

Conclusion: VeLU通过动态调整输入方差，显著提升了深度神经网络的优化稳定性和泛化能力。

Abstract: Activation functions are fundamental in deep neural networks and directly
impact gradient flow, optimization stability, and generalization. Although ReLU
remains standard because of its simplicity, it suffers from vanishing gradients
and lacks adaptability. Alternatives like Swish and GELU introduce smooth
transitions, but fail to dynamically adjust to input statistics. We propose
VeLU, a Variance-enhanced Learning Unit as an activation function that
dynamically scales based on input variance by integrating ArcTan-Sin
transformations and Wasserstein-2 regularization, effectively mitigating
covariate shifts and stabilizing optimization. Extensive experiments on
ViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm
VeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.
The codes of VeLU are publicly available on GitHub.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [262] [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
*Natalia Tomashenko,Xiaoxiao Miao,Emmanuel Vincent,Junichi Yamagishi*

Main category: eess.AS

TL;DR: 首届VoicePrivacy攻击者挑战赛旨在评估攻击系统对语音匿名化系统的效果，最佳攻击系统将EER降低了25-44%。


<details>
  <summary>Details</summary>
Motivation: 评估攻击系统对语音匿名化系统的性能，推动相关技术的发展。

Method: 提供训练、开发和评估数据集及基线攻击系统，参与者开发自动说话人验证系统并提交分数。

Result: 最佳攻击系统将EER相对基线降低了25-44%。

Conclusion: 挑战赛成功展示了攻击系统的改进潜力，为语音隐私保护提供了重要参考。

Abstract: The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand
Challenge which focuses on evaluating attacker systems against a set of voice
anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,
development, and evaluation datasets were provided along with a baseline
attacker. Participants developed their attacker systems in the form of
automatic speaker verification systems and submitted their scores on the
development and evaluation data. The best attacker systems reduced the equal
error rate (EER) by 25-44% relative w.r.t. the baseline.

</details>


### [263] [Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training](https://arxiv.org/abs/2504.14409)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François G. Germain,Jonathan Le Roux*

Main category: eess.AS

TL;DR: MERL提出了一种基于神经声场的RIR估计系统，用于ICASSP 2025的数据增强任务。


<details>
  <summary>Details</summary>
Motivation: 解决房间脉冲响应（RIR）估计和说话者距离估计的数据增强问题。

Method: 预训练神经声场模型，利用外部数据集和房间几何信息进行适配，并预测RIR以训练距离估计模型。

Result: 成功生成RIR并用于改进说话者距离估计。

Conclusion: 该方法有效结合了神经声场和几何信息，为RIR估计和距离估计提供了新思路。

Abstract: This report details MERL's system for room impulse response (RIR) estimation
submitted to the Generative Data Augmentation Workshop at ICASSP 2025 for
Augmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task
2). We first pre-train a neural acoustic field conditioned by room geometry on
an external large-scale dataset in which pairs of RIRs and the geometries are
provided. The neural acoustic field is then adapted to each target room by
using the enrollment data, where we leverage either the provided room
geometries or geometries retrieved from the external dataset, depending on
availability. Lastly, we predict the RIRs for each pair of source and receiver
locations specified by Task 1, and use these RIRs to train the speaker distance
estimation model in Task 2.

</details>


### [264] [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://arxiv.org/abs/2504.14906)
*Huadai Liu,Tianyi Luo,Qikai Jiang,Kaicheng Luo,Peiwen Sun,Jialei Wan,Rongjie Huang,Qian Chen,Wen Wang,Xiangtai Li,Shiliang Zhang,Zhijie Yan,Zhou Zhao,Wei Xue*

Main category: eess.AS

TL;DR: 该论文提出了一种新任务360V2SA，旨在从360度视频生成空间音频，并提出了OmniAudio框架和Sphere360数据集，实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频到音频生成技术主要关注视场视频和非空间音频，缺乏3D环境中声音源的空间线索。

Method: 提出了OmniAudio框架，利用自监督预训练和双分支结构处理全景和视场视频输入，生成空间音频。

Result: OmniAudio在Sphere360数据集上实现了最先进的性能。

Conclusion: 该研究为360度视频生成空间音频提供了有效解决方案，并公开了代码和数据集。

Abstract: Traditional video-to-audio generation techniques primarily focus on
field-of-view (FoV) video and non-spatial audio, often missing the spatial cues
necessary for accurately representing sound sources in 3D environments. To
address this limitation, we introduce a novel task, 360V2SA, to generate
spatial audio from 360-degree videos, specifically producing First-order
Ambisonics (FOA) audio - a standard format for representing 3D spatial audio
that captures sound directionality and enables realistic 3D audio reproduction.
We first create Sphere360, a novel dataset tailored for this task that is
curated from real-world data. We also design an efficient semi-automated
pipeline for collecting and cleaning paired video-audio data. To generate
spatial audio from 360-degree video, we propose a novel framework OmniAudio,
which leverages self-supervised pre-training using both spatial audio data (in
FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a
dual-branch framework that utilizes both panoramic and FoV video inputs to
capture comprehensive local and global information from 360-degree videos.
Experimental results demonstrate that OmniAudio achieves state-of-the-art
performance across both objective and subjective metrics on Sphere360. Code and
datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo
page is available at https://OmniAudio-360V2SA.github.io.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [265] [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
*Akash V. Maharaj,David Arbour,Daniel Lee,Uttaran Bhattacharya,Anup Rao,Austin Zane,Avi Feller,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 本文提出了一种用于监控、基准测试和持续改进企业AI助手的综合框架，包含错误检测、基准构建和持续改进策略。


<details>
  <summary>Details</summary>
Motivation: 企业AI助手在关键领域部署时，错误输出可能导致重大事故，因此需要系统化的方法来提高其可靠性和性能。

Method: 框架包括三部分：(1) 分层错误检测框架，(2) 可扩展的基准构建方法，(3) 多维评估的持续改进策略。

Result: 通过该框架，企业可以系统化提升AI助手的可靠性和性能。

Conclusion: 该多维度评估方法为增强AI系统的鲁棒性和可信度提供了新途径。

Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy
is paramount, making each erroneous output a potentially significant incident.
This paper presents a comprehensive framework for monitoring, benchmarking, and
continuously improving such complex, multi-component systems under active
development by multiple teams. Our approach encompasses three key elements: (1)
a hierarchical ``severity'' framework for incident detection that identifies
and categorizes errors while attributing component-specific error rates,
facilitating targeted improvements; (2) a scalable and principled methodology
for benchmark construction, evaluation, and deployment, designed to accommodate
multiple development teams, mitigate overfitting risks, and assess the
downstream impact of system modifications; and (3) a continual improvement
strategy leveraging multidimensional evaluation, enabling the identification
and implementation of diverse enhancement opportunities. By adopting this
holistic framework, organizations can systematically enhance the reliability
and performance of their AI Assistants, ensuring their efficacy in critical
enterprise environments. We conclude by discussing how this multifaceted
evaluation approach opens avenues for various classes of enhancements, paving
the way for more robust and trustworthy AI systems.

</details>


### [266] [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
*Jennifer Hu,Michael A. Lepori,Michael Franke*

Main category: cs.AI

TL;DR: 研究探讨了Transformer模型的层时动态是否能够预测人类实时处理特征，发现其比模型输出更具预测力。


<details>
  <summary>Details</summary>
Motivation: 探索AI模型内部处理策略与人类认知过程的相似性，以更深入地理解人类认知。

Method: 通过五个跨领域和模态的研究，比较Transformer模型的层时动态与人类实时处理特征。

Result: 层时动态在模型输出基础上提供了额外的预测能力。

Conclusion: Transformer处理与人类处理可能受相似输入特性影响，为AI模型作为认知研究工具提供了新视角。

Abstract: Modern AI models are increasingly being used as theoretical tools to study
human cognition. One dominant approach is to evaluate whether human-derived
measures (such as offline judgments or real-time processing) are predicted by a
model's output: that is, the end-product of forward pass(es) through the
network. At the same time, recent advances in mechanistic interpretability have
begun to reveal the internal processes that give rise to model outputs, raising
the question of whether models and humans might arrive at outputs using similar
"processing strategies". Here, we investigate the link between real-time
processing in humans and "layer-time" dynamics in Transformer models. Across
five studies spanning domains and modalities, we test whether the dynamics of
computation in a single forward pass of pre-trained Transformers predict
signatures of processing in humans, above and beyond properties of the model's
output probability distribution. We consistently find that layer-time dynamics
provide additional predictive power on top of output measures. Our results
suggest that Transformer processing and human processing may be facilitated or
impeded by similar properties of an input stimulus, and this similarity has
emerged through general-purpose objectives such as next-token prediction or
image recognition. Our work suggests a new way of using AI models to study
human cognition: not just as a black box mapping stimuli to responses, but
potentially also as explicit processing models.

</details>


### [267] [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
*Mingyu Kim,Jongwoo Ko,Mijung Park*

Main category: cs.AI

TL;DR: 提出了一种基于贝叶斯学习原理的新训练目标函数，以平衡提示学习的适应性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法容易过拟合微调数据，泛化性差。

Method: 通过贝叶斯学习原理，设计了一个先验对数函数，其均值由预训练模型参数化，后验对应微调模型。

Result: 新目标函数使微调模型在适应下游任务的同时保持接近预训练模型。

Conclusion: 该方法有效平衡了适应性和泛化性，提升了提示学习的性能。

Abstract: Prompt learning is a popular fine-tuning method for vision-language models
due to its efficiency. It requires a small number of additional learnable
parameters while significantly enhancing performance on target tasks. However,
most existing methods suffer from overfitting to fine-tuning data, yielding
poor generalizability. To address this, we propose a new training objective
function based on a Bayesian learning principle to balance adaptability and
generalizability. We derive a prior over the logits, where the mean function is
parameterized by the pre-trained model, while the posterior corresponds to the
fine-tuned model. This objective establishes a balance by allowing the
fine-tuned model to adapt to downstream tasks while remaining close to the
pre-trained model.

</details>


### [268] [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
*Saad Hameed,Basheer Qolomany,Samir Brahim Belhaouari,Mohamed Abdallah,Junaid Qadir,Ala Al-Fuqaha*

Main category: cs.AI

TL;DR: 该论文提出了一种结合LLMs（如ChatGPT-3.5和Llama3）与PSO的方法，用于深度学习超参数调优，显著提高了收敛速度并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型架构设计依赖人工调优或高计算成本的优化方法，现有方法效率不足。

Method: 将LLMs集成到PSO中，利用LLMs的建议替换表现不佳的粒子位置，加速搜索空间探索。

Result: 实验表明，该方法在三种场景下（Rastrigin函数优化、LSTM时间序列回归、CNN材料分类）显著提升收敛速度，计算复杂度降低20%-60%。

Conclusion: 该方法为深度学习模型优化提供了高效解决方案，适用于多种应用场景。

Abstract: Determining the ideal architecture for deep learning models, such as the
number of layers and neurons, is a difficult and resource-intensive process
that frequently relies on human tuning or computationally costly optimization
approaches. While Particle Swarm Optimization (PSO) and Large Language Models
(LLMs) have been individually applied in optimization and deep learning, their
combined use for enhancing convergence in numerical optimization tasks remains
underexplored. Our work addresses this gap by integrating LLMs into PSO to
reduce model evaluations and improve convergence for deep learning
hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the
difficulties of efficiency and convergence by using LLMs (particularly
ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster
achievement of target objectives. Our method speeds up search space exploration
by substituting underperforming particle placements with best suggestions
offered by LLMs. Comprehensive experiments across three scenarios -- (1)
optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)
networks for time series regression, and (3) using Convolutional Neural
Networks (CNNs) for material classification -- show that the method
significantly improves convergence rates and lowers computational costs.
Depending on the application, computational complexity is lowered by 20% to 60%
compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in
model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by
60% for both regression and classification tasks, all while preserving accuracy
and error rates. This groundbreaking methodology offers a very efficient and
effective solution for optimizing deep learning models, leading to substantial
computational performance improvements across a wide range of applications.

</details>


### [269] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui,Xingdi Yuan,Zhang Xiao,Prithviraj Ammanabrolu,Marc-Alexandre Côté*

Main category: cs.AI

TL;DR: TALES是一个多样化的文本冒险游戏集合，旨在挑战和评估大型语言模型（LLMs）的推理能力。尽管在合成游戏中表现优异，但顶级LLM在人类设计的游戏中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度的增加，需要更复杂的推理能力来支持顺序决策，因此需要评估LLMs的多样化推理能力。

Method: 使用TALES（合成和人类编写的文本冒险游戏）对多种LLMs（开源和闭源）进行测试，并进行定性分析。

Result: 顶级LLM在合成游戏中表现良好，但在人类设计的游戏中成功率低于15%。

Conclusion: TALES为评估LLMs的推理能力提供了有效工具，但LLMs在复杂人类任务中仍有改进空间。

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>


### [270] [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
*Li He,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为DAR的简单对齐算法，利用在线AI奖励优化策略改进，替代传统的RLHF方法，减少了实现复杂度并提高了学习效率。


<details>
  <summary>Details</summary>
Motivation: 在线AI反馈（OAIF）虽然是一种有前景的替代RLHF的方法，但直接替换人类反馈会限制语言模型学习更细粒度的AI监督信号。

Method: 提出Direct Advantage Regression（DAR），一种基于在线AI奖励的RL-free对齐算法，通过加权监督微调优化策略改进。

Result: 实验表明，AI奖励比AI偏好更有效，DAR在GPT-4-Turbo和MT-bench评估中优于OAIF和在线RLHF基线。

Conclusion: DAR是一种高效且理论一致的对齐算法，能够显著提升语言模型的性能。

Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement
Learning from Human Feedback (RLHF) by utilizing online AI preference in
aligning language models (LLMs). However, the straightforward replacement of
humans with AI deprives LLMs from learning more fine-grained AI supervision
beyond binary signals. In this paper, we propose Direct Advantage Regression
(DAR), a simple alignment algorithm using online AI reward to optimize policy
improvement through weighted supervised fine-tuning. As an RL-free approach,
DAR maintains theoretical consistency with online RLHF pipelines while
significantly reducing implementation complexity and improving learning
efficiency. Our empirical results underscore that AI reward is a better form of
AI supervision consistently achieving higher human-AI agreement as opposed to
AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show
that DAR outperforms both OAIF and online RLHF baselines.

</details>


### [271] [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
*Yansheng Qiu,Haoquan Zhang,Zhaopan Xu,Ming Li,Diping Song,Zheng Wang,Kaipeng Zhang*

Main category: cs.AI

TL;DR: AI Idea Bench 2025是一个评估LLM生成AI研究想法的框架，解决了现有评估方法的不足，包括知识泄漏和缺乏开放基准。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成想法的评估存在知识泄漏、缺乏开放基准和可行性分析受限等问题，限制了突破性研究的发现。

Method: 提出AI Idea Bench 2025框架，包含3,495篇AI论文数据集和评估方法，从原创内容对齐和通用参考材料两个维度评估想法质量。

Result: 该框架为评估和比较LLM生成想法的技术提供了资源，支持科学发现的自动化。

Conclusion: AI Idea Bench 2025是一个有价值的工具，有助于提升LLM在科学发现中的潜力。

Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction
and achieved significant success in the generation of novel ideas. However,
current assessments of idea generation overlook crucial factors such as
knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded
truth, and the limited scope of feasibility analysis constrained by prompt
design. These limitations hinder the potential of uncovering groundbreaking
research ideas. In this paper, we present AI Idea Bench 2025, a framework
designed to quantitatively evaluate and compare the ideas generated by LLMs
within the domain of AI research from diverse perspectives. The framework
comprises a comprehensive dataset of 3,495 AI papers and their associated
inspired works, along with a robust evaluation methodology. This evaluation
system gauges idea quality in two dimensions: alignment with the ground-truth
content of the original papers and judgment based on general reference
material. AI Idea Bench 2025's benchmarking system stands to be an invaluable
resource for assessing and comparing idea-generation techniques, thereby
facilitating the automation of scientific discovery.

</details>


### [272] [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
*Antoun Yaacoub,Jérôme Da-Rugna,Zainab Assaghir*

Main category: cs.AI

TL;DR: 研究评估了将布鲁姆分类法整合到AI驱动的Moodle插件OneClickQuiz中，以改进多选题生成与认知目标的匹配。结果显示，高级认知水平问题更复杂，DistilBERT模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨布鲁姆分类法是否能提升AI生成问题与特定认知目标的匹配度。

Method: 开发了3691个按布鲁姆分类法分类的问题数据集，并测试多种分类模型（如逻辑回归、朴素贝叶斯、线性SVC和DistilBERT）。

Result: 高级认知水平问题更复杂，DistilBERT表现最佳（验证准确率91%）。

Conclusion: 布鲁姆分类法整合到AI工具有潜力，高级模型如DistilBERT能显著提升教育内容生成质量。

Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,
an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice
Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured
framework for categorizing educational objectives into hierarchical cognitive
levels. Our research investigates whether incorporating this taxonomy can
improve the alignment of AI-generated questions with specific cognitive
objectives. We developed a dataset of 3691 questions categorized according to
Bloom's levels and employed various classification models-Multinomial Logistic
Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a
Transformer-based model (DistilBERT)-to evaluate their effectiveness in
categorizing questions. Our results indicate that higher Bloom's levels
generally correlate with increased question length, Flesch-Kincaid Grade Level
(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher
cognitive demands. Multinomial Logistic Regression showed varying accuracy
across Bloom's levels, performing best for "Knowledge" and less accurately for
higher-order levels. Merging higher-level categories improved accuracy for
complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective
classification for lower levels but struggled with higher-order tasks.
DistilBERT achieved the highest performance, significantly improving
classification of both lower and higher-order cognitive levels, achieving an
overall validation accuracy of 91%. This study highlights the potential of
integrating Bloom's Taxonomy into AI-driven assessment tools and underscores
the advantages of advanced models like DistilBERT for enhancing educational
content generation.

</details>


### [273] [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
*Yuhang Liu,Pengxiang Li,Congkai Xie,Xavier Hu,Xiaotian Han,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: InfiGUI-R1是一个基于MLLM的GUI代理，通过两阶段训练框架Actor2Reasoner，从反应性执行者转变为深思熟虑的推理者，提升了GUI任务的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理多依赖手动设计的推理模板或隐式推理，缺乏对复杂环境的适应性和深度规划能力。

Method: 采用两阶段训练：1. 推理注入（通过空间推理蒸馏建立基础推理能力）；2. 深思熟虑增强（通过强化学习优化推理，包括子目标引导和错误恢复场景构建）。

Result: 实验表明InfiGUI-R1在GUI基础和轨迹任务中表现优异。

Conclusion: 通过从反应性执行转向深思熟虑推理，InfiGUI-R1显著提升了GUI代理的性能和适应性。

Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User
Interface (GUI) Agents, showing promise in automating tasks on computing
devices. Recent works have begun exploring reasoning in GUI tasks with
encouraging results. However, many current approaches rely on manually designed
reasoning templates, which may result in reasoning that is not sufficiently
robust and adaptive for complex GUI environments. Meanwhile, some existing
agents continue to operate as Reactive Actors, relying primarily on implicit
reasoning that may lack sufficient depth for GUI tasks demanding planning and
error recovery. We argue that advancing these agents requires a shift from
reactive acting towards acting based on deliberate reasoning. To facilitate
this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed
through our Actor2Reasoner framework, a reasoning-centric, two-stage training
approach designed to progressively evolve agents from Reactive Actors to
Deliberative Reasoners. The first stage, Reasoning Injection, focuses on
establishing a basic reasoner. We employ Spatial Reasoning Distillation to
transfer cross-modal spatial reasoning capabilities from teacher models to
MLLMs through trajectories with explicit reasoning steps, enabling models to
integrate GUI visual-spatial information with logical reasoning before action
generation. The second stage, Deliberation Enhancement, refines the basic
reasoner into a deliberative one using Reinforcement Learning. This stage
introduces two approaches: Sub-goal Guidance, which rewards models for
generating accurate intermediate sub-goals, and Error Recovery Scenario
Construction, which creates failure-and-recovery training scenarios from
identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves
strong performance in GUI grounding and trajectory tasks. Resources at
https://github.com/Reallm-Labs/InfiGUI-R1.

</details>


### [274] [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
*Ahsan Bilal,Muhammad Ahmed Mohsin,Muhammad Umer,Muhammad Awais Khan Bangash,Muhammad Ali Jamshed*

Main category: cs.AI

TL;DR: 该综述从多智能体强化学习（MARL）角度探讨了大型语言模型（LLMs）的元思维能力发展，旨在提升其可靠性、灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs存在幻觉和缺乏自我评估机制等局限性，需通过元思维增强其复杂或高风险任务的表现。

Method: 分析了RLHF、自蒸馏和思维链提示等方法，并探讨了多智能体架构（如监督者-代理层次结构、代理辩论和心理理论框架）如何模拟人类内省行为。

Result: 提出了通过MARL中的奖励机制、自我对弈和持续学习方法来构建内省、自适应且可信的LLMs的路线图。

Conclusion: 讨论了评估指标、数据集及未来研究方向（如神经科学启发架构和混合符号推理），为开发更强大的LLMs提供了指导。

Abstract: This survey explores the development of meta-thinking capabilities in Large
Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)
perspective. Meta-thinking self-reflection, assessment, and control of thinking
processes is an important next step in enhancing LLM reliability, flexibility,
and performance, particularly for complex or high-stakes tasks. The survey
begins by analyzing current LLM limitations, such as hallucinations and the
lack of internal self-assessment mechanisms. It then talks about newer methods,
including RL from human feedback (RLHF), self-distillation, and
chain-of-thought prompting, and each of their limitations. The crux of the
survey is to talk about how multi-agent architectures, namely supervisor-agent
hierarchies, agent debates, and theory of mind frameworks, can emulate
human-like introspective behavior and enhance LLM robustness. By exploring
reward mechanisms, self-play, and continuous learning methods in MARL, this
survey gives a comprehensive roadmap to building introspective, adaptive, and
trustworthy LLMs. Evaluation metrics, datasets, and future research avenues,
including neuroscience-inspired architectures and hybrid symbolic reasoning,
are also discussed.

</details>


### [275] [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
*Haoming Li,Zhaoliang Chen,Jonathan Zhang,Fei Liu*

Main category: cs.AI

TL;DR: 本文分析了现有规划基准的不足，提出分类和推荐，以指导算法选择和未来基准开发。


<details>
  <summary>Details</summary>
Motivation: 规划在智能代理中至关重要，但目前缺乏对规划基准的全面理解，导致算法比较和选择困难。

Method: 研究多种规划基准，分类为具体环境、网络导航、调度、游戏与谜题及日常任务自动化，并分析其适用性。

Result: 提出适用于不同算法的最佳基准，并指出未来基准开发的潜在方向。

Conclusion: 通过系统分析规划基准，为算法选择和未来研究提供了实用指导。

Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g.,
creating travel itineraries within a budget, holds immense potential in both
scientific and commercial contexts. Moreover, optimal plans tend to require
fewer resources compared to ad-hoc methods. To date, a comprehensive
understanding of existing planning benchmarks appears to be lacking. Without
it, comparing planning algorithms' performance across domains or selecting
suitable algorithms for new scenarios remains challenging. In this paper, we
examine a range of planning benchmarks to identify commonly used testbeds for
algorithm development and highlight potential gaps. These benchmarks are
categorized into embodied environments, web navigation, scheduling, games and
puzzles, and everyday task automation. Our study recommends the most
appropriate benchmarks for various algorithms and offers insights to guide
future benchmark development.

</details>


### [276] [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
*Jiaqi Wei,Hao Zhou,Xiang Zhang,Di Zhang,Zijie Qiu,Wei Wei,Jinzhe Li,Wanli Ouyang,Siqi Sun*

Main category: cs.AI

TL;DR: AlignRAG提出了一种新的测试时框架，通过迭代的批判驱动对齐（CDA）步骤解决RAG中的推理不对齐问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道未能确保推理轨迹与检索内容的一致性，导致推理不对齐问题。

Method: AlignRAG通过构建上下文丰富的训练语料、生成对比性批判、训练专门的批判语言模型（CLM）以及应用CDA步骤迭代优化推理轨迹。

Result: AlignRAG在实验中表现优于所有基线方法，并可无缝集成到现有RAG管道中。

Conclusion: AlignRAG通过重新定义RAG为结构化推理轨迹，并建立测试时框架，为检索感知生成提供了实用进展。

Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm
for knowledge-grounded text generation. However, existing RAG pipelines often
fail to ensure that the reasoning trajectories align with the evidential
constraints imposed by retrieved content. In this paper, we reframe RAG as a
problem of retrieval-aware reasoning and identify a core challenge: reasoning
misalignment-the mismatch between a model's reasoning trajectory and the
retrieved evidence. To address this challenge, we propose AlignRAG, a novel
test-time framework that mitigates reasoning misalignment through iterative
Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that
rely on static training or post-hoc selection, AlignRAG actively refines
reasoning trajectories during inference by enforcing fine-grained alignment
with evidence. Our framework introduces a new paradigm for retrieval-aware
reasoning by: (1) constructing context-rich training corpora; (2) generating
contrastive critiques from preference-aware reasoning trajectories; (3)
training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning
misalignments; and (4) applying CDA steps to optimize reasoning trajectories
iteratively. Empirical results demonstrate that AlignRAG consistently
outperforms all baselines and could integrate as a plug-and-play module into
existing RAG pipelines without further changes. By reconceptualizing RAG as a
structured reasoning trajectory and establishing the test-time framework for
correcting reasoning misalignments in RAG, AlignRAG provides practical
advancements for retrieval-aware generation.

</details>


### [277] [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
*Hongru Wang,Cheng Qian,Wanjun Zhong,Xiusi Chen,Jiahao Qiu,Shijue Huang,Bowen Jin,Mengdi Wang,Kam-Fai Wong,Heng Ji*

Main category: cs.AI

TL;DR: 论文提出了一种名为OTC-PO的强化学习框架，优化工具集成推理中的工具使用效率，减少调用次数并保持答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法常忽视工具使用的效率和成本，导致过度调用工具或工具使用不足，影响性能和成本。

Method: 提出OTC-PO框架，结合正确性和工具效率的奖励机制，并在PPO和GRPO中实现（OTC-PPO和OTC-GRPO）。

Result: 实验显示，工具调用减少73.1%，工具效率提升229.4%，同时保持答案准确性。

Conclusion: OTC-PO是首个明确优化工具使用效率的强化学习框架，显著提升了工具集成推理的性能。

Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with
the ability to invoke external tools, such as search engines and code
interpreters, to solve tasks beyond the capabilities of language-only
reasoning. While reinforcement learning (RL) has shown promise in improving TIR
by optimizing final answer correctness, existing approaches often overlook the
efficiency and cost associated with tool usage. This can lead to suboptimal
behavior, including excessive tool calls that increase computational and
financial overhead, or insufficient tool use that compromises answer quality.
In this work, we propose Optimal Tool Call-controlled Policy Optimization
(OTC-PO), a simple yet effective RL-based framework that encourages models to
produce accurate answers with minimal tool calls. Our method introduces a
tool-integrated reward that jointly considers correctness and tool efficiency,
promoting high tool productivity. We instantiate this framework within both
Proximal Policy Optimization (PPO) and Group Relative Preference Optimization
(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and
Qwen-Math across multiple QA benchmarks show that our approach reduces tool
calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while
maintaining comparable answer accuracy. To the best of our knowledge, this is
the first RL-based framework that explicitly optimizes tool-use efficiency in
TIR.

</details>


### [278] [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
*Yao Shi,Rongkeng Liang,Yong Xu*

Main category: cs.AI

TL;DR: EducationQ框架通过多智能体对话评估LLMs的教学能力，发现教学效果与模型规模或通用推理能力无线性关系，小模型可能优于大模型。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过于依赖知识回忆，忽视了交互式教学法的重要性。

Method: 使用EducationQ框架模拟动态教育场景，测试14个LLMs在13个学科和10个难度级别上的表现。

Result: 教学效果与模型规模无关，小模型在某些情况下表现更优；专家评估与自动化分析高度一致。

Conclusion: 下一代教育AI应针对特定教学效果进行优化，而非简单扩大模型规模。

Abstract: Large language models (LLMs) increasingly serve as educational tools, yet
evaluating their teaching capabilities remains challenging due to the
resource-intensive, context-dependent, and methodologically complex nature of
teacher-student interactions. We introduce EducationQ, a multi-agent dialogue
framework that efficiently assesses teaching capabilities through simulated
dynamic educational scenarios, featuring specialized agents for teaching,
learning, and evaluation. Testing 14 LLMs across major AI Organizations
(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13
disciplines and 10 difficulty levels reveals that teaching effectiveness does
not correlate linearly with model scale or general reasoning capabilities -
with some smaller open-source models outperforming larger commercial
counterparts in teaching contexts. This finding highlights a critical gap in
current evaluations that prioritize knowledge recall over interactive pedagogy.
Our mixed-methods evaluation, combining quantitative metrics with qualitative
analysis and expert case studies, identifies distinct pedagogical strengths
employed by top-performing models (e.g., sophisticated questioning strategies,
adaptive feedback mechanisms). Human expert evaluations show 78% agreement with
our automated qualitative analysis of effective teaching behaviors, validating
our methodology. EducationQ demonstrates that LLMs-as-teachers require
specialized optimization beyond simple scaling, suggesting next-generation
educational AI prioritize targeted enhancement of specific pedagogical
effectiveness.

</details>


### [279] [SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam](https://arxiv.org/abs/2504.15252)
*Tue Vo,Lakshay Sharma,Tuan Dinh,Khuong Dinh,Trang Nguyen,Trung Phan,Minh Do,Duong Vu*

Main category: cs.AI

TL;DR: 提出SuoiAI，一种端到端流程，用于构建越南水生无脊椎动物数据集，并利用机器学习技术进行物种分类。


<details>
  <summary>Details</summary>
Motivation: 理解和监测水生生物多样性对生态健康和保护工作至关重要。

Method: 通过半监督学习减少标注工作量，并利用先进的目标检测和分类模型进行数据收集、标注和模型训练。

Result: 旨在解决数据稀缺、细粒度分类和多样化环境条件下的部署等挑战。

Conclusion: SuoiAI为水生生物多样性监测提供了一种高效且可扩展的解决方案。

Abstract: Understanding and monitoring aquatic biodiversity is critical for ecological
health and conservation efforts. This paper proposes SuoiAI, an end-to-end
pipeline for building a dataset of aquatic invertebrates in Vietnam and
employing machine learning (ML) techniques for species classification. We
outline the methods for data collection, annotation, and model training,
focusing on reducing annotation effort through semi-supervised learning and
leveraging state-of-the-art object detection and classification models. Our
approach aims to overcome challenges such as data scarcity, fine-grained
classification, and deployment in diverse environmental conditions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [280] [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
*Jiakai Tang,Jingsen Zhang,Zihang Tian,Xueyang Feng,Lei Wang,Xu Chen*

Main category: cs.IR

TL;DR: 提出了一种基于人类反馈的优化框架，利用大语言模型模拟人类反馈，改进可解释推荐系统的解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏交互数据中依赖传统监督学习，无法提供有效的反馈信号以优化解释质量。

Method: 采用动态交互优化机制，利用大语言模型预测人类反馈，引入定制化奖励评分和帕累托优化，设计离策略优化流程。

Result: 在四个数据集上的实验证明了该方法的优越性。

Conclusion: 该框架显著提升了可解释推荐的性能，同时避免了高人力成本。

Abstract: Recent advancements in explainable recommendation have greatly bolstered user
experience by elucidating the decision-making rationale. However, the existing
methods actually fail to provide effective feedback signals for potentially
better or worse generated explanations due to their reliance on traditional
supervised learning paradigms in sparse interaction data. To address these
issues, we propose a novel human-like feedback-driven optimization framework.
This framework employs a dynamic interactive optimization mechanism for
achieving human-centered explainable requirements without incurring high labor
costs. Specifically, we propose to utilize large language models (LLMs) as
human simulators to predict human-like feedback for guiding the learning
process. To enable the LLMs to deeply understand the task essence and meet
user's diverse personalized requirements, we introduce a human-induced
customized reward scoring method, which helps stimulate the language
understanding and logical reasoning capabilities of LLMs. Furthermore,
considering the potential conflicts between different perspectives of
explanation quality, we introduce a principled Pareto optimization that
transforms the multi-perspective quality enhancement task into a
multi-objective optimization problem for improving explanation performance. At
last, to achieve efficient model training, we design an off-policy optimization
pipeline. By incorporating a replay buffer and addressing the data distribution
biases, we can effectively improve data utilization and enhance model
generality. Extensive experiments on four datasets demonstrate the superiority
of our approach.

</details>


### [281] [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
*Ronak Pradeep,Nandan Thakur,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.IR

TL;DR: 提出了一种基于LLMs的自动评估框架AutoNuggetizer，用于评估RAG系统，验证了其与人工评估的一致性。


<details>
  <summary>Details</summary>
Motivation: RAG系统的评估是当前研究的瓶颈，需要一种自动化的评估方法以推动进展。

Method: 采用TREC QA Track的nugget评估方法，通过LLMs自动生成和分配nuggets，并与人工评估对比。

Result: 自动评估与人工评估在运行级别上表现出较强一致性，尤其在独立自动化组件时效果更佳。

Conclusion: 该框架在质量和效率间提供了权衡，但需进一步研究以优化每主题一致性。

Abstract: Large Language Models (LLMs) have significantly enhanced the capabilities of
information access systems, especially with retrieval-augmented generation
(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to
continued progress, a challenge we tackle in this work by proposing an
automatic evaluation framework that is validated against human annotations. We
believe that the nugget evaluation methodology provides a solid foundation for
evaluating RAG systems. This approach, originally developed for the TREC
Question Answering (QA) Track in 2003, evaluates systems based on atomic facts
that should be present in good answers. Our efforts focus on "refactoring" this
methodology, where we describe the AutoNuggetizer framework that specifically
applies LLMs to both automatically create nuggets and automatically assign
nuggets to system answers. In the context of the TREC 2024 RAG Track, we
calibrate a fully automatic approach against strategies where nuggets are
created manually or semi-manually by human assessors and then assigned manually
to system answers. Based on results from a community-wide evaluation, we
observe strong agreement at the run level between scores derived from fully
automatic nugget evaluation and human-based variants. The agreement is stronger
when individual framework components such as nugget assignment are automated
independently. This suggests that our evaluation framework provides tradeoffs
between effort and quality that can be used to guide the development of future
RAG systems. However, further research is necessary to refine our approach,
particularly in establishing robust per-topic agreement to diagnose system
failures effectively.

</details>


### [282] [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
*Juyeon Kim,Geon Lee,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TL;DR: KGMEL是一个利用知识图谱三元组增强多模态实体链接的新框架，通过生成、检索和重排三个阶段显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法忽略了知识图谱的结构信息，KGMEL旨在利用这些信息减少歧义并提高准确性。

Method: KGMEL分三阶段：生成高质量三元组、学习联合表示检索候选实体、重排优化匹配结果。

Result: 在基准数据集上，KGMEL表现优于现有方法。

Conclusion: KGMEL通过整合知识图谱三元组，显著提升了多模态实体链接的准确性。

Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities
in a knowledge base, facilitating various applications such as semantic search
and question answering. Recent advances in multimodal entity linking (MEL) have
shown that combining text and images can reduce ambiguity and improve alignment
accuracy. However, most existing MEL methods overlook the rich structural
information available in the form of knowledge-graph (KG) triples. In this
paper, we propose KGMEL, a novel framework that leverages KG triples to enhance
MEL. Specifically, it operates in three stages: (1) Generation: Produces
high-quality triples for each mention by employing vision-language models based
on its text and images. (2) Retrieval: Learns joint mention-entity
representations, via contrastive learning, that integrate text, images, and
(generated or KG) triples to retrieve candidate entities for each mention. (3)
Reranking: Refines the KG triples of the candidate entities and employs large
language models to identify the best-matching entity for the mention. Extensive
experiments on benchmark datasets demonstrate that KGMEL outperforms existing
methods. Our code and datasets are available at:
https://github.com/juyeonnn/KGMEL.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [283] [Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation](https://arxiv.org/abs/2504.14541)
*Yi Yu,Song Xia,Xun Lin,Chenqi Kong,Wenhan Yang,Shijian Lu,Yap-Peng Tan,Alex C. Kot*

Main category: cs.CR

TL;DR: 论文提出了一种新的训练范式，通过触发激活模型增强对抗样本的可转移性鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的可转移性对深度神经网络构成威胁，现有防御方法存在效率低、效果差等问题。

Method: 提出一种触发激活模型，在干净数据上随机猜测，在触发数据上准确预测，并通过联合优化触发器和模型提升鲁棒性。

Result: 实验证明该方法在多种数据集和攻击方法下有效且优于现有方法。

Conclusion: 触发激活模型能有效抵御可转移对抗样本，并通过理论分析和实验验证了其优越性。

Abstract: Adversarial examples, characterized by imperceptible perturbations, pose
significant threats to deep neural networks by misleading their predictions. A
critical aspect of these examples is their transferability, allowing them to
deceive {unseen} models in black-box scenarios. Despite the widespread
exploration of defense methods, including those on transferability, they show
limitations: inefficient deployment, ineffective defense, and degraded
performance on clean images. In this work, we introduce a novel training
paradigm aimed at enhancing robustness against transferable adversarial
examples (TAEs) in a more efficient and effective way. We propose a model that
exhibits random guessing behavior when presented with clean data
$\boldsymbol{x}$ as input, and generates accurate predictions when with
triggered data $\boldsymbol{x}+\boldsymbol{\tau}$. Importantly, the trigger
$\boldsymbol{\tau}$ remains constant for all data instances. We refer to these
models as \textbf{models with trigger activation}. We are surprised to find
that these models exhibit certain robustness against TAEs. Through the
consideration of first-order gradients, we provide a theoretical analysis of
this robustness. Moreover, through the joint optimization of the learnable
trigger and the model, we achieve improved robustness to transferable attacks.
Extensive experiments conducted across diverse datasets, evaluating a variety
of attacking methods, underscore the effectiveness and superiority of our
approach.

</details>


### [284] [REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.14554)
*Chongye Guo,Jinhu Fu,Junfeng Fang,Kun Wang,Guorui Feng*

Main category: cs.CR

TL;DR: 论文提出了一种基于模型编辑的无训练后门攻击方法REDEditing，揭示了模型编辑技术在图像生成模型中的安全风险，并实现了更高的攻击成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，文本到图像（T2I）模型的安全性尤为重要，尤其是后门攻击的威胁。论文旨在揭示模型编辑技术对图像生成模型的安全风险。

Method: 通过模型编辑实现无训练后门攻击，提出REDEditing方法，基于等效属性对齐和隐蔽毒化原则，采用等效关系检索和联合属性转移技术。

Result: REDEditing的攻击成功率比现有方法高11%，仅需一行代码即可提升输出自然性和后门隐蔽性24%。

Conclusion: 论文揭示了可编辑图像生成模型的安全漏洞，呼吁提高对此类风险的认识。

Abstract: The rapid advancement of generative AI highlights the importance of
text-to-image (T2I) security, particularly with the threat of backdoor
poisoning. Timely disclosure and mitigation of security vulnerabilities in T2I
models are crucial for ensuring the safe deployment of generative models. We
explore a novel training-free backdoor poisoning paradigm through model
editing, which is recently employed for knowledge updating in large language
models. Nevertheless, we reveal the potential security risks posed by model
editing techniques to image generation models. In this work, we establish the
principles for backdoor attacks based on model editing, and propose a
relationship-driven precise backdoor poisoning method, REDEditing. Drawing on
the principles of equivalent-attribute alignment and stealthy poisoning, we
develop an equivalent relationship retrieval and joint-attribute transfer
approach that ensures consistent backdoor image generation through concept
rebinding. A knowledge isolation constraint is proposed to preserve benign
generation integrity. Our method achieves an 11\% higher attack success rate
compared to state-of-the-art approaches. Remarkably, adding just one line of
code enhances output naturalness while improving backdoor stealthiness by 24\%.
This work aims to heighten awareness regarding this security vulnerability in
editable image generation models.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [285] [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CY

TL;DR: 论文介绍了Thousand Voices of Trauma数据集，包含3,000个基于PTSD治疗的合成对话，填补了心理健康数据空白。


<details>
  <summary>Details</summary>
Motivation: AI心理健康支持系统因缺乏治疗对话数据（尤其是创伤治疗）而受限，需解决数据不足问题。

Method: 使用确定性和概率生成方法创建了包含500个案例、多种人口统计和创伤类型的合成数据集。

Result: 数据集展示了真实的创伤类型和症状分布，临床专家验证了其治疗保真度。

Conclusion: 该隐私保护数据集为创伤治疗AI应用和临床培训提供了宝贵资源。

Abstract: The advancement of AI systems for mental health support is hindered by
limited access to therapeutic conversation data, particularly for trauma
treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset
of 3,000 therapy conversations based on Prolonged Exposure therapy protocols
for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique
cases, each explored through six conversational perspectives that mirror the
progression of therapy from initial anxiety to peak distress to emotional
processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,
49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10
trauma-related behaviors using deterministic and probabilistic generation
methods. Analysis reveals realistic distributions of trauma types (witnessing
violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse
20.8%). Clinical experts validated the dataset's therapeutic fidelity,
highlighting its emotional depth while suggesting refinements for greater
authenticity. We also developed an emotional trajectory benchmark with
standardized metrics for evaluating model responses. This privacy-preserving
dataset addresses critical gaps in trauma-focused mental health data, offering
a valuable resource for advancing both patient-facing applications and
clinician training tools.

</details>


### [286] [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
*Sanchaita Hazra,Bodhisattwa Prasad Majumder,Tuhin Chakrabarty*

Main category: cs.CY

TL;DR: 论文呼吁关注AI对工作的长期影响，提出支持劳动力转型和公平补偿机制的国际治理框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究过于关注技术风险，忽视了AI对社会结构和人类生计的深远影响。

Method: 通过经济理论分析AI对劳动力市场的结构性影响，并提出国际版权和集体许可机制。

Result: AI可能导致收入不平等加剧和创造力平庸化，需通过公平补偿和治理框架解决。

Conclusion: 建议建立支持工人权益的全球AI治理框架，促进经济正义和共享繁荣。

Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing
manipulation of human behavior, and eliminating existential risks in
cybersecurity or biosecurity. While pressing, this narrow focus overlooks
critical human-centric considerations that shape the long-term trajectory of a
society. In this position paper, we identify the risks of overlooking the
impact of AI on the future of work and recommend comprehensive transition
support towards the evolution of meaningful labor with human agency. Through
the lens of economic theories, we highlight the intertemporal impacts of AI on
human livelihood and the structural changes in labor markets that exacerbate
income inequality. Additionally, the closed-source approach of major
stakeholders in AI development resembles rent-seeking behavior through
exploiting resources, breeding mediocrity in creative labor, and monopolizing
innovation. To address this, we argue in favor of a robust international
copyright anatomy supported by implementing collective licensing that ensures
fair compensation mechanisms for using data to train AI models. We strongly
recommend a pro-worker framework of global AI governance to enhance shared
prosperity and economic justice while reducing technical debt.

</details>


### [287] [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
*Ali Safari*

Main category: cs.CY

TL;DR: 研究分析了Airbnb评论的情感极性对接受率和租金的影响，发现正面评论比例高但数量对价格影响不大，情感极性更关键。


<details>
  <summary>Details</summary>
Motivation: 探讨Airbnb评论的情感（正面/负面）是否影响房东的接受率和租金定价。

Method: 收集数千条评论，使用NLP分类情感，并通过t检验和相关性分析验证结果。

Result: 90%以上评论为正面，情感极性（而非数量）对接受率影响更大；预算房源评论多但价格稳定，高端房源评论少但价格高。

Conclusion: 在高度正面的评论环境中，情感质量比数量更能影响客户行为和定价策略。

Abstract: This research examines whether Airbnb guests' positive and negative comments
influence acceptance rates and rental prices across six U.S. regions: Rhode
Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of
reviews were collected and analyzed using Natural Language Processing (NLP) to
classify sentiments as positive or negative, followed by statistical testing
(t-tests and basic correlations) on the average scores. The findings reveal
that over 90 percent of reviews in each region are positive, indicating that
having additional reviews does not significantly enhance prices. However,
listings with predominantly positive feedback exhibit slightly higher
acceptance rates, suggesting that sentiment polarity, rather than the sheer
volume of reviews, is a more critical factor for host success. Additionally,
budget listings often gather extensive reviews while maintaining competitive
pricing, whereas premium listings sustain higher prices with fewer but highly
positive reviews. These results underscore the importance of sentiment quality
over quantity in shaping guest behavior and pricing strategies in an
overwhelmingly positive review environment.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [288] [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
*Theo Jaffrelot Inizan,Sherry Yang,Aaron Kaplan,Yen-hsu Lin,Jian Yin,Saber Mirzaei,Mona Abdelgaid,Ali H. Alawadhi,KwangHwan Cho,Zhiling Zheng,Ekin Dogus Cubuk,Christian Borgs,Jennifer T. Chayes,Kristin A. Persson,Omar M. Yaghi*

Main category: cond-mat.mtrl-sci

TL;DR: MOFGen是一个由多个AI代理组成的系统，用于加速MOF材料的发现，成功生成了大量新型MOF结构并通过实验验证。


<details>
  <summary>Details</summary>
Motivation: 解决在CO2捕获和水收集领域导航庞大化学空间并确保可合成性的挑战。

Method: 结合大型语言模型、扩散模型、量子力学代理和合成可行性代理，生成和优化MOF结构。

Result: 生成了数十万种新型MOF结构，并通过实验成功合成了五种AI设计的MOF。

Conclusion: MOFGen为自动化可合成材料发现迈出了重要一步。

Abstract: Generative models and machine learning promise accelerated material discovery
in MOFs for CO2 capture and water harvesting but face significant challenges
navigating vast chemical spaces while ensuring synthetizability. Here, we
present MOFGen, a system of Agentic AI comprising interconnected agents: a
large language model that proposes novel MOF compositions, a diffusion model
that generates crystal structures, quantum mechanical agents that optimize and
filter candidates, and synthetic-feasibility agents guided by expert rules and
machine learning. Trained on all experimentally reported MOFs and computational
databases, MOFGen generated hundreds of thousands of novel MOF structures and
synthesizable organic linkers. Our methodology was validated through
high-throughput experiments and the successful synthesis of five "AI-dreamt"
MOFs, representing a major step toward automated synthesizable material
discovery.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [289] [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
*Xingyu Lu,Tianke Zhang,Chang Meng,Xiaobei Wang,Jinpeng Wang,YiFan Zhang,Shisong Tang,Changyi Liu,Haojie Ding,Kaiyu Jiang,Kaiyu Tang,Bin Wen,Hai-Tao Zheng,Fan Yang,Tingting Gao,Di Zhang,Kun Gai*

Main category: cs.SI

TL;DR: 该论文提出了KuaiMod框架，用于解决短视频平台内容审核的挑战，结合视觉语言模型和链式推理，显著提升了审核效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容审核存在手动成本高、自动化方法精度低、法规更新慢等问题，亟需高效解决方案。

Method: 提出KuaiMod框架，包括训练数据构建、离线适应和在线部署优化，利用视觉语言模型和链式推理。

Result: KuaiMod在基准测试中表现最佳，用户举报率降低20%，并提升了平台活跃度和使用时长。

Conclusion: KuaiMod为短视频内容审核提供了高效动态的解决方案，具有实际应用价值。

Abstract: Exponentially growing short video platforms (SVPs) face significant
challenges in moderating content detrimental to users' mental health,
particularly for minors. The dissemination of such content on SVPs can lead to
catastrophic societal consequences. Although substantial efforts have been
dedicated to moderating such content, existing methods suffer from critical
limitations: (1) Manual review is prone to human bias and incurs high
operational costs. (2) Automated methods, though efficient, lack nuanced
content understanding, resulting in lower accuracy. (3) Industrial moderation
regulations struggle to adapt to rapidly evolving trends due to long update
cycles. In this paper, we annotate the first SVP content moderation benchmark
with authentic user/reviewer feedback to fill the absence of benchmark in this
field. Then we evaluate various methods on the benchmark to verify the
existence of the aforementioned limitations. We further propose our common-law
content moderation framework named KuaiMod to address these challenges. KuaiMod
consists of three components: training data construction, offline adaptation,
and online deployment & refinement. Leveraging large vision language model
(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video
toxicity based on sparse user feedback and fosters dynamic moderation policy
with rapid update speed and high accuracy. Offline experiments and large-scale
online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the
best moderation performance on our benchmark. The deployment of KuaiMod reduces
the user reporting rate by 20% and its application in video recommendation
increases both Daily Active User (DAU) and APP Usage Time (AUT) on several
Kuaishou scenarios. We have open-sourced our benchmark at
https://kuaimod.github.io.

</details>


### [290] [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
*Yulong Li,Zhixiang Lu,Feilong Tang,Simin Lai,Ming Hu,Yuxuan Zhang,Haochen Xue,Zhaodong Wu,Imran Razzak,Qingxia Li,Jionglong Su*

Main category: cs.SI

TL;DR: 提出了一种结合多维霍克斯过程和图神经网络的方法，用于建模社交网络中意见传播的动态，并引入新数据集VISTA以支持研究。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉社交媒体中复杂的公共意见动态，需要新方法来建模多层次互动和情感传播。

Method: 整合多维霍克斯过程与图神经网络，建模社交网络中节点间的意见传播，同时考虑评论的层次关系。

Result: 提出的方法能有效捕捉多层次互动和情感传播，VISTA数据集为研究提供了高质量数据支持。

Conclusion: 该方法为未来研究提供了强解释性和基准，VISTA数据集填补了高质量数据的空白。

Abstract: The rapid development of social media has significantly reshaped the dynamics
of public opinion, resulting in complex interactions that traditional models
fail to effectively capture. To address this challenge, we propose an
innovative approach that integrates multi-dimensional Hawkes processes with
Graph Neural Network, modeling opinion propagation dynamics among nodes in a
social network while considering the intricate hierarchical relationships
between comments. The extended multi-dimensional Hawkes process captures the
hierarchical structure, multi-dimensional interactions, and mutual influences
across different topics, forming a complex propagation network. Moreover,
recognizing the lack of high-quality datasets capable of comprehensively
capturing the evolution of public opinion dynamics, we introduce a new dataset,
VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015
second-level comments, and 29,578 third-level comments, covering diverse
domains such as politics, entertainment, sports, health, and medicine. The
dataset is annotated with detailed sentiment labels across 11 categories and
clearly defined hierarchical relationships. When combined with our method, it
offers strong interpretability by linking sentiment propagation to the comment
hierarchy and temporal evolution. Our approach provides a robust baseline for
future research.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [291] [Segmentation with Noisy Labels via Spatially Correlated Distributions](https://arxiv.org/abs/2504.14795)
*Ryu Tadokoro,Tsukasa Takagi,Shin-ichi Maeda*

Main category: eess.IV

TL;DR: 提出了一种基于贝叶斯估计的概率模型，用于处理语义分割中因空间相关性导致的标注错误，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 在语义分割中，高质量标注对模型准确性至关重要，但实际场景（如医学影像和遥感）中标注错误常见且具有空间相关性。

Method: 采用近似贝叶斯估计，假设标注错误存在空间相关性，并通过高斯分布和KMS矩阵建模这种相关性。

Result: 实验表明，利用标注错误的空间相关性显著提升了性能，在特定任务（如肺部分割）中接近使用干净标注的效果。

Conclusion: 该方法有效解决了标注错误的空间相关性问题，为实际场景中的语义分割提供了实用解决方案。

Abstract: In semantic segmentation, the accuracy of models heavily depends on the
high-quality annotations. However, in many practical scenarios such as medical
imaging and remote sensing, obtaining true annotations is not straightforward
and usually requires significant human labor. Relying on human labor often
introduces annotation errors, including mislabeling, omissions, and
inconsistency between annotators. In the case of remote sensing, differences in
procurement time can lead to misaligned ground truth annotations. These label
errors are not independently distributed, and instead usually appear in
spatially connected regions where adjacent pixels are more likely to share the
same errors. To address these issues, we propose an approximate Bayesian
estimation based on a probabilistic model that assumes training data includes
label errors, incorporating the tendency for these errors to occur with spatial
correlations between adjacent pixels. Bayesian inference requires computing the
posterior distribution of label errors, which becomes intractable when spatial
correlations are present. We represent the correlation of label errors between
adjacent pixels through a Gaussian distribution whose covariance is structured
by a Kac-Murdock-Szeg\"{o} (KMS) matrix, solving the computational challenges.
Through experiments on multiple segmentation tasks, we confirm that leveraging
the spatial correlation of label errors significantly improves performance.
Notably, in specific tasks such as lung segmentation, the proposed method
achieves performance comparable to training with clean labels under moderate
noise levels. Code is available at
https://github.com/pfnet-research/Bayesian_SpatialCorr.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [292] [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
*Yuheng Huang,Lei Ma,Keizaburo Nishikino,Takumi Akazaki*

Main category: cs.SE

TL;DR: PtTrust是一个基于内部状态预训练的两阶段风险评估框架，旨在提高代码LLM的可信度，通过预训练和微调实现跨任务和语言的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM生成的代码可能存在错误、不安全或不可靠的问题，现有方法在规模和实用性上不足，需要一种可扩展的解决方案。

Method: PtTrust采用两阶段方法：无监督预训练学习LLM状态的通用表示，再用小规模标注数据训练风险预测器。

Result: PtTrust在代码行级风险评估中表现有效，能跨任务和语言泛化，并提供直观可解释的特征。

Conclusion: PtTrust为代码LLM的可扩展和可信保障迈出了重要一步。

Abstract: The pre-training paradigm plays a key role in the success of Large Language
Models (LLMs), which have been recognized as one of the most significant
advancements of AI recently. Building on these breakthroughs, code LLMs with
advanced coding capabilities bring huge impacts on software engineering,
showing the tendency to become an essential part of developers' daily routines.
However, the current code LLMs still face serious challenges related to
trustworthiness, as they can generate incorrect, insecure, or unreliable code.
Recent exploratory studies find that it can be promising to detect such risky
outputs by analyzing LLMs' internal states, akin to how the human brain
unconsciously recognizes its own mistakes. Yet, most of these approaches are
limited to narrow sub-domains of LLM operations and fall short of achieving
industry-level scalability and practicability. To address these challenges, in
this paper, we propose PtTrust, a two-stage risk assessment framework for code
LLM based on internal state pre-training, designed to integrate seamlessly with
the existing infrastructure of software companies. The core idea is that the
risk assessment framework could also undergo a pre-training process similar to
LLMs. Specifically, PtTrust first performs unsupervised pre-training on
large-scale unlabeled source code to learn general representations of LLM
states. Then, it uses a small, labeled dataset to train a risk predictor. We
demonstrate the effectiveness of PtTrust through fine-grained, code line-level
risk assessment and demonstrate that it generalizes across tasks and different
programming languages. Further experiments also reveal that PtTrust provides
highly intuitive and interpretable features, fostering greater user trust. We
believe PtTrust makes a promising step toward scalable and trustworthy
assurance for code LLMs.

</details>


### [293] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Main category: cs.SE

TL;DR: CRUST-Bench是一个用于评估C到Rust转译的数据集，包含100个C代码库及其对应的安全Rust接口和测试用例。研究发现，当前最先进的LLM在此任务上表现有限。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估C到安全Rust转译的数据集，阻碍了转译系统的开发和改进。

Method: 引入CRUST-Bench数据集，包含100个C代码库及其手动编写的安全Rust接口和测试用例，以评估转译系统的能力。

Result: OpenAI o1模型在单次尝试中仅能完成15个任务，表明安全且符合习惯的Rust生成仍具挑战性。

Conclusion: CRUST-Bench为改进转译系统提供了基准，有助于推动C到Rust的代码迁移。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [294] [Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering](https://arxiv.org/abs/2504.14135)
*Jonathan Embley-Riches,Jianwei Liu,Simon Julier,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 提出了一种结合Unreal Engine高级渲染与MuJoCo高精度物理模拟的仿真框架Unreal Robotics Lab（URL），用于机器人研究中的高保真仿真。


<details>
  <summary>Details</summary>
Motivation: 解决机器人研究中同时实现真实感渲染和精确物理模拟的挑战。

Method: 集成Unreal Engine的渲染能力和MuJoCo的物理模拟，支持复杂环境效果（如烟雾、火、水）。

Result: 框架成功用于视觉导航和SLAM方法的基准测试，展示了其在多样化场景中的实用性。

Conclusion: 该框架为机器人研究和仿真到现实的迁移提供了强大工具。

Abstract: High-fidelity simulation is essential for robotics research, enabling safe
and efficient testing of perception, control, and navigation algorithms.
However, achieving both photorealistic rendering and accurate physics modeling
remains a challenge. This paper presents a novel simulation framework--the
Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced
rendering capabilities with MuJoCo's high-precision physics simulation. Our
approach enables realistic robotic perception while maintaining accurate
physical interactions, facilitating benchmarking and dataset generation for
vision-based robotics applications. The system supports complex environmental
effects, such as smoke, fire, and water dynamics, which are critical for
evaluating robotic performance under adverse conditions. We benchmark visual
navigation and SLAM methods within our framework, demonstrating its utility for
testing real-world robustness in controlled yet diverse scenarios. By bridging
the gap between physics accuracy and photorealistic rendering, our framework
provides a powerful tool for advancing robotics research and sim-to-real
transfer.

</details>


### [295] [Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions](https://arxiv.org/abs/2504.14078)
*M-Mahdi Naddaf-Sh,Andrew Lee,Kin Yen,Eemon Amini,Iman Soltani*

Main category: cs.RO

TL;DR: 研究探讨红外相机技术在低能见度条件下提升紧急车辆驾驶员安全的潜力，通过实验和调查验证其效果。


<details>
  <summary>Details</summary>
Motivation: 低能见度环境（如夜间和浓雾）增加了紧急车辆（如拖车和除雪车）的碰撞风险，现有辅助系统效果有限。

Method: 结合实验室实验、实地测试和驾驶员调查，评估红外相机技术及其在现有车队中的改装可行性。

Result: 红外技术显著提升驾驶员意识，并提供了可扩展部署的数据支持建议。

Conclusion: 红外相机技术是提升紧急车辆在低能见度条件下安全的有效解决方案。

Abstract: This study investigates the potential of infrared (IR) camera technology to
enhance driver safety for emergency vehicles operating in low-visibility
conditions, particularly at night and in dense fog. Such environments
significantly increase the risk of collisions, especially for tow trucks and
snowplows that must remain operational in challenging conditions. Conventional
driver assistance systems often struggle under these conditions due to limited
visibility. In contrast, IR cameras, which detect the thermal signatures of
obstacles, offer a promising alternative. The evaluation combines controlled
laboratory experiments, real-world field tests, and surveys of emergency
vehicle operators. In addition to assessing detection performance, the study
examines the feasibility of retrofitting existing Department of Transportation
(DoT) fleets with cost-effective IR-based driver assistance systems. Results
underscore the utility of IR technology in enhancing driver awareness and
provide data-driven recommendations for scalable deployment across legacy
emergency vehicle fleets.

</details>


### [296] [SG-Reg: Generalizable and Efficient Scene Graph Registration](https://arxiv.org/abs/2504.14440)
*Chuhao Liu,Zhijian Qiao,Jieqi Shi,Ke Wang,Peize Liu,Shaojie Shen*

Main category: cs.RO

TL;DR: 论文提出了一种多模态语义节点融合的场景图网络，用于刚性语义场景图的注册，解决了传统方法依赖手工特征或真实标注的问题，并在多智能体任务中减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决传统语义辅助注册方法依赖手工特征或真实标注的问题，提升在实际环境中的应用能力。

Method: 设计多模态语义节点融合的场景图网络，包括开放集语义特征、局部拓扑与空间感知、形状特征，并采用粗到细的匹配策略和鲁棒姿态估计。

Result: 在双智能体SLAM基准测试中显著优于手工基线，注册召回率略高于视觉闭环网络，且每次查询仅需52 KB通信带宽。

Conclusion: 该方法在减少资源消耗的同时提升了注册性能，适用于实际多智能体任务。

Abstract: This paper addresses the challenges of registering two rigid semantic scene
graphs, an essential capability when an autonomous agent needs to register its
map against a remote agent, or against a prior map. The hand-crafted
descriptors in classical semantic-aided registration, or the ground-truth
annotation reliance in learning-based scene graph registration, impede their
application in practical real-world environments. To address the challenges, we
design a scene graph network to encode multiple modalities of semantic nodes:
open-set semantic feature, local topology with spatial awareness, and shape
feature. These modalities are fused to create compact semantic node features.
The matching layers then search for correspondences in a coarse-to-fine manner.
In the back-end, we employ a robust pose estimator to decide transformation
according to the correspondences. We manage to maintain a sparse and
hierarchical scene representation. Our approach demands fewer GPU resources and
fewer communication bandwidth in multi-agent tasks. Moreover, we design a new
data generation approach using vision foundation models and a semantic mapping
module to reconstruct semantic scene graphs. It differs significantly from
previous works, which rely on ground-truth semantic annotations to generate
data. We validate our method in a two-agent SLAM benchmark. It significantly
outperforms the hand-crafted baseline in terms of registration success rate.
Compared to visual loop closure networks, our method achieves a slightly higher
registration recall while requiring only 52 KB of communication bandwidth for
each query frame. Code available at:
\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.

</details>


### [297] [Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction](https://arxiv.org/abs/2504.14588)
*Wenke Xia,Ruoxuan Feng,Dong Wang,Di Hu*

Main category: cs.RO

TL;DR: 论文提出了Phoenix框架，通过运动指令连接高级语义反思与低级机器人动作修正，结合双进程运动调整机制和多任务运动条件扩散策略，实现精确的机器人动作修正。


<details>
  <summary>Details</summary>
Motivation: 构建通用自修正系统对机器人从失败中恢复至关重要，但如何将语义反思转化为细粒度机器人动作修正仍具挑战性。

Method: 提出Phoenix框架，结合双进程运动调整机制和多任务运动条件扩散策略，利用运动指令连接语义反思与动作修正。

Result: 在RoboMimic仿真和真实场景实验中，框架展现出卓越的泛化能力和鲁棒性。

Conclusion: Phoenix框架通过运动指令和终身学习方法，显著提升了机器人动作修正的精确性和泛化能力。

Abstract: Building a generalizable self-correction system is crucial for robots to
recover from failures. Despite advancements in Multimodal Large Language Models
(MLLMs) that empower robots with semantic reflection ability for failure,
translating semantic reflection into how to correct fine-grained robotic
actions remains a significant challenge. To address this gap, we build the
Phoenix framework, which leverages motion instruction as a bridge to connect
high-level semantic reflection with low-level robotic action correction. In
this motion-based self-reflection framework, we start with a dual-process
motion adjustment mechanism with MLLMs to translate the semantic reflection
into coarse-grained motion instruction adjustment. To leverage this motion
instruction for guiding how to correct fine-grained robotic actions, a
multi-task motion-conditioned diffusion policy is proposed to integrate visual
observations for high-frequency robotic action correction. By combining these
two models, we could shift the demand for generalization capability from the
low-level manipulation policy to the MLLMs-driven motion adjustment model and
facilitate precise, fine-grained robotic action correction. Utilizing this
framework, we further develop a lifelong learning method to automatically
improve the model's capability from interactions with dynamic environments. The
experiments conducted in both the RoboMimic simulation and real-world scenarios
prove the superior generalization and robustness of our framework across a
variety of manipulation tasks. Our code is released at
\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.

</details>


### [298] [Latent Representations for Visual Proprioception in Inexpensive Robots](https://arxiv.org/abs/2504.14634)
*Sahara Sheikholeslami,Ladislau Bölöni*

Main category: cs.RO

TL;DR: 本文探讨了如何通过单次快速回归架构从单张外部相机图像实现视觉本体感知，并比较了多种潜在表示方法。


<details>
  <summary>Details</summary>
Motivation: 低成本机器人在非结构化环境中通常缺乏精确的本体感知能力，本文旨在探索视觉替代方案的可行性。

Method: 研究了CNN、VAE、ViT和未校准基准标记等多种潜在表示方法，并采用适应有限数据的微调技术。

Result: 在低成本6自由度机器人上的实验验证了方法的准确性。

Conclusion: 视觉本体感知在简单操作环境中具有潜力，但仍需进一步优化。

Abstract: Robotic manipulation requires explicit or implicit knowledge of the robot's
joint positions. Precise proprioception is standard in high-quality industrial
robots but is often unavailable in inexpensive robots operating in unstructured
environments. In this paper, we ask: to what extent can a fast, single-pass
regression architecture perform visual proprioception from a single external
camera image, available even in the simplest manipulation settings? We explore
several latent representations, including CNNs, VAEs, ViTs, and bags of
uncalibrated fiducial markers, using fine-tuning techniques adapted to the
limited data available. We evaluate the achievable accuracy through experiments
on an inexpensive 6-DoF robot.

</details>


### [299] [A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment](https://arxiv.org/abs/2504.15129)
*Kangyao Huang,Hao Wang,Yu Luo,Jingyu Chen,Jintao Chen,Xiangkui Zhang,Xiangyang Ji,Huaping Liu*

Main category: cs.RO

TL;DR: 提出一个平台，实现端到端深度强化学习策略的无缝迁移，解决四旋翼无人机在非结构化户外环境中的学习挑战。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机在现实环境中应用学习方法的挑战，如大量模拟数据需求、实时处理要求和模拟与现实的差距。

Method: 整合训练环境、飞行动力学控制、DRL算法、MAVROS中间件和硬件，形成从零训练到现实部署的完整工作流。

Result: 平台支持多种环境任务（如悬停、避障、轨迹跟踪等），并通过实验验证了其高效性和鲁棒性。

Conclusion: 该平台为四旋翼无人机的学习策略从模拟到现实的快速部署提供了有效解决方案。

Abstract: Deploying robot learning methods to a quadrotor in unstructured outdoor
environments is an exciting task. Quadrotors operating in real-world
environments by learning-based methods encounter several challenges: a large
amount of simulator generated data required for training, strict demands for
real-time processing onboard, and the sim-to-real gap caused by dynamic and
noisy conditions. Current works have made a great breakthrough in applying
learning-based methods to end-to-end control of quadrotors, but rarely mention
the infrastructure system training from scratch and deploying to reality, which
makes it difficult to reproduce methods and applications. To bridge this gap,
we propose a platform that enables the seamless transfer of end-to-end deep
reinforcement learning (DRL) policies. We integrate the training environment,
flight dynamics control, DRL algorithms, the MAVROS middleware stack, and
hardware into a comprehensive workflow and architecture that enables
quadrotors' policies to be trained from scratch to real-world deployment in
several minutes. Our platform provides rich types of environments including
hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and
planning in unknown environments, as a physical experiment benchmark. Through
extensive empirical validation, we demonstrate the efficiency of proposed
sim-to-real platform, and robust outdoor flight performance under real-world
perturbations. Details can be found from our website
https://emnavi.tech/AirGym/.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [300] [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
*Jon Kleinberg,Fan Wei*

Main category: math.CO

TL;DR: 论文提出了一种抽象的语言生成框架，研究算法在未知语言中生成新字符串的能力，重点探讨了有效性与广度之间的权衡，并提出了一种新算法以解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）的理论基础，探讨语言生成中的有效性与广度之间的核心矛盾，并量化这一权衡关系。

Method: 提出了一种抽象的语言生成框架，将生成视为对抗性游戏，并引入密度度量来量化广度。开发了一种新算法，确保输出在目标语言中具有严格正密度。

Result: 新算法解决了现有方法输出密度为零的问题，同时揭示了实现最大广度可能需要在高密度和低密度表示之间无限振荡。

Conclusion: 通过引入新的语言族拓扑结构，论文为语言生成的理论研究提供了新视角，并展示了算法设计的潜在复杂性。

Abstract: The recent successes of large language models (LLMs) have led to a surge of
theoretical research into language generation. A recent line of work proposes
an abstract view, called language generation in the limit, where generation is
seen as a game between an adversary and an algorithm: the adversary generates
strings from an unknown language $K$, chosen from a countable collection of
candidate languages, and after seeing a finite set of these strings, the
algorithm must generate new strings from $K$ that it has not seen before. This
formalism highlights a key tension: the trade-off between validity (the
algorithm should only produce strings from the language) and breadth (it should
be able to produce many strings from the language). This trade-off is central
in applied language generation as well, where it appears as a balance between
hallucination (generating invalid utterances) and mode collapse (generating
only a restricted set of outputs). Despite its importance, this trade-off has
been challenging to study quantitatively. We develop ways to quantify this
trade-off by formalizing breadth using measures of density. Existing algorithms
for language generation in the limit produce output sets that can have zero
density in the true language, and this important failure of breadth might seem
unavoidable. We show, however, that such a failure is not necessary: we provide
an algorithm for language generation in the limit whose outputs have strictly
positive density in $K$. We also study the internal representations built by
these algorithms, specifically the sequence of hypothesized candidate languages
they consider, and show that achieving the strongest form of breadth may
require oscillating indefinitely between high- and low-density representations.
Our analysis introduces a novel topology on language families, with notions of
convergence and limit points playing a key role.

</details>
